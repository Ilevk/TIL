{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce74409ebfe428344c104d4a6e08d72e24d97315"
   },
   "source": [
    "# 2019. 02. 08 시작\n",
    "https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard 커널 공부 및 번역하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Stacked Regressions to predict House Prices\n",
    "\n",
    "## Serigne\n",
    "\n",
    "### July 2017\n",
    "### If you use parts of this notebook in your scripts/notebooks, giving some kind of credit would be very much appreciated :) You can for instance link back to this notebook. Thanks!\n",
    "\n",
    "This competition is very important to me as it helped me to begin my journey on Kaggle few months ago. I've read some great notebooks here. To name a few : \n",
    "1. Comprehensive data exploration with Python by Pedro Marcelino: Great and very motivational data analysis\n",
    "\n",
    "2. A study on Regression applied to the Ames dataset by Julien CohenSolal : Thorough featrues engineering and deep dive into linear regression analysis but really easy to follow for beginners.\n",
    "\n",
    "3. Regularized Linear Models by Alexandru Papiu : Great Starter kernel on modelling and Cross-validation \n",
    "\n",
    "I can't recomment enough every beginner to go carefully through these kernels(and of course through many others great kernels) and get their first insights in data science and kaggle compeititons.\n",
    "\n",
    "After that (and some basic practices) you should be more confident to go through this great script by Human Analog who did an impressive work on featrues engineering.\n",
    "\n",
    "As the dataset is partivularly handy, i decided few days ago to get back in this competition and apply things I learnt so far, especially stacking models. For that purpose, we build two stacking classes ( the simplest approach and a less simple one).\n",
    "\n",
    "As these classes are written for general purpose, you can easily adapt them and/or extend them for your regressin problems. The overall approach is hopefully concise and easy to follow.\n",
    "\n",
    "The featrues engineering is rather parsimonious (at least compared to some others great scripts) . It is pretty much :\n",
    "+ **Imputing missing values** by proceeding sequentially though the data\n",
    "+ **Transforming** some numerical variables that seem really categorical\n",
    "+ **Label Encoding** some categorical variables that may contain information in their ordering set \n",
    "+ **Box Cox Transformation** of skewed featrues (instead of log-transformation) : This gave me a slightly bertter result both on leaderboard and cross-validation.\n",
    "+ **Getting dummy variables** for categorical featrues.\n",
    "\n",
    "Then we choose many base models (mostly sklearn based models + sklearn API of DMLC's XGBoost and Microsoft's LightGBM), cross-validate them on the data before stacking/ensembling them. The key here is to make the (linear) models robust to outliears. This improved the result both on LB and cross-validations.\n",
    "\n",
    "To my surprise, this does well on LB (0.11420 and top 4% the last time i tested it)\n",
    "\n",
    "** Hope that at the end of this notebook, stacking will be clear for those, like myself, who found the concept not so easy to grasp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8e8565c3cb200da32179431696f1366784ffe89"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "pd.set_option('display.float_format', lambda x : '{:.3f}'.format(x))\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output(['ls', '../input']).decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7e8bbef8d2930e72aca8cc20c74b2339c16ec950"
   },
   "outputs": [],
   "source": [
    "# now let's import and put the train and test datasets in pandas dataframe\n",
    "\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "70fb55b21ff582b85d3d1afc7bc1cefbc54c8bd6"
   },
   "outputs": [],
   "source": [
    "#display the first five rows of the train dataset/\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e8f77629a9b7b2d42bc47ac7310c9f10a7a56c0"
   },
   "outputs": [],
   "source": [
    "#display the first five rows of the test dataset.\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d9029486d742e6f149c2ee9e450e595de186699"
   },
   "outputs": [],
   "source": [
    "# check the numbers of samples and featrues\n",
    "print('the train data size before dropping ID feature is : {}'.format(train.shape))\n",
    "print('the test data size before dropping ID feature is : {}'.format(test.shape))\n",
    "\n",
    "#Save the 'Id' column\n",
    "train_ID = train['Id']\n",
    "test_ID = test['Id']\n",
    "\n",
    "#Now drop the 'Id' column since it's unnecessary for the prediction process.\n",
    "train.drop('Id', axis=1, inplace=True)\n",
    "test.drop('Id', axis=1, inplace=True)\n",
    "\n",
    "#check again the data size after dropping the 'Id' variable\n",
    "print('\\nThe train data size after dropping Id feature is : {}'.format(train.shape))\n",
    "print('The test data size after dropping Id feature is : {}'.format(test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18037375761cfede63f4236aebed4651c4126bd7"
   },
   "source": [
    "# Data Processing\n",
    "\n",
    "## Outliers\n",
    "\n",
    "Documentation for the Ames Housing Data indicates the there are outliers present in the training darta\n",
    "<br><br>\n",
    "\n",
    "Let's explore these outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "43bc7919cb6a6a575cc21b99e89b92d0783f41db"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GrLivArea', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ebb1846dffae2a775b733c61a998286e266d50f"
   },
   "source": [
    "We can see at the bottom right two with extreamely large GrLivArea that are of a low price. These values are huge outliers. Therefore, we can safely delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "915f90d86d04801cbbf5a1679eb10c8a55d5bdeb"
   },
   "outputs": [],
   "source": [
    "#Deleting outliers\n",
    "train = train.drop(train[(train['GrLivArea'] > 4000) & (train['SalePrice']<300000)].index)\n",
    "\n",
    "#Check the graphic again\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(train['GrLivArea'], train['SalePrice'])\n",
    "plt.ylabel('SalePrice', fontsize=13)\n",
    "plt.xlabel('GrLivArea', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ca22425d4994a665575fc77861ba88617c26963"
   },
   "source": [
    "#### Note:\n",
    "Outliers removal is note always safe. We decided to delete these two as they are very huge and really bad ( extreamely large areas for very low prices ).\n",
    "<br>\n",
    "There are probably others outliers in the training data. However, removing all them may affect badly our models if ever there were also outliers in the test data. That's why, instead of removing them all, we will just manage to make some of our models robust on them. You can refer to the modelling part of this notebook for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3feb82aaf5128173692474cecec454778bf159d2"
   },
   "source": [
    "## Target Variable \n",
    "\n",
    "<br>\n",
    "**SalePrice** is the variable we need to predict. So let's do some analysis on this variable first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a9ec28bbceae65ae60f293ca683dad7759a5d1d5"
   },
   "outputs": [],
   "source": [
    "sns.distplot(train['SalePrice'], fit=norm)\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['SalePrice'])\n",
    "print('\\n my = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "# Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$) {:.2f} and $\\sigma=$ {:2f}'.format(mu, sigma)],\n",
    "          loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "# Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train['SalePrice'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17a51773eefa0bba7e2fbdb3637526f3cc881263"
   },
   "source": [
    "The target variable is right skewed. As (linear) models love normally distributed data, we need to transform this variable and make it more normally distributed.\n",
    "\n",
    "#### Log-transformation of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e2dc4fcba22eafb55791b7a9bd9d6bd669ddad2"
   },
   "outputs": [],
   "source": [
    "# We use the numpy function log1p which applies log(1+x) to all elements of the column\n",
    "train['SalePrice'] = np.log1p(train['SalePrice'])\n",
    "\n",
    "# Check the new distribution\n",
    "sns.distplot(train['SalePrice'], fit=norm)\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['SalePrice'])\n",
    "print('\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "# Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f})'.format(mu, sigma)],\n",
    "          loc='best')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('SalePrice distribution')\n",
    "\n",
    "#Get also the QQ-plot\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(train['SalePrice'], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c06ea3724f41bf8baaab30a1415c9a81688f0db5"
   },
   "source": [
    "## 2019. 02. 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a640a8fd5019b708d71e6ffa2b40478e3689ac7"
   },
   "source": [
    "The skew seems now corrected and the data appears more normally distributed.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d367982bc7bed1ed23a4747ea06c7d356ab09ad8"
   },
   "source": [
    "### Features engineering\n",
    "\n",
    "let's"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
