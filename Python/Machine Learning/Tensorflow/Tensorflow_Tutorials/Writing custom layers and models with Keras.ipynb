{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 케라스 모델과 레이어를 맞춤형으로 사용하기\n",
    "##### url: https://www.tensorflow.org/guide/keras/custom_layers_and_models\n",
    "\n",
    "- 목차\n",
    "    - 설정\n",
    "    - Layer 클래스\n",
    "        - 레이어에서 가중치와 연산 캡슐화하기\n",
    "        - 모범 예제: 입력 차원이 알려지기 전까지 가중치 생성 지연시키기\n",
    "        - 재귀적으로 레이어 구성하기\n",
    "        - 데이터가 모델을 통과하는 동안 생성되는 손실 값을 재귀적으로 수집하는 레이어\n",
    "        - 선택적으로 레이어 직렬화 활성화하기\n",
    "        - call 메소드의 training 인자가 가지는 특권\n",
    "    - 모델 작성하기\n",
    "        - Model 클래스\n",
    "        - 하나로 합치기: 엔드 투 엔드 예제\n",
    "        - 객체 지향 개발을 넘어서: 함수형 API\n",
    "        \n",
    "### 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session() # 노트북의 상태를 쉽게 초기화할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer 클래스\n",
    "### 레이어에서 가중치와 연산 캡슐화하기\n",
    "사용하게 될 주요 데이터구조는 `Layer` 입니다. 레이어는 상태(레이어의 가중치,'weight')와 입력을 출력으로 변환하는 것(호출,'call', 레이어의 순전파)을 캡슐화합니다.<br><br>\n",
    "여기 밀집되어 연결된 레이어가 있습니다. 이것들은 변수 `w`와 `b`로 나타내어지는 상태를 가지고 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.02640932  0.01592169 -0.05877681 -0.0904264 ]\n",
      " [-0.02640932  0.01592169 -0.05877681 -0.0904264 ]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class Linear(layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim, units),\n",
    "                                                  dtype='float32'),\n",
    "                             trainable=True)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(initial_value=b_init(shape=(units,),\n",
    "                                                  dtype='float32'),\n",
    "                             trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "x = tf.ones((2, 2))\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`w`와 `b`가 자동적으로 레이어 속성 집합으로써 추적된다는 것을 확인해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한, `add_weight` 메소드를 사용하여 레이어가 가진 가중치에 더 빠르게 접근할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.08907721 -0.00604777  0.08206306 -0.02428109]\n",
      " [ 0.08907721 -0.00604777  0.08206306 -0.02428109]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, units),\n",
    "                                initializer='random_normal',\n",
    "                                trainable=True)\n",
    "        self.b = self.add_weight(shape=(units,),\n",
    "                                initializer='zeros',\n",
    "                                trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "s = tf.ones((2, 2))\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 레이어는 학습할 수 없는 가중치를 가질 수 있습니다. \n",
    "학습 가능한 가중치들 사이에서, 레이어에 학습할 수 없는 가중치들을 추가할 수 있습니다.<br>\n",
    "레이어를 학습하는 과정에서 역전파 시 이러한 가중치들은 고려하지 않아야 합니다.<br><br>\n",
    "어떻게 학습할 수 없는 가중치를 더하고 사용하는지 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "class ComputeSum(layers.Layer):\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),\n",
    "                                 trainable=False)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total\n",
    "    \n",
    "x = tf.ones((2, 2))\n",
    "my_sum = ComputeSum(2)\n",
    "y = my_sum(x)\n",
    "print(y.numpy())\n",
    "y = my_sum(x)\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 가중치들은 `layer.weights`의 일부분이지만, 학습할 수 없는 가중치로 분류됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 1\n",
      "non-trainable weights: 1\n",
      "trainable_weights: []\n"
     ]
    }
   ],
   "source": [
    "print('weights:', len(my_sum.weights))\n",
    "print('non-trainable weights:', len(my_sum.non_trainable_weights))\n",
    "\n",
    "# 학습할 수 있는 가중치가 포함되어 있지 않습니다.\n",
    "print('trainable_weights:', my_sum.trainable_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모범 예제: 입력 차원이 알려지기 전까지 가중치 생성 지연시키기\n",
    "\n",
    "로지스틱 회귀 예시에서, `Linear` 레이어는 `__init__` 메소드에서 가중치 `w`와 `b`의 크기를 계산하기 위해 사용되는 `input_dim`을 인자로 가지고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(layers. Layer):\n",
    "    \n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(units, ),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "많은 경우에서 입력의 크기를 미리 알 수 없습니다. 또한 레이어를 먼저 생성한 후, 입력 값이 알려졌을 때 게으르게(lazily) 가중치를 생성하길 원할 수도 있습니다.<br><br>\n",
    "Keras API에서, 레이어의 가중치를 `build(input_shape)` 메소드에서 생성하는 것을 권합니다. 해당 메소드는 다음과 같습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units, ),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레이어의 `__call__` 메소드는 자동적으로 `build` 메소드를 가장 먼저 호출하여 수행합니다. 이제 게으르고(lazy) 사용하기 쉬운 레이어가 완성되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = Linear(32) # 인스턴스를 생성하는 단계에서, 호출될 입력 데이터에 대해 알지 못합니다.\n",
    "y = linear_layer(x) # 레이어의 가중치는 해당 레이어가 호출되었을 때 가장 먼저 동적으로 생성됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 재귀적으로 레이어 구성하기\n",
    "만약, 레이어 인스턴스를 다른 레이어의 속성으로서 할당하게되면, 외부 레이어는 내부 레이어의 가중치를 트래킹하기 시작합니다.<br><br>\n",
    "`__init__` 메소드 내에서 이러한 서브레이어를 생성하는 것이 좋습니다. (서브레이어들은 일반적으로 `build` 메소드를 가지고 있으므로 외부 레이어가 생성(build)될 때 해당 레이어들도 같이 생성됩니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 6\n",
      "trainable weights: 6\n"
     ]
    }
   ],
   "source": [
    "# Linear 클래스를 재사용한다고 가정해봅시다.\n",
    "# `build` 메소드는 위에서 정의한대로 사용합니다.\n",
    "\n",
    "class MLPBlock(layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "    \n",
    "\n",
    "mlp = MLPBlock()\n",
    "y = mlp(tf.ones(shape=(3, 64)))\n",
    "print('weights:', len(mlp.weights))\n",
    "print('trainable weights:', len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터가 모델을 통과하는 동안 생성되는 손실 값을 재귀적으로 수집하는 레이어\n",
    "`call` 메소드를 작성할 때, 나중에 학습 루프를 작성할 때 사용할 손실 텐서를 만들 수 있습니다.<br>\n",
    "이는 `self.add_loss(value)`를 호출해서 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 활성화 정규화 손실 함수를 생성하는 레이어\n",
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 손실 값들은(내부 레이어 어디서든 생성되는 것들을 포함하여) `layer.losses`를 통해 값을 확인할 수 있습니다.<br>\n",
    "해당 값은 맨 앞 레이어의 `__call__`이 호출될 때마다 초기화 됩니다. 그래서 `layer.losses`는 마지막 학습에서 생성된 손실 값을 가지고 있게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OuterLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.activity_reg(inputs)\n",
    "    \n",
    "layer = OuterLayer()\n",
    "assert len(layer.losses) == 0 # 레이어가 아직 호출되지 않았기 때문에 손실 값이 존재하지 않습니다.\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1 # 레이어를 호출함으로써 손실 값 하나를 생성했습니다.\n",
    "\n",
    "# `layer.losses` 는 각 __call__ 메소드의 시작에서 초기화됩니다.\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1 # 현재 손실 값은 마지막 호출에서 생성된 값 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "추가적으로 `loss`는 내부 레이어의 가중치에서 생성된 손실 값도 포함하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.0015565304>]\n"
     ]
    }
   ],
   "source": [
    "class OuterLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.dense = layers.Dense(32, kernel_regularizer = tf.keras.regularizers.l2(1e-3))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "    \n",
    "    \n",
    "layer = OuterLayer()\n",
    "_ = layer(tf.zeros((1, 1)))\n",
    "\n",
    "# 위의 'kernel_regularizaer'에서 생성된 값은 \n",
    "# 1e-3 * sum(layer.dense.kernel ** 2)와 같습니다.\n",
    "print(layer.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 손실 값들은 학습 루프를 작성할 때, 다음과 같이 고려됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 옵티마이저를 생성 합니다.\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# # 데이터셋의 배치를 사용해 반복합니다.\n",
    "# for x_batch_train, y_batch_train in train_dataset:\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         logits = layer(x_batch_train) # 미니 배치의 로짓\n",
    "#         loss_value = loss_fn(y_batch_train, logits) # 미니 배치의 손실 값\n",
    "#         loss_value += sum(model.losses) # 순전파가 진행되는 동안 생성된 추가 로스 더하기\n",
    "        \n",
    "#     grads = tape.gradient(loss_Value, model.trainable_weights)\n",
    "#     optimizer.apply_gradients(zip(grads, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 루프에 대한 자세한 가이드는 두 번째 섹션의 `guide to training and evaluation`에서 다룹니다.<br><br>\n",
    "### 선택적으로 레이어 직렬화 활성화하기\n",
    "함수형 모델의 일부분으로써 맞춤형 레이어를 직렬화 하고자 한다면, 선택적으로 `get_config` 메소드를 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'units': 64}\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'units': self.units}\n",
    "    \n",
    "\n",
    "# 이제 해당 레이어가 가진 구성 정보로부터 레이어를 재생성 할 수 있습니다.\n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "부모 `Layer` 클래스의 `__init__` 메소드는 `name`이나 `dtype` 같은 몇개의 키워드 인자를 가지고 있습니다. <br>\n",
    "이러한 인자들을 `__init__`에 정의된 부모 클래스에 전달하고 레이어의 구성 정보(config)에 포함시키는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'linear_8', 'trainable': True, 'dtype': 'float32', 'units': 64}\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({'units': self.units})\n",
    "        return config\n",
    "    \n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 레이어가 가진 구성 정보로부터 역직렬화를 할 때 더 많은 유연성이 필요하다면, `from_config` 메소드를 오버라이드 할 수 있습니다.<br>\n",
    "`from_config`의 기본 구현은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_config(cls, config):\n",
    "    return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직렬화와 저장에 대해 더 알아보기 위해서는 [Guide to Savinng and Serializing Models](https://www.tensorflow.org/guide/keras/save_and_serialize) 를 살펴보세요.<br><br>\n",
    "### call 메소드의 training 인자가 가지는 특권\n",
    "몇몇 레이어, 특히 `BatchNormalization` 레이어와 `Dropout` 레이어는 훈련과 추론 단계에서 서로 다르게 작동합니다.<br>\n",
    "이러한 레이어를 위한 표준으로 `call` 메소드 내에서 `training`(boolean) 인자를 사용할 수 있습니다.<br><br>\n",
    "`training` 인자를 사용하여, 레이어 내부에서 내장된 학습 및 검증 루프가 학습이나 추론단계에서 적절하게 작동할 수 있도록 할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDropout(layers.Layer):\n",
    "    \n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(CostomDroupout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "        \n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 작성하기\n",
    "### Model 클래스\n",
    "일반적으로, 내부 연산 블록을 정의하기 위해 `Layer` 클래스를 사용하게 됩니다. 그리고 나중에 사용할 객체로써 `Model` 클래스를 사용하여 외부 모델을 정의합니다. <br><br>\n",
    "레즈넷50 모델 인스턴스는 `Layer` 클래스의 서브클래스로 이루어진 여러 레즈넷 블록을 가지고 있습니다. 전체 레즈넷 네트워크를 하나의 `Model` 클래스가 감싸고 있습니다.<br><br>\n",
    "`Model` 클래스는 `Layer` 클래스와 같은 API를 가지고 있으며, 다음과 같은 차이점이 있습니다.<br>\n",
    "- 학습, 검증, 예측 루프에 사용되는 내장 함수를 가지고 있습니다. (`model.fit()`, `model.evaluate()`, `model.predict()`)\n",
    "- `model.layers` 속성을 통해 내부 레이어의 리스트에 접근할 수 있습니다.\n",
    "- 저장 및 직렬화 API를 가지고 있습니다.\n",
    "\n",
    "효과적으로, \"Layer\" 클래스는 우리가 'layer'라고 부르는 것(convolution layer, recurrent layer) 또는 'block'이라 부르는 것('Resnet block' or 'Inception block')에 대응합니다.<br><br>\n",
    "한편, \"Model\" 클래스는 우리가 'model'이라고 부르는 것(deep learning model)이나 'network'라고 부르는 것(deep neural network)에 대응합니다.<br><br>\n",
    "작은 레즈넷 모델 예제를 살펴보고, 모델을 생성하기 위해 `Model` 클래스를 사용하겠습니다. 그리고 `fit()` 함수를 사용해 학습을 진행한 뒤에 `save_weights` 함수로 모델을 저장하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResNet(tf.keras.Model):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super(Resnet, self).__init__()\n",
    "#         self.block_1 = ResNetBlock()\n",
    "#         self.block_2 = ResNetBlock()\n",
    "#         self.global_pool = layers.GlobalAveragePooling2D()\n",
    "#         self.classifier = Desne(num_classes)\n",
    "        \n",
    "#     def call(self, inputs):\n",
    "#         x = self.block_1(inputs)\n",
    "#         x = self.block_2(x)\n",
    "#         x = self.global_pool(x)\n",
    "#         return self.classifier(x)\n",
    "    \n",
    "# resnet = ResNet()\n",
    "# dataset = ...\n",
    "# resnet.fit(dataset, epochs=10)\n",
    "# resnet.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하나로 합치기: 엔드 투 엔드 예제\n",
    "지금까지 배운 내용은 다음과 같습니다.\n",
    "- `Layer` 클래스는 `__init__` 또는 `build`로 생성된 내부 상태와 `call` 함수 내부의 연산을 캡슐화 한다.\n",
    "- Layer 클래스는 새로운 거대한 연산을 위한 블록을 생성하기 위해 반복적으로 중첩됩니다.\n",
    "- Layer 클래스는 손실 함수(일반적으로 정규화 손실 함수)를 생성하고 추적할 수 있습니다.\n",
    "- 학습하고자 하는 외부 컨테이너는 `Model` 클래스이며, `Model` 클래스는 `Layer` 클래스와 비슷하지만 학습 및 직렬화 유틸리티가 추가되었습니다.<br><br>\n",
    "이러한 것들을 하나로 합쳐 엔드 투 엔드 예제를 만들어보겠습니다. MNIST 숫자 이미지를 생성하는 Variational AutoEncoder를 구현해보겠습니다. <br><br>\n",
    "VAE는 `Model` 클래스를 서브클래싱하고 `Layer` 클래스의 서브클래스로 이루어진 레이어들의 중첩 구조로 구성합니다. 그리고 KL Divergence라 불리는 정규화 손실 함수를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    # (z_mean, z_log_var) 는 z로 부터 샘플링되며, 벡터는 숫자를 인코딩한 결과입니다.\n",
    "    \n",
    "    def call(sefl, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 + z_log_var) * epsilon\n",
    "    \n",
    "    \n",
    "class Encoder(layers.Layer):\n",
    "    # MNIST 숫자를 (z_mean, z_log_var, z)이 세가지 값으로 매핑합니다.\n",
    "    \n",
    "    def __init__(self,\n",
    "                 latent_dim=32,\n",
    "                 intermediate_dim=64,\n",
    "                 name='encoder',\n",
    "                 **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "    \n",
    "class Decoder(layers.Layer):\n",
    "    # z를 변환하여 인코딩된 숫자 벡터를 읽을 수 있는 숫자로 되돌립니다.\n",
    "    \n",
    "    def __init__(self,\n",
    "                 original_dim,\n",
    "                 intermediate_dim=64,\n",
    "                 name='decoder',\n",
    "                 **kwargs):\n",
    "        super(Decoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
    "        self.dense_output = layers.Dense(original_dim, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "    \n",
    "\n",
    "class VariationalAutoEncoder(tf.keras.Model):\n",
    "    # 인코더와 디코더를 연결하여 엔드 투 엔드 학습 모델을 구성합니다.\n",
    "    \n",
    "    def __init__(self,\n",
    "                 original_dim,\n",
    "                 intermediate_dim=64,\n",
    "                 latent_dim=32,\n",
    "                 name='autoencoder',\n",
    "                 **kwargs):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim,\n",
    "        self.encoder = Encoder(latent_dim=latent_dim,\n",
    "                              intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # KL divergence 손실 함수 추가\n",
    "        kl_loss = - 0.5 * tf.reduce_mean(\n",
    "                z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "step 0: mean loss = tf.Tensor(0.37233296, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(0.11438663, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(0.09296365, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(0.085019186, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(0.081082925, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(0.07838162, shape=(), dtype=float32)\n",
      "step 600: mean loss = tf.Tensor(0.07667232, shape=(), dtype=float32)\n",
      "step 700: mean loss = tf.Tensor(0.07537877, shape=(), dtype=float32)\n",
      "step 800: mean loss = tf.Tensor(0.07444117, shape=(), dtype=float32)\n",
      "step 900: mean loss = tf.Tensor(0.0736006, shape=(), dtype=float32)\n",
      "Start of epoch 1\n",
      "step 0: mean loss = tf.Tensor(0.0733748, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(0.072845384, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(0.07246542, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(0.07206686, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(0.07181289, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(0.07147775, shape=(), dtype=float32)\n",
      "step 600: mean loss = tf.Tensor(0.0712426, shape=(), dtype=float32)\n",
      "step 700: mean loss = tf.Tensor(0.07100683, shape=(), dtype=float32)\n",
      "step 800: mean loss = tf.Tensor(0.07081786, shape=(), dtype=float32)\n",
      "step 900: mean loss = tf.Tensor(0.07059313, shape=(), dtype=float32)\n",
      "Start of epoch 2\n",
      "step 0: mean loss = tf.Tensor(0.07053005, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(0.07039187, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(0.07028141, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(0.070162326, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(0.07009362, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(0.06995744, shape=(), dtype=float32)\n",
      "step 600: mean loss = tf.Tensor(0.069871314, shape=(), dtype=float32)\n",
      "step 700: mean loss = tf.Tensor(0.06975904, shape=(), dtype=float32)\n",
      "step 800: mean loss = tf.Tensor(0.06968072, shape=(), dtype=float32)\n",
      "step 900: mean loss = tf.Tensor(0.069571376, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "original_dim = 784\n",
    "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "# 에폭 수 만큼 반복\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    \n",
    "    # 데이터 셋의 배치 크기 만큼 데이터를 반복 학습\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            # Reconstruction loss 계산\n",
    "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            loss += sum(vae.losses) # KLD 손실을 더함\n",
    "            \n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "        \n",
    "        loss_metric(loss)\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print('step %s: mean loss = %s' % (step, loss_metric.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAE 모델은 `Model` 클래스를 서브클래싱한 것이므로, 내장된 학습 루프를 사용합니다.<br>\n",
    "다음과 같이 수행됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0733\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 2s 26us/sample - loss: 0.0677\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13a21da58>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VariationalAutoEncoder(784, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 객체 지향 개발을 넘어서: 함수형 API\n",
    "이번 예제가 너무 객체 지향적이지는 않으셨나요? 함수형 API를 사용해서도 모델을 작성하실 수 있습니다.<br>\n",
    "중요한 점은, 하나의 스타일을 선택하는 것이 다른 스타일을 사용하여 작성된 구성요소를 방해하지 않는다는 점입니다. 원한다면 두 스타일을 적재적소에 섞을 수 있습니다. <br><br>\n",
    "예제로, 위에서 작성했던 `Sampling` 레이어를 재사용하여 함수형 API 예제를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0731\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0677\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 2s 29us/sample - loss: 0.0676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1534d1978>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "# 인코더 모델 정의\n",
    "original_inputs = tf.keras.Input(shape=(original_dim,), name='encoder_input')\n",
    "x = layers.Dense(intermediate_dim, activation='relu',)(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name='encoder')\n",
    "\n",
    "# 디코더 모델 정의\n",
    "latent_inputs = tf.keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation='sigmoid')(x)\n",
    "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name='decoder')\n",
    "\n",
    "# VAE 모델 정의\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name='vae')\n",
    "\n",
    "# KL Divergence 손실 함수 추가\n",
    "kl_loss = - 0.5 * tf.reduce_mean(\n",
    "        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae.add_loss(kl_loss)\n",
    "\n",
    "# 학습\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs=3, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyle",
   "language": "python",
   "name": "kyle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
