{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data: 텐서플로우 입력 파이프라인 만들기\n",
    "##### url: https://www.tensorflow.org/guide/data\n",
    "\n",
    "- 목차\n",
    "    - 기본 구조\n",
    "        - 데이터셋 구조\n",
    "    - 입력 데이터 불러오기\n",
    "        - 넘파이 행렬 사용하기\n",
    "        - 파이썬 제너레이터 사용하기\n",
    "        - TFRecord 데이터 사용하기\n",
    "        - 텍스트 데이터 사용하기\n",
    "        - CSV 데이터 사용하기\n",
    "        - 데이터 집합 사용하기\n",
    "    - 데이터 원소 배치 만들기\n",
    "        - 간단한 배치 만들기\n",
    "        - 패딩과 함께 텐서 배치 만들기\n",
    "    - 학습 흐름\n",
    "        - 다중 에폭 수행하기\n",
    "        - 입력 데이터를 임의로 섞기\n",
    "    - 데이터 전처리\n",
    "        - 이미지 데이터 복호화 및 리사이즈하기\n",
    "        - 임의의 파이썬 함수 적용하기\n",
    "        - tf.Example 프로토콜 버퍼 메시지 파싱하기\n",
    "        - 시계열 윈도우 적용하기\n",
    "        - 리샘플링\n",
    "    - 고수준 API 사용하기\n",
    "        - tf.keras\n",
    "        - tf.estimator\n",
    "\n",
    "`tf_data` API는 간단하고 재사용이 가능한 조각을 통해 복잡한 입력 파이프라인을 만들 수 있게 해줍니다.<br>\n",
    "예를 들어 이미지 모델을 위한 파이프라인은 파일 시스템에 분산되어 있는 파일들을 모아주고, 각 이미지에 임의성을 부여하며 학습 배치로 사용될 선택된 이미지를 임의로 병합할 수 있습니다.<br>\n",
    "텍스트 모델을 위한 파이프라인은 원본 텍스트 데이터에서 심볼을 추출하는 것을 포함해 룩업 테이블과 함께 임베딩 식별자로 변환하거나 서로 다른 길이를 가진 시퀀스로 배치를 만들 수 있습니다.<br>\n",
    "`tf.data` API는 거대한 양의 데이터를 다룰 수 있게 만들어주며, 서로 다른 데이터 포맷을 읽거나 복잡한 변환을 수행할 수 있습니다.<br><br>\n",
    "`tf.data` API는 하나 또는 그 이상의 컴포넌트로 구성된 각 원소의 시퀀스를 표현하는 `tf.data.Dataset` 추상화를 도입합니다. <br>\n",
    "예를 들어, 이미지 파이프라인에서 이미지와 라벨로 표현된 텐서 컴포넌트의 쌍으로 원소들은 하나의 학습 샘플이 될 수 있습니다.\n",
    "\n",
    "- 데이터 소스는 메모리에 저장되어 있거나 하나 또는 그 이상의 파일로 저장된 데이터로부터 `Dataset` 객체를 만듭니다.\n",
    "- 데이터 변환은 하나 또는 그 이상의 `tf.data.Dataset` 객체로부터 데이터셋을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 구조\n",
    "입력 파이프라인을 만드려면, 반드시 데이터 소스와 함께 시작해야 합니다. <br>\n",
    "예를 들어, 메모리 내의 데이터로부터 `Dataset`를 만들기 위해서는 `tf.data.Dataset.from_tensors()` 함수 또는 `tf.data.Dataset.from_tensor_slices()`함수를 사용할 수 있습니다.<br>\n",
    "대안으로, 입력 데이터가 권장되는 TFRecord 포맷의 파일로 저장되어 있다면 `tf.data.TFRecordDataset()` 함수를 사용할 수 있습니다.<br><br>한번 `Dataset` 객체를 만들게 되면, `tf.data.Dataset` 객체의 연쇄 메소드를 통해 새로운 `Dataset`으로 변환할 수 있습니다.<br>\n",
    "예를 들어, 단일 원소에 적용되는 `Dataset.map()` 함수와 다중 원소에 적용되는 `Dataset.batch()` 변환을 수행할 수 있습니다.<br><br>\n",
    "`Dataset` 객체는 '파이썬 이터러블(Iterable)' 객체이기 때문에 각 원소를 for loop을 사용해 모델에 입력할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "3\n",
      "0\n",
      "8\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for elem in dataset:\n",
    "    print(elem.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아니면 직접적으로 `iter` 함수를 사용해 파이썬 이터레이터를 사용하고 `next` 함수를 사용해 원소를 소진 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "it = iter(dataset)\n",
    "\n",
    "print(next(it).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대안으로 데이터셋 원소는 `reduce` 변환을 사용해 소진될 수 있으며, 해당 변환은 단일 결과를 생성하기위해 모든 원소를 압축합니다.<br>\n",
    "다음 예제를 통해 데이터셋의 정수 합을 계산하는 `reduce` 변환이 어떻게 사용되는지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(dataset.reduce(0, lambda state, value: state + value).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터셋 구조\n",
    "데이터셋은 내재적으로 같은 구조를 가지고 있는 원소와 `Tensor`, `SparseTensor`, `ReggedTensor`, `TensorArray`, `Dataset`처럼 `tf.TypeSpec`에 의해 표현되는 타입 속성 구조를 가진 각각의 컴포넌트로 이루어져 있습니다.<br><br>\n",
    "`Dataset.element_spec` 프로퍼티는 각 원소 컴포넌트의 타입을 검사할 수 있게 해줍니다. <br>\n",
    "해당 프로퍼티는 원소의 구조를 매칭하기 위한 `tf.TypeSpec` 객체의 내부 내재된 구조를 반환하며 원소의 구조는 단일 컴포넌트, 튜플 컴포넌트 또는 내재된 튜풀 컴포넌트일 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(10,), dtype=tf.float32, name=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단일 컴포넌트\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10]))\n",
    "\n",
    "dataset1.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(100,), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 튜플 컴포넌트\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.random.uniform([4]),\n",
    "     tf.random.uniform([4, 100], maxval=100, dtype=tf.int32))\n",
    ")\n",
    "\n",
    "dataset2.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(10,), dtype=tf.float32, name=None),\n",
       " (TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(100,), dtype=tf.int32, name=None)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 내재된 튜플 컴포넌트\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "\n",
    "dataset3.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseTensorSpec(TensorShape([3, 4]), tf.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 희소 텐서를 포함하는 데이터셋\n",
    "dataset4 = tf.data.Dataset.from_tensors(\n",
    "   tf.SparseTensor(indices=[[0, 0], [1, 2]],\n",
    "                   values=[1, 2],\n",
    "                   dense_shape=[3, 4]\n",
    "                  )\n",
    ")\n",
    "dataset4.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.sparse_tensor.SparseTensor"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value_type를 사용해서 element spec에 의해 표현되는 값의 타입 살펴보기\n",
    "dataset4.element_spec.value_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset` 변환은 모든 구조의 데이터셋을 지원합니다. <br>\n",
    "각 원소에 적용 되는 함수인 `Dataset.map()` 또는 `Dataset.filter()` 변환을 사용할 떄, 원소 구조는 함수의 인자로 결정됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (10,), types: tf.int32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(\n",
    "    tf.random.uniform([4, 10], minval=1, maxval=10, dtype=tf.int32)\n",
    ")\n",
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 5 5 6 2 1 8 5 3 9]\n",
      "[1 9 8 5 3 4 5 9 4 5]\n",
      "[9 1 1 5 2 4 7 5 2 5]\n",
      "[6 9 2 4 5 6 4 9 2 8]\n"
     ]
    }
   ],
   "source": [
    "for z in dataset1:\n",
    "    print(z.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((), (100,)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.random.uniform([4]),\n",
    "     tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)\n",
    "    \n",
    "    )\n",
    ")\n",
    "\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((10,), ((), (100,))), types: (tf.int32, (tf.float32, tf.int32))>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "\n",
    "dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (10,), (), (100,)\n",
      "shapes: (10,), (), (100,)\n",
      "shapes: (10,), (), (100,)\n",
      "shapes: (10,), (), (100,)\n"
     ]
    }
   ],
   "source": [
    "for a, (b, c) in dataset3:\n",
    "    print(f'shapes: {a.shape}, {b.shape}, {c.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 입력 데이터 읽어오기\n",
    "### 넘파이 행렬 읽어오기\n",
    "더 많은 예제는 [Loading NumPy arrays](https://www.tensorflow.org/tutorials/load_data/numpy)를 확인하세요.<br><br>\n",
    "만약 모든 입력 데이터가 메모리에 올라갈 수 있다면, `Dataset`을 만드는 가장 쉬운 방법은 `tf.Tensor` 객체로 변환하거나, `Dataset.from_tensor_slices()` 함수를 사용하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((28, 28), ()), types: (tf.float64, tf.uint8)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = train\n",
    "images = images/255.0\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: 위의 코드 스니펫은 변수(feature)와 라벨(label) 행렬을 `tf.constant()` 연산으로써 텐서플로우 그래프에 포함합니다. 이러한 작업은 작은 데이터셋에는 유용할 수 있으나, 여러번에 걸쳐 행렬의 내용을 복사하므로 메모리를 낭비할 수 있습니다. 그리고 2GB로 제한된 `tf.GraphDef` 프로토콜 버퍼의 크기에 도달할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파이썬 제너레이터로 읽어오기\n",
    "다른 `tf.data.Dataset`으로써 쉽게 수집될 수 있는 일반적인 데이터 소스는 파이썬 제너레이터가 있습니다.\n",
    "\n",
    "> 주의: 이것은 이식성과 확장성을 제한할 수 있는 가장 편리한 접근 방법이지만, 이러한 작업은 반드시 제너레이터를 생성한 같은 파이썬 프로세스에서 실행되어야하며, 여전히 파이썬 GIL이 적용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(stop):\n",
    "    i = 0\n",
    "    while i<stop :\n",
    "        yield i\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for n in count(5):\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset.from_generator` 생성자는 파이썬 제너레이터를 완전히 함수적인 `tf.data.Dataset`으로 변환합니다.<br><br>\n",
    "그 생성자는 입력으로써 iterator가 아닌 callable을 사용합니다. 이를 통해 제너레이터가 끝에 다다랐을 때 재시작될 수 있도록 합니다.<br>\n",
    "선택적으로 callable의 인자로서 넘겨줄 수 있도록 `args` 인수를 사용할 수 있습니다.<br><br>\n",
    "`output_types` 인수는 내부적으로 `tf.data`가 `tf.Graph`를 만들기 때문에 요구됩니다. 그리고 그래프 엣지는 `tf.dtype`을 요구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_counter = tf.data.Dataset.from_generator(count,\n",
    "                                            args=[25],\n",
    "                                            output_types=tf.int32,\n",
    "                                            output_shapes = (), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[10 11 12 13 14 15 16 17 18 19]\n",
      "[20 21 22 23 24  0  1  2  3  4]\n",
      "[ 5  6  7  8  9 10 11 12 13 14]\n",
      "[15 16 17 18 19 20 21 22 23 24]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[10 11 12 13 14 15 16 17 18 19]\n",
      "[20 21 22 23 24  0  1  2  3  4]\n",
      "[ 5  6  7  8  9 10 11 12 13 14]\n",
      "[15 16 17 18 19 20 21 22 23 24]\n"
     ]
    }
   ],
   "source": [
    "for count_batch in ds_counter.repeat().batch(10).take(10):\n",
    "    print(count_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`outpyt_shapes` 인수는 요구되지 않습니다. 하지만 많은 텐서플로우 연산들이 알려지지 않은(unknown) 랭크를 지원하지 않기 때문에 입력하는 것을 강력히 권해드립니다.<br>\n",
    "만약 특정한 축의 길이가 알려지지 않거나 또는 변할 수 있다면, `output_shape`에 `None` 을 세팅합니다.<br><br>\n",
    "또한, `output_shapes`와 `output_types`는 다른 데이터 메소드와서도 동일한 중첩 규칙을 따른다는 것도 중요합니다.<br><br>\n",
    "여기에 두 측면을 살펴볼 제너레이터 예제가 있습니다. 이 제너레이터는 행렬의 튜플을 반환하며, 두번째 행렬은 길이가 알려지지 않은 벡터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_series():\n",
    "    i = 0\n",
    "    while True:\n",
    "        size = np.random.randint(0, 10)\n",
    "        yield i, np.random.normal(size=(size,))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : [-1.5401  1.2474  0.989   1.5283  1.1427 -0.0665]\n",
      "1 : [-2.0727  1.0102  0.3989]\n",
      "2 : [-0.0215 -0.1404  2.114 ]\n",
      "3 : [ 1.1446  0.2048 -0.2775]\n",
      "4 : [1.3588 0.2155 2.5134 0.1715 0.4316]\n",
      "5 : []\n",
      "6 : [-1.0698  0.1139 -0.2755 -0.4252]\n"
     ]
    }
   ],
   "source": [
    "for i, series in gen_series():\n",
    "    print(i, ':', str(series))\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 출력 값은 `int32`이고, 두 번째 값은 `float32`<br><br>\n",
    "첫 번째 아이템은 `()` 차원의 스칼라이며, 두 번째는 알려지지 않은 길이인 `(None, )`차원의 벡터 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((), <unknown>), types: (tf.int32, tf.float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_series = tf.data.Dataset.from_generator(\n",
    "    gen_series,\n",
    "    output_types=(tf.int32, tf.float32),\n",
    "    output_shapes=((), (None)))\n",
    "\n",
    "ds_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 일반적인 `tf.data.Dataset`처럼 사용될 수 있습니다. 변화할 수 있는 차원을 가진 데이터셋을 배치로 만들 때, `Dataset.padded_batch`를 사용할 필요가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9 14  2 20  1 21  0  3 11 28]\n",
      "\n",
      "[[-0.0239  0.5971  0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [ 2.2549  0.5714  0.8862  0.0574 -0.4186 -0.3982  0.      0.      0.    ]\n",
      " [ 1.6973  1.4021 -0.0633  0.5444  1.0153 -1.5627  1.5152  0.219  -1.1141]\n",
      " [ 0.2389 -1.078  -1.1097 -0.5589  0.      0.      0.      0.      0.    ]\n",
      " [-1.0254 -0.2037  0.1754 -0.2031 -1.712  -1.5481  0.      0.      0.    ]\n",
      " [-0.3888 -0.2074  1.3462  0.      0.      0.      0.      0.      0.    ]\n",
      " [ 1.7791 -0.0361  0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [ 0.      0.      0.      0.      0.      0.      0.      0.      0.    ]\n",
      " [ 0.5533  0.4009 -1.7127  0.      0.      0.      0.      0.      0.    ]\n",
      " [-0.4679  0.9785  0.5473  0.      0.      0.      0.      0.      0.    ]]\n"
     ]
    }
   ],
   "source": [
    "# padded_batch를 사용해서 배치 사이즈를 통해 일정한 길이로 맞춰줄 수 있습니다.\n",
    "ds_series_batch = ds_series.shuffle(20).padded_batch(10, padded_shapes=([], [None]))\n",
    "\n",
    "ids, sequence_batch = next(iter(ds_series_batch))\n",
    "print(ids.numpy(), end='\\n\\n')\n",
    "print(sequence_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 현실적인 예제로, `tf.data.Dataset`로써 `preprocessing.image.ImageDataGenerator`로 래핑을 시도해보겠습니다.<br><br>\n",
    "먼저 데이터를 다운받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers = tf.keras.utils.get_file(\n",
    "    'flower_photos',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    untar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`image.ImageDataGenerator`를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(img_gen.flow_from_directory(flowers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (32, 256, 256, 3)\n",
      "float32 (32, 5)\n"
     ]
    }
   ],
   "source": [
    "print(images.dtype, images.shape)\n",
    "print(labels.dtype, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((32, 256, 256, 3), (32, 5)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_generator(\n",
    "    img_gen.flow_from_directory, \n",
    "    args=[flowers],\n",
    "    output_types=(tf.float32, tf.float32), \n",
    "    output_shapes=([32,256,256,3], [32,5])\n",
    ")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecord 데이터 사용하기\n",
    "\n",
    "End-to-end 예제는 [Loading TFRecords](https://www.tensorflow.org/tutorials/load_data/tf_records)를 살펴보세요.<br><br>\n",
    "`tf.data` API는 다양한 파일 포맷을 지원합니다. 그래서 데이터 셋이 메모리에 전부 적재되지 않더라도 크기가 큰 데이터셋을 처리할 수 있습니다.<br>\n",
    "예를 들어, TFRecord 파일 포맷은 많은 텐서플로우 어플리케이션이 학습 데이터에 사용하는 간단한 레코드-지향 이진 포맷입니다.<br>\n",
    "`tf.data.TFRecordDataset` 클래스는 입력 파이프라인의 일부로써 하나 또는 그 이상의 TFRecord 파일의 컨텐츠를 스트림할 수 있도록 해줍니다.<br><br>\n",
    "French Street Name Signs(FSNS) 데이터셋의 테스트 파일을 사용한 예제를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 개의 파일의 예제의 전부를 불러와 데이터셋을 생성합니다.\n",
    "fsns_test_file = tf.keras.utils.get_file(\"fsns.tfrec\", \n",
    "                                         \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.data.TFRecordDataset`의 `filename` 인자는 문자열이거나 문자열의 리스트 또는, `tf.Tensor` 문자열일 수도 있습니다.<br>\n",
    "그러므로 학습과 검증의 목적으로 두개로 이루어진 파일을 가지고 있다면, 입력 인자로써 파일 이름을 사용해 데이터셋을 생성하기 위한 팩토리 메소드를 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "많은 텐서플로우 프로젝트는 TFRecord 파일에 있는 직렬화된 `tf.train.Example` 레코드를 사용합니다.<br>\n",
    "해당 레코드들은 확인되기 전에, 복호화가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes_list {\n",
       "  value: \"Rue Perreyon\"\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_example = next(iter(dataset))\n",
    "parsed = tf.train.Example.FromString(raw_example.numpy())\n",
    "\n",
    "parsed.features.feature['image/text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 데이터 사용하기\n",
    "\n",
    "End-to-End 예제는 [Loading Text](https://www.tensorflow.org/tutorials/load_data/text)를 살펴보세요.<br><br>\n",
    "많은 데이터셋이 하나 또는 그 이상의 텍스트 파일로 분산되어 있습니다.<br>\n",
    "`tf.data.TextLineDataset`은 하나 또는 이상의 텍스트 파일로부터 행을 추출하는 가장 쉬운 방법을 제공합니다을<br>\n",
    "하나 또는 더 많은 파일 이름이 주어지면, `TextLineDataset`은 해당 파일의 행 당 하나의 문자열-값 원소를 생성하게됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_url = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
    "file_names = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
    "\n",
    "file_paths = [\n",
    "    tf.keras.utils.get_file(file_name, directory_url + file_name) for file_name in file_names\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 파일의 앞의 몇 행을 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus' son;\"\n",
      "b'His wrath pernicious, who ten thousand woes'\n",
      "b\"Caused to Achaia's host, sent many a soul\"\n",
      "b'Illustrious into Ades premature,'\n",
      "b'And Heroes gave (so stood the will of Jove)'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 파일들 사이에서 행이 번갈아 일어나도록 `Dataset,interleave` 함수를 사용합니다.<br>\n",
    "이는 파일이 손쉽게 섞일 수 있도록 만들어줍니다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "b\"\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus' son;\"\n",
      "b\"\\xef\\xbb\\xbfOf Peleus' son, Achilles, sing, O Muse,\"\n",
      "b'\\xef\\xbb\\xbfSing, O goddess, the anger of Achilles son of Peleus, that brought'\n",
      "\n",
      "b'His wrath pernicious, who ten thousand woes'\n",
      "b'The vengeance, deep and deadly; whence to Greece'\n",
      "b'countless ills upon the Achaeans. Many a brave soul did it send'\n",
      "\n",
      "b\"Caused to Achaia's host, sent many a soul\"\n",
      "b'Unnumbered ills arose; which many a soul'\n",
      "b'hurrying down to Hades, and many a hero did it yield a prey to dogs and'\n"
     ]
    }
   ],
   "source": [
    "files_ds = tf.data.Dataset.from_tensor_slices(file_paths)\n",
    "lines_ds = files_ds.interleave(tf.data.TextLineDataset, cycle_length=3)\n",
    "\n",
    "for i, line in enumerate(lines_ds.take(9)):\n",
    "    if i % 3 == 0:\n",
    "        print()\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본 값으로 `TextLineDataset`은 각 파일의 모든 행을 생성하므로 파일이 주석을 포함하고 있거나 헤더 라인부터 시작된다면, 이는 바람직하지 않을 수 있습니다.<br>\n",
    "그런 행들은 `Dataset.skip()` 또는 `Dataset.filter()` 변환을 사용하여 제거할 수 있습니다.<br>\n",
    "첫 번째 행을 건너뛰고 생존자만을 필터링하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "titanic_lines = tf.data.TextLineDataset(titanic_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone'\n",
      "b'0,male,22.0,1,0,7.25,Third,unknown,Southampton,n'\n",
      "b'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n'\n",
      "b'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y'\n",
      "b'1,female,35.0,1,0,53.1,First,C,Southampton,n'\n",
      "b'0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y'\n",
      "b'0,male,2.0,3,1,21.075,Third,unknown,Southampton,n'\n",
      "b'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n'\n",
      "b'1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n'\n",
      "b'1,female,4.0,1,1,16.7,Third,G,Southampton,n'\n"
     ]
    }
   ],
   "source": [
    "for line in titanic_lines.take(10):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def survived(line):\n",
    "    return tf.not_equal(tf.strings.substr(line, 0, 1), '0')\n",
    "\n",
    "survivors = titanic_lines.skip(1).filter(survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n'\n",
      "b'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y'\n",
      "b'1,female,35.0,1,0,53.1,First,C,Southampton,n'\n",
      "b'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n'\n",
      "b'1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n'\n",
      "b'1,female,4.0,1,1,16.7,Third,G,Southampton,n'\n",
      "b'1,male,28.0,0,0,13.0,Second,unknown,Southampton,y'\n",
      "b'1,female,28.0,0,0,7.225,Third,unknown,Cherbourg,y'\n",
      "b'1,male,28.0,0,0,35.5,First,A,Southampton,y'\n",
      "b'1,female,38.0,1,5,31.3875,Third,unknown,Southampton,n'\n"
     ]
    }
   ],
   "source": [
    "for line in survivors.take(10):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV 데이터 사용하기\n",
    "더 많은 예제는 [Loading CSV Files](https://www.tensorflow.org/tutorials/load_data/csv)와 [Loading Pandas DataFrames](https://www.tensorflow.org/tutorials/load_data/pandas)를 확인하세요.<br><br>\n",
    "\n",
    "CSV는 일반 텍스트로 이루어진 정형 데이터를 저장하기 위한 인기있는 파일 포맷입니다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_file = tf.keras.utils.get_file(\"train.csv\", \n",
    "                                       \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived     sex   age  n_siblings_spouses  parch     fare  class     deck  \\\n",
       "0         0    male  22.0                   1      0   7.2500  Third  unknown   \n",
       "1         1  female  38.0                   1      0  71.2833  First        C   \n",
       "2         1  female  26.0                   0      0   7.9250  Third  unknown   \n",
       "3         1  female  35.0                   1      0  53.1000  First        C   \n",
       "4         0    male  28.0                   0      0   8.4583  Third  unknown   \n",
       "\n",
       "   embark_town alone  \n",
       "0  Southampton     n  \n",
       "1    Cherbourg     n  \n",
       "2  Southampton     y  \n",
       "3  Southampton     n  \n",
       "4   Queenstown     y  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(titanic_file, index_col=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 데이터 파일을 모두 메모리에 적재할 수 있다면, 동일한 `Dataset.from_tensor_slices` 메소드에 딕셔너리 형태로 전달하여 작동시킵니다.<br>\n",
    "이는 데이터를 쉽게 불러올 수 있도록 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   'survived'          : 0\n",
      "   'sex'               : b'male'\n",
      "   'age'               : 22.0\n",
      "   'n_siblings_spouses': 1\n",
      "   'parch'             : 0\n",
      "   'fare'              : 7.25\n",
      "   'class'             : b'Third'\n",
      "   'deck'              : b'unknown'\n",
      "   'embark_town'       : b'Southampton'\n",
      "   'alone'             : b'n'\n"
     ]
    }
   ],
   "source": [
    "titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "\n",
    "for feature_batch in titanic_slices.take(1):\n",
    "    for key, value in feature_batch.items():\n",
    "        print('   {!r:20s}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더욱 확장성있는 접근 방법은 필요할 때 디스크에서 불러오는 방법이 있습니다.<br><br>\n",
    "`tf.data` 모듈은 `RFC 4180`을 따르는 하나 또는 그 이상의 CSV 파일로부터 레코드를 추출하는 메소드를 제공합니다.<br><br>\n",
    "`experimental.make_csv_dataset` 함수는 CSV 파일 집합을 읽기 위한 고수준 인터페이스입니다. <br>\n",
    "해당 함수는 열 타입의 추론과 배치 만들기, 섞기 등 많은 기능들을 제공하여 사용하기에 간편합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kyle/kyle/lib/python3.6/site-packages/tensorflow_core/python/data/experimental/ops/readers.py:521: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "WARNING:tensorflow:From /Users/kyle/kyle/lib/python3.6/site-packages/tensorflow_core/python/data/experimental/ops/readers.py:215: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    }
   ],
   "source": [
    "titanic_batches = tf.data.experimental.make_csv_dataset(\n",
    "    titanic_file, batch_size=4,\n",
    "    label_name='survived'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'survived' : [0 1 0 1]\n",
      "features\n",
      "    'sex'               : [b'male' b'female' b'male' b'male']\n",
      "    'age'               : [17. 15. 28.  1.]\n",
      "    'n_siblings_spouses': [1 0 0 1]\n",
      "    'parch'             : [1 1 0 2]\n",
      "    'fare'              : [  7.2292 211.3375   7.225   20.575 ]\n",
      "    'class'             : [b'Third' b'First' b'Third' b'Third']\n",
      "    'deck'              : [b'unknown' b'B' b'unknown' b'unknown']\n",
      "    'embark_town'       : [b'Cherbourg' b'Southampton' b'Cherbourg' b'Southampton']\n",
      "    'alone'             : [b'n' b'n' b'y' b'n']\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in titanic_batches.take(1):\n",
    "    print(f\"'survived' : {label_batch}\")\n",
    "    print('features')\n",
    "    for key, value in feature_batch.items():\n",
    "        print('    {!r:20s}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정한 컬럼들의 부분 집합이 필요하다면, `select_columns` 인자를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_batches = tf.data.experimental.make_csv_dataset(\n",
    "    titanic_file, batch_size=4,\n",
    "    label_name='survived', select_columns=['class', 'fare', 'survived']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'survived': [1 0 1 1]\n",
      "    'fare'              : [32.5    10.5167 78.85    8.1125]\n",
      "    'class'             : [b'Second' b'Third' b'First' b'Third']\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in titanic_batches.take(1):\n",
    "    print(f\"'survived': {label_batch}\")\n",
    "    for key, value in feature_batch.items():\n",
    "        print('    {!r:20s}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더욱 세분화된 제어를 제공하는 저수준 `experimental.CsvDataset` 클래스가 있습니다.<br>\n",
    "해당 클래스는 열 타입 추론을 제공하지는 않습니다. 대신에 각 열에 대한 타입을 반드시 명확히 해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, b'male', 22.0, 1, 0, 7.25, b'Third', b'unknown', b'Southampton', b'n']\n",
      "[1, b'female', 38.0, 1, 0, 71.2833, b'First', b'C', b'Cherbourg', b'n']\n",
      "[1, b'female', 26.0, 0, 0, 7.925, b'Third', b'unknown', b'Southampton', b'y']\n",
      "[1, b'female', 35.0, 1, 0, 53.1, b'First', b'C', b'Southampton', b'n']\n",
      "[0, b'male', 28.0, 0, 0, 8.4583, b'Third', b'unknown', b'Queenstown', b'y']\n",
      "[0, b'male', 2.0, 3, 1, 21.075, b'Third', b'unknown', b'Southampton', b'n']\n",
      "[1, b'female', 27.0, 0, 2, 11.1333, b'Third', b'unknown', b'Southampton', b'n']\n",
      "[1, b'female', 14.0, 1, 0, 30.0708, b'Second', b'unknown', b'Cherbourg', b'n']\n",
      "[1, b'female', 4.0, 1, 1, 16.7, b'Third', b'G', b'Southampton', b'n']\n",
      "[0, b'male', 20.0, 0, 0, 8.05, b'Third', b'unknown', b'Southampton', b'y']\n"
     ]
    }
   ],
   "source": [
    "# 이런 방식으로 각 열의 타입을 명확하게 해야합니다.\n",
    "titanic_types = [tf.int32, tf.string, tf.float32, tf.int32, tf.int32, tf.float32, tf.string,\n",
    "                 tf.string, tf.string, tf.string]\n",
    "dataset = tf.data.experimental.CsvDataset(titanic_file, titanic_types, header=True)\n",
    "\n",
    "for line in dataset.take(10):\n",
    "    print([item.numpy() for item in line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "몇몇의 열이 비어있다면, 이 저수준 인터페이스는 열 타입 대신 기본 값을 제공할 수 있도록 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting missing.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile missing.csv\n",
    "1,2,3,4\n",
    ",2,3,4\n",
    "1,,3,4\n",
    "1,2,,4\n",
    "1,2,3,\n",
    ",,,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (4,), types: tf.int32>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두개의 CSV 파일 레코드를 읽어 데이터셋을 생성합니다.\n",
    "# 각각 4개의 실수 열이 있으며, 결측값을 가지고 있을 수 있습니다.\n",
    "\n",
    "record_defaults = [999, 999, 999, 999]\n",
    "dataset = tf.data.experimental.CsvDataset('missing.csv', record_defaults)\n",
    "dataset = dataset.map(lambda *items: tf.stack(items))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n",
      "[999   2   3   4]\n",
      "[  1 999   3   4]\n",
      "[  1   2 999   4]\n",
      "[  1   2   3 999]\n",
      "[999 999 999 999]\n"
     ]
    }
   ],
   "source": [
    "for line in dataset:\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본 값으로, `CsvDataset`은 파일의 모든 행에 대한 모든 컬럼을 생성합니다. 이는 파일이 헤더나 입력으로 요구되지 않는 컬럼으로 시작될 경우에 바람직하지 않을 수도 있습니다.<br>\n",
    "이러한 행들들과 필드는 `header`와 `select_cols` 인수로 각각 제거할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (2,), types: tf.int32>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두개의 CSV 파일 레코드를 읽어 데이터셋을 생성합니다.\n",
    "# 헤더와 2, 4번째 열의 데이터를 추출합니다.\n",
    "record_defaults = [999, 999]\n",
    "dataset = tf.data.experimental.CsvDataset('missing.csv',\n",
    "                                          record_defaults,\n",
    "                                          select_cols=[1, 3])\n",
    "dataset = dataset.map(lambda *items: tf.stack(items))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4]\n",
      "[2 4]\n",
      "[999   4]\n",
      "[2 4]\n",
      "[  2 999]\n",
      "[999 999]\n"
     ]
    }
   ],
   "source": [
    "for line in dataset:\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일 집합 사용하기\n",
    "파일의 집합으로서 많은 데이터셋이 분산되어 있습니다. 각 파일은 데이터 샘플 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers_root = tf.keras.utils.get_file(\n",
    "    'flower_photos',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    untar=True)\n",
    "\n",
    "flowers_root = pathlib.Path(flowers_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 해당 이미지들은 CC-BY에 라이선스 되어있습니다. 상세한 내용은 LICENCE.txt 를 확인하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roses\n",
      "sunflowers\n",
      "daisy\n",
      "dandelion\n",
      "tulips\n",
      "LICENSE.txt\n"
     ]
    }
   ],
   "source": [
    "for item in flowers_root.glob('*'):\n",
    "    print(item.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 클래스 디렉토리의 파일은 데이터 샘플 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'/Users/kyle/.keras/datasets/flower_photos/daisy/4668543441_79040ca329_n.jpg'\n",
      "b'/Users/kyle/.keras/datasets/flower_photos/tulips/13956300996_07e64a3dbd_n.jpg'\n",
      "b'/Users/kyle/.keras/datasets/flower_photos/tulips/6905876618_12732b74de_b.jpg'\n",
      "b'/Users/kyle/.keras/datasets/flower_photos/roses/3526860692_4c551191b1_m.jpg'\n",
      "b'/Users/kyle/.keras/datasets/flower_photos/tulips/13910604778_e5f4588420.jpg'\n"
     ]
    }
   ],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))\n",
    "\n",
    "for f in list_ds.take(5):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.io.read_file` 함수를 사용하여 데이터를 읽을 수 있고, 경로로부터 라벨을 추출하여 `(image, label)`쌍을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "    label = tf.strings.split(file_path, '/')[-2]\n",
    "    return tf.io.read_file(file_path), label\n",
    "\n",
    "labeled_ds = list_ds.map(process_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xe2\\x0cXICC_PROFILE\\x00\\x01\\x01\\x00\\x00\\x0cHLino\\x02\\x10\\x00\\x00mntrRGB XYZ \\x07\\xce\\x00\\x02\\x00\\t\\x00\\x06\\x001\\x00\\x00acspMSFT\\x00\\x00\\x00\\x00IEC sRGB\\x00\\x00\\x00\\x00\\x00\\x00'\n",
      "\n",
      "b'tulips'\n"
     ]
    }
   ],
   "source": [
    "for image_raw, label_text in labeled_ds.take(1):\n",
    "    print(repr(image_raw.numpy()[:100]))\n",
    "    print()\n",
    "    print(label_text.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 원소 배치 만들기\n",
    "### 간단한 배치만들기\n",
    "가장 간단한 형태의 배치는 데이터 셋의 연속적인 `n`개의 원소를 쌓아 하나의 원소로 만드는 것 입니다. `Dataset.batch()` 변환은 명확하게 <br>\n",
    "`tf.stack()` 연산과 같은 제약조건을 가지며 각 컴포넌트의 원소에 적용됩니다.<br>\n",
    "예를 들어 컴포넌트 i에 대해 모든 원소들은 반드시 정확하게 같은 차원을 가진 텐서가 있어야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1, 2, 3]), array([ 0, -1, -2, -3])]\n",
      "[array([4, 5, 6, 7]), array([-4, -5, -6, -7])]\n",
      "[array([ 8,  9, 10, 11]), array([ -8,  -9, -10, -11])]\n",
      "[array([12, 13, 14, 15]), array([-12, -13, -14, -15])]\n"
     ]
    }
   ],
   "source": [
    "inc_dataset     = tf.data.Dataset.range(100)\n",
    "dec_dataset     = tf.data.Dataset.range(0, -100, -1)\n",
    "dataset         = tf.data.Dataset.zip((inc_dataset, dec_dataset))\n",
    "batched_dataset = dataset.batch(4)\n",
    "\n",
    "for batch in batched_dataset.take(4):\n",
    "    print([arr.numpy() for arr in batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.data`가 차원 정보 전파를 시도하는 동안에 `Data.batch`의 기본 설정 값은 알려지지 않은(unknown) 배치 사이즈를 결과로 내놓습니다.<br>\n",
    "이는 마지막 배치의 일부가 비어있을 수 있기 때문입니다. 차원에 `None`이 있는 것을 확인하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None,), (None,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`drop_remainder` 인자를 사용하여 마지막 배치를 무시하고, 전체 차원 전파를 수행하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((7,), (7,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_dataset = dataset.batch(7, drop_remainder=True)\n",
    "batched_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패딩을 추가하여 텐서 배치 만들기\n",
    "상기 방법은 같은 크기를 갖는 모든 텐서들을 위해 작동했습니다. 하지만, 많은 모델들은(예시, Sequence models) 다양한 크기를 가질 수 있는 입력 데이터로 작동됩니다.<br>\n",
    "이러한 경우를 다루기 위해, `Dataset.padded_batch()` 변환은 패딩을 추가하는 것으로 하나 또는 많은 차원을 지정하며, 다른 차원의 텐서로 배치를 만들 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [1 0 0]\n",
      " [2 2 0]\n",
      " [3 3 3]]\n",
      "\n",
      "[[4 4 4 4 0 0 0]\n",
      " [5 5 5 5 5 0 0]\n",
      " [6 6 6 6 6 6 0]\n",
      " [7 7 7 7 7 7 7]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(100)\n",
    "dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x))\n",
    "dataset = dataset.padded_batch(4, padded_shapes=(None, ))\n",
    "\n",
    "for batch in dataset.take(2):\n",
    "    print(batch.numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset.padded_batch()` 변환은 각 차원의 각 컴포넌트에 대해 다른 패딩을 부여할 수 있습니다. 그리고 고정 길이이거나 가변 길이가 될 수 있습니다.<br>\n",
    "패딩 값을 재정의 하는 것도 가능합니다. 기본 값은 0 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 워크 플로우\n",
    "### 다중 에폭 처리하기\n",
    "`tf.data` API는 같은 데이터의 다중 에폭을 처리할 주요 2가지 방법을 제공합니다.<br><br>\n",
    "다중 에폭에서 데이터셋을 반복하는 가장 쉬운 방법은 `Dataset.repeat()` 변환을 사용하는 것 입니다. 먼저 타이타닉 데이터셋을 생성해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "titatinc_lines = tf.data.TextLineDataset(titanic_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_batch_sizes(ds):\n",
    "    batch_sizes = [batch.shape[0] for batch in ds]\n",
    "    plt.bar(range(len(batch_sizes)), batch_sizes)\n",
    "    plt.xlabel('Batch number')\n",
    "    plt.ylabel('Batch size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아무 인자도 없이 `Dataset.repeat()` 변환을 적용하는 것은 입력 데이터셋을 무제한으로 반복하도록 합니다.<br><br>\n",
    "`Dataset.repeat` 변환은 한 에폭과 다음 에폭의 시작을 알리지 않고 인수를 연결합니다.<br>\n",
    "`Dataset.repeat` 이후에 적용된 `Dataset.batch`는 에폭의 경계를 넘는 배치를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAUc0lEQVR4nO3dfbRddX3n8fcH4gNgJVBuM5igySALi0wRmmIcZlwKthOQGmbqWFhqo6Urq1NaqbXlwa4Z2lVrsbRSu3TsyogSnRRLEQeqhSETYVFdgr0gEiAoqTyFBnN94GEJBQLf+ePs7B5u7k0u9+acfZP7fq1119n7d/bZv28e7vmc395n/3aqCkmSAPbpugBJ0uxhKEiSWoaCJKllKEiSWoaCJKk1r+sCZuKQQw6pxYsXd12GJO1Rbrnllu9X1chEz+3RobB48WJGR0e7LkOS9ihJ7p/sOQ8fSZJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJae/QVzTOx+Lwvz3gf91341t26z/H7G8Q+94Q/tzXunv0NYp97wp97T6hxNnOkIElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpNbAQiHJp5NsTXJHX9tFSe5OcnuSLyaZ3/fc+Uk2Jfl2kv80qLokSZMb5EjhUmD5uLZ1wNFV9TPAd4DzAZIcBZwOvLZ5zf9Msu8Aa5MkTWBgoVBVNwI/HNd2XVVta1ZvAhY1yyuAz1fVU1V1L7AJOH5QtUmSJtblOYVfBa5plhcCD/Y9t7lpkyQNUSehkOT3gW3A2mm8dlWS0SSjY2Nju784SZrDhh4KSd4DnAq8s6qqaX4IOKxvs0VN2w6qanVVLa2qpSMjIwOtVZLmmqGGQpLlwDnA26rqib6nrgZOT/KSJEuAI4BvDLM2SdIA76eQ5DLgTcAhSTYDF9D7ttFLgHVJAG6qql+vqjuTXA7cRe+w0llV9eygapMkTWxgoVBVZ0zQfMlOtv9j4I8HVY8kade8olmS1DIUJEktQ0GS1DIUJEktQ0GS1DIUJEktQ0GS1DIUJEktQ0GS1DIUJEktQ0GS1DIUJEktQ0GS1DIUJEktQ0GS1DIUJEktQ0GS1DIUJEktQ0GS1DIUJEktQ0GS1DIUJEktQ0GS1DIUJEmtgYVCkk8n2Zrkjr62g5OsS3JP83hQ054kf5lkU5Lbkxw3qLokSZMb5EjhUmD5uLbzgPVVdQSwvlkHOBk4ovlZBXxygHVJkiYxsFCoqhuBH45rXgGsaZbXAKf1tX+2em4C5ic5dFC1SZImNuxzCguqakuz/DCwoFleCDzYt93mpm0HSVYlGU0yOjY2NrhKJWkO6uxEc1UVUNN43eqqWlpVS0dGRgZQmSTNXcMOhe9tPyzUPG5t2h8CDuvbblHTJkkaomGHwtXAymZ5JXBVX/uvNN9CWgY82neYSZI0JPMGteMklwFvAg5Jshm4ALgQuDzJmcD9wDuazf8eOAXYBDwBvHdQdUmSJjewUKiqMyZ56qQJti3grEHVIkmaGq9oliS1DAVJUstQkCS1DAVJUstQkCS1DAVJUstQkCS1BnadgiRpYovP+/KM93HfhW/dDZXsyJGCJKllKEiSWoaCJKllKEiSWoaCJKllKEiSWoaCJKllKEiSWoaCJKllKEiSWoaCJKllKEiSWoaCJKllKEiSWoaCJKnVSSgkeX+SO5PckeSyJC9NsiTJzUk2JfmbJC/uojZJmst2GQpJ9k/y35P8r2b9iCSnTrfDJAuB9wFLq+poYF/gdOAjwMVV9WrgR8CZ0+1DkjQ9UxkpfAZ4CnhDs/4Q8KEZ9jsP2C/JPGB/YAtwInBF8/wa4LQZ9iFJeoGmEgqHV9WfAs8AVNUTQKbbYVU9BPwZ8AC9MHgUuAV4pKq2NZttBhZO9Pokq5KMJhkdGxubbhmSpAlMJRSeTrIfUABJDqc3cpiWJAcBK4AlwCuAA4DlU319Va2uqqVVtXRkZGS6ZUiSJjBvCtv8AXAtcFiStcAJwHtm0OdbgHuragwgyZXNPucnmdeMFhbRO0wlSRqiXYZCVV2X5BZgGb3DRmdX1fdn0OcDwLIk+wNPAicBo8D1wNuBzwMrgatm0IckaRqm8u2j9cDrq+rLVfWlqvp+ktXT7bCqbqZ3QvlWYENTw2rgXOB3kmwCfhK4ZLp9SJKmZyqHj5YA5yb5uar6w6Zt6Uw6raoLgAvGNX8XOH4m+5UkzcxUTjQ/Qu8Qz4Ikf5fkwAHXJEnqyFRCIVW1rap+A/gC8FXgpwZbliSpC1M5fPRX2xeq6tIkG4CzBleSJKkrk4ZCkpdX1WPA3yY5uO+pe4HfHXhlkqSh29lI4a+BU+ldbVw8/yrmAv7tAOuSJHVg0lCoqlObxyXDK0eS1KWpXKdwQpIDmuV3JfloklcOvjRJ0rBN5dtHnwSeSHIM8AHgn4DPDbQqSVInphIK26qq6E1i9/Gq+gTwE4MtS5LUhal8JfXxJOcD7wLemGQf4EWDLUuS1IWpjBR+md5U2WdW1cP0ZjC9aKBVSZI6MZVZUh8GPtq3/gDw2UEWJUnqxlRGCpKkOcJQkCS1DAVJUmuX5xSSnEDvlpyvarYPUFXlNBeStJeZyldSLwHeT28OpGcHW44kqUtTCYVHq+qagVciSerczqbOPq5ZvD7JRcCV9K5XAKCqbh1wbZKkIdvZSOHPx63335e5gBN3fzmSpC7tbOrsNw+zEElS96YydfaHk8zvWz8oyYcGW5YkqQtTuU7h5Kp6ZPtKVf0IOGVwJUmSujKVUNg3yUu2ryTZD3jJTraXJO2hphIKa4H1Sc5MciawjhlOiJdkfpIrktydZGOSNyQ5OMm6JPc0jwfNpA9J0gu3y1Coqo8AHwJ+uvn5o6ZtJj4GXFtVrwGOATYC5wHrq+oIYH2zLkkaoqlMc/GRqjoXuHaCthcsyYHAG4H3AFTV08DTSVYAb2o2WwPcAEyrD0nS9Ezl8NHPT9B28gz6XAKMAZ9J8s0kn0pyALCgqrY02zwMLJjoxUlWJRlNMjo2NjaDMiRJ400aCkn+W5INwJFJbu/7uRe4fQZ9zgOOAz5ZVccCP2bcoaLmntA10YuranVVLa2qpSMjIzMoQ5I03s4OH/01cA3wJzz/TfvxqvrhDPrcDGyuqpub9Sua/X8vyaFVtSXJocDWGfQhSZqGSUcKVfVoVd1XVWdU1f3Ak/Q+vb8sySun22Fze88HkxzZNJ0E3AVcDaxs2lYCV023D0nS9EzlRPMv0rtH8yvofXp/Fb1vC712Bv3+FrA2yYuB7wLvpRdQlzdfe70feMcM9i9JmoapTJ39IWAZ8P+q6tgkbwbeNZNOq+o2nj/B3nYnzWS/kqSZmcq3j56pqh8A+yTZp6quZ+I3dEnSHm4qI4VHkrwMuJHeIZ+t9L4xJEnay0xlpLACeILeLTmvBf4J+MVBFiVJ6sYuRwpVtX1U8FySLwM/aK4jkCTtZXZ28dqyJDckuTLJsUnuAO6gdz3B8uGVKEkalp2NFD4OfBA4EPgKvfsq3JTkNcBl9M2FJEnaO+zsnMK8qrquqv4WeLiqbgKoqruHU5okadh2FgrP9S0/Oe45zylI0l5oZ4ePjknyGBBgv2aZZv2lA69MkjR0k4ZCVe07zEIkSd2bynUKkqQ5wlCQJLUMBUlSy1CQJLUMBUlSy1CQJLUMBUlSy1CQJLUMBUlSy1CQJLUMBUlSy1CQJLUMBUlSq7NQSLJvkm8m+VKzviTJzUk2JfmbJC/uqjZJmqu6HCmcDWzsW/8IcHFVvRr4EXBmJ1VJ0hzWSSgkWQS8FfhUsx7gROCKZpM1wGld1CZJc1lXI4W/AM7hX2/5+ZPAI1W1rVnfDCyc6IVJViUZTTI6NjY2+EolaQ4ZeigkORXYWlW3TOf1VbW6qpZW1dKRkZHdXJ0kzW07u0fzoJwAvC3JKfTu9fxy4GPA/CTzmtHCIuChDmqTpDlt6COFqjq/qhZV1WLgdOArVfVO4Hrg7c1mK4Grhl2bJM11s+k6hXOB30myid45hks6rkeS5pwuDh+1quoG4IZm+bvA8V3WI0lz3WwaKUiSOmYoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahoIkqTX0UEhyWJLrk9yV5M4kZzftBydZl+Se5vGgYdcmSXNdFyOFbcAHquooYBlwVpKjgPOA9VV1BLC+WZckDdHQQ6GqtlTVrc3y48BGYCGwAljTbLYGOG3YtUnSXNfpOYUki4FjgZuBBVW1pXnqYWDBJK9ZlWQ0yejY2NhQ6pSkuaKzUEjyMuALwG9X1WP9z1VVATXR66pqdVUtraqlIyMjQ6hUkuaOTkIhyYvoBcLaqrqyaf5ekkOb5w8FtnZRmyTNZV18+yjAJcDGqvpo31NXAyub5ZXAVcOuTZLmunkd9HkC8G5gQ5LbmrYPAhcClyc5E7gfeEcHtUnSnDb0UKiqrwKZ5OmThlmLJOn5vKJZktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJrVkXCkmWJ/l2kk1Jzuu6HkmaS2ZVKCTZF/gEcDJwFHBGkqO6rUqS5o5ZFQrA8cCmqvpuVT0NfB5Y0XFNkjRnpKq6rqGV5O3A8qr6tWb93cDrq+o3+7ZZBaxqVo8Evj3Akg4Bvj/A/e8O1rh7WOPuYY27zyDrfFVVjUz0xLwBdTgwVbUaWD2MvpKMVtXSYfQ1Xda4e1jj7mGNu09Xdc62w0cPAYf1rS9q2iRJQzDbQuEfgSOSLEnyYuB04OqOa5KkOWNWHT6qqm1JfhP4v8C+wKer6s4OSxrKYaoZssbdwxp3D2vcfTqpc1adaJYkdWu2HT6SJHXIUJAktQyFCcz2qTaSHJbk+iR3Jbkzydld1zSZJPsm+WaSL3Vdy2SSzE9yRZK7k2xM8oauaxovyfubf+s7klyW5KWzoKZPJ9ma5I6+toOTrEtyT/N40Cys8aLm3/r2JF9MMn+21dj33AeSVJJDhlWPoTDOHjLVxjbgA1V1FLAMOGsW1rjd2cDGrovYhY8B11bVa4BjmGX1JlkIvA9YWlVH0/sSxundVgXApcDycW3nAeur6ghgfbPepUvZscZ1wNFV9TPAd4Dzh13UOJeyY40kOQz4BeCBYRZjKOxo1k+1UVVbqurWZvlxem9iC7utakdJFgFvBT7VdS2TSXIg8EbgEoCqerqqHum2qgnNA/ZLMg/YH/jnjuuhqm4EfjiueQWwplleA5w21KLGmajGqrquqrY1qzfRux6qM5P8PQJcDJwDDPXbQIbCjhYCD/atb2YWvuFul2QxcCxwc7eVTOgv6P2nfq7rQnZiCTAGfKY5zPWpJAd0XVS/qnoI+DN6nxi3AI9W1XXdVjWpBVW1pVl+GFjQZTFT8KvANV0XMV6SFcBDVfWtYfdtKOzBkrwM+ALw21X1WNf19EtyKrC1qm7pupZdmAccB3yyqo4Ffkz3hzyepzkuv4JegL0COCDJu7qtateq9333Wfud9yS/T+9Q7Nqua+mXZH/gg8D/6KJ/Q2FHe8RUG0leRC8Q1lbVlV3XM4ETgLcluY/eIbgTk/zvbkua0GZgc1VtH2ldQS8kZpO3APdW1VhVPQNcCfz7jmuazPeSHArQPG7tuJ4JJXkPcCrwzpp9F2sdTu8DwLea359FwK1J/s0wOjcUdjTrp9pIEnrHwDdW1Ue7rmciVXV+VS2qqsX0/g6/UlWz7tNtVT0MPJjkyKbpJOCuDkuayAPAsiT7N//2JzHLTob3uRpY2SyvBK7qsJYJJVlO77Dm26rqia7rGa+qNlTVT1XV4ub3ZzNwXPN/deAMhXGaE1Dbp9rYCFze8VQbEzkBeDe9T9+3NT+ndF3UHuy3gLVJbgdeB3y443qepxnFXAHcCmyg93vb+VQNSS4Dvg4cmWRzkjOBC4GfT3IPvRHOhbOwxo8DPwGsa353/moW1thdPbNv5CRJ6oojBUlSy1CQJLUMBUlSy1CQJLUMBUlSy1DQXiXJs83XDL+V5NYkO73Iq5kh9TemsN8bknR2s/ck9w1zpkzNXYaC9jZPVtXrquoYerNf/skutp8P7DIU9mTNJHrSlBgK2pu9HPgR9OaJSrK+GT1saCYcg97FVYc3o4uLmm3Pbbb5VpL+i6/+a5JvJPlOkv84vrMkb2pGFNvvzbC2uQL5eZ/0kyxNckOz/AdJ1iT5hyT3J/kvSf606f/aZjqT7c5p2r+R5NXN60eSfCHJPzY/J/Tt93NJvgZ8bjf+nWov5ycI7W32S3Ib8FLgUODEpv1fgP9cVY81b843Jbma3uR3R1fV6wCSnExv8rnXV9UTSQ7u2/e8qjq+uXr8AnpX7I53LPBaelNbf43e1edf3UXNhwNvpnf/jq8Dv1RV5yT5Ir2px/9Ps92jVfXvkvwKvRloT6V3L4iLq+qrSV5J70r8n262Pwr4D1X15C76l1qGgvY2T/a9wb8B+GySo4EAH07yRnpTeS9k4mmd3wJ8ZvucOFXVP8/99okHbwEWT9L/N6pqc9P/bc12uwqFa6rqmSQb6N1A59qmfcO4fi7re7y4r96jmgEJwMub2XMBrjYQ9EIZCtprVdXXm1HBCHBK8/izzRvwffRGEy/EU83js0z+u/NU33L/dtv418O14/t9qqn3uSTP9M3a+dy4fmqC5X2AZVX1L/07bELix5P+SaRJeE5Be60kr6H3yfsHwIH07u/wTJI3A69qNnuc3uRo260D3tvMac+4w0czcR/ws83yL01zH7/c9/j1Zvk6ehP6AZDkddPctwQ4UtDeZ/s5BegdMlpZVc8mWQv8XXOIZhS4G6CqfpDka+ndNP2aqvq95o11NMnTwN/Tu+HJTP0hcEmSPwJumOY+Dmpmcn0KOKNpex/wiaZ9HnAj8OszrFVzmLOkSpJaHj6SJLUMBUlSy1CQJLUMBUlSy1CQJLUMBUlSy1CQJLX+P4pBq0r2sPeeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic_batches = titanic_lines.repeat(3).batch(128)\n",
    "plot_batch_sizes(titanic_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 에폭의 명확한 분리가 필요하다면, `Dataset.batch`를 repeat 이전에 적용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "titanic_batches = titanic_lines.batch(128).repeat(3)\n",
    "\n",
    "plot_batch_sizes(titanic_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 각 에폭의 마지막에서 사용자 지정 계산(통계 수집)을 수행하고 싶다면, 각 에폭의 데이터 셋 반복을 재시작하는 것이 가장 간단합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(116,)\n",
      "End of epoch:  0\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(116,)\n",
      "End of epoch:  1\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(116,)\n",
      "End of epoch:  2\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "dataset = titanic_lines.batch(128)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataset:\n",
    "        print(batch.shape)\n",
    "    print('End of epoch: ', epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyle",
   "language": "python",
   "name": "kyle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
