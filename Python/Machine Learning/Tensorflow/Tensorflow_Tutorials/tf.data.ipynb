{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data: 텐서플로우 입력 파이프라인 만들기\n",
    "##### url: https://www.tensorflow.org/guide/data\n",
    "\n",
    "- 목차\n",
    "    - 기본 구조\n",
    "        - 데이터셋 구조\n",
    "    - 입력 데이터 불러오기\n",
    "        - 넘파이 행렬 사용하기\n",
    "        - 파이썬 제너레이터 사용하기\n",
    "        - TFRecord 데이터 사용하기\n",
    "        - 텍스트 데이터 사용하기\n",
    "        - CSV 데이터 사용하기\n",
    "        - 데이터 집합 사용하기\n",
    "    - 데이터 원소 배치 만들기\n",
    "        - 간단한 배치 만들기\n",
    "        - 패딩과 함께 텐서 배치 만들기\n",
    "    - 학습 흐름\n",
    "        - 다중 에폭 수행하기\n",
    "        - 입력 데이터를 임의로 섞기\n",
    "    - 데이터 전처리\n",
    "        - 이미지 데이터 복호화 및 리사이즈하기\n",
    "        - 임의의 파이썬 함수 적용하기\n",
    "        - tf.Example 프로토콜 버퍼 메시지 파싱하기\n",
    "        - 시계열 윈도우 적용하기\n",
    "        - 리샘플링\n",
    "    - 고수준 API 사용하기\n",
    "        - tf.keras\n",
    "        - tf.estimator\n",
    "\n",
    "`tf_data` API는 간단하고 재사용이 가능한 조각을 통해 복잡한 입력 파이프라인을 만들 수 있게 해줍니다.<br>\n",
    "예를 들어 이미지 모델을 위한 파이프라인은 파일 시스템에 분산되어 있는 파일들을 모아주고, 각 이미지에 임의성을 부여하며 학습 배치로 사용될 선택된 이미지를 임의로 병합할 수 있습니다.<br>\n",
    "텍스트 모델을 위한 파이프라인은 원본 텍스트 데이터에서 심볼을 추출하는 것을 포함해 룩업 테이블과 함께 임베딩 식별자로 변환하거나 서로 다른 길이를 가진 시퀀스로 배치를 만들 수 있습니다.<br>\n",
    "`tf.data` API는 거대한 양의 데이터를 다룰 수 있게 만들어주며, 서로 다른 데이터 포맷을 읽거나 복잡한 변환을 수행할 수 있습니다.<br><br>\n",
    "`tf.data` API는 하나 또는 그 이상의 컴포넌트로 구성된 각 원소의 시퀀스를 표현하는 `tf.data.Dataset` 추상화를 도입합니다. <br>\n",
    "예를 들어, 이미지 파이프라인에서 이미지와 라벨로 표현된 텐서 컴포넌트의 쌍으로 원소들은 하나의 학습 샘플이 될 수 있습니다.\n",
    "\n",
    "- 데이터 소스는 메모리에 저장되어 있거나 하나 또는 그 이상의 파일로 저장된 데이터로부터 `Dataset` 객체를 만듭니다.\n",
    "- 데이터 변환은 하나 또는 그 이상의 `tf.data.Dataset` 객체로부터 데이터셋을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 구조\n",
    "입력 파이프라인을 만드려면, 반드시 데이터 소스와 함께 시작해야 합니다. <br>\n",
    "예를 들어, 메모리 내의 데이터로부터 `Dataset`를 만들기 위해서는 `tf.data.Dataset.from_tensors()` 함수 또는 `tf.data.Dataset.from_tensor_slices()`함수를 사용할 수 있습니다.<br>\n",
    "대안으로, 입력 데이터가 권장되는 TFRecord 포맷의 파일로 저장되어 있다면 `tf.data.TFRecordDataset()` 함수를 사용할 수 있습니다.<br><br>한번 `Dataset` 객체를 만들게 되면, `tf.data.Dataset` 객체의 연쇄 메소드를 통해 새로운 `Dataset`으로 변환할 수 있습니다.<br>\n",
    "예를 들어, 단일 원소에 적용되는 `Dataset.map()` 함수와 다중 원소에 적용되는 `Dataset.batch()` 변환을 수행할 수 있습니다.<br><br>\n",
    "`Dataset` 객체는 '파이썬 이터러블(Iterable)' 객체이기 때문에 각 원소를 for loop을 사용해 모델에 입력할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "3\n",
      "0\n",
      "8\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for elem in dataset:\n",
    "    print(elem.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아니면 직접적으로 `iter` 함수를 사용해 파이썬 이터레이터를 사용하고 `next` 함수를 사용해 원소를 소진 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "it = iter(dataset)\n",
    "\n",
    "print(next(it).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대안으로 데이터셋 원소는 `reduce` 변환을 사용해 소진될 수 있으며, 해당 변환은 단일 결과를 생성하기위해 모든 원소를 압축합니다.<br>\n",
    "다음 예제를 통해 데이터셋의 정수 합을 계산하는 `reduce` 변환이 어떻게 사용되는지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(dataset.reduce(0, lambda state, value: state + value).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터셋 구조\n",
    "데이터셋은 내재적으로 같은 구조를 가지고 있는 원소와 `Tensor`, `SparseTensor`, `ReggedTensor`, `TensorArray`, `Dataset`처럼 `tf.TypeSpec`에 의해 표현되는 타입 속성 구조를 가진 각각의 컴포넌트로 이루어져 있습니다.<br><br>\n",
    "`Dataset.element_spec` 프로퍼티는 각 원소 컴포넌트의 타입을 검사할 수 있게 해줍니다. <br>\n",
    "해당 프로퍼티는 원소의 구조를 매칭하기 위한 `tf.TypeSpec` 객체의 내부 내재된 구조를 반환하며 원소의 구조는 단일 컴포넌트, 튜플 컴포넌트 또는 내재된 튜풀 컴포넌트일 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(10,), dtype=tf.float32, name=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단일 컴포넌트\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10]))\n",
    "\n",
    "dataset1.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
       " TensorSpec(shape=(100,), dtype=tf.int32, name=None))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 튜플 컴포넌트\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.random.uniform([4]),\n",
    "     tf.random.uniform([4, 100], maxval=100, dtype=tf.int32))\n",
    ")\n",
    "\n",
    "dataset2.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(10,), dtype=tf.float32, name=None),\n",
       " (TensorSpec(shape=(), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(100,), dtype=tf.int32, name=None)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 내재된 튜플 컴포넌트\n",
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "\n",
    "dataset3.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseTensorSpec(TensorShape([3, 4]), tf.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 희소 텐서를 포함하는 데이터셋\n",
    "dataset4 = tf.data.Dataset.from_tensors(\n",
    "   tf.SparseTensor(indices=[[0, 0], [1, 2]],\n",
    "                   values=[1, 2],\n",
    "                   dense_shape=[3, 4]\n",
    "                  )\n",
    ")\n",
    "dataset4.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.sparse_tensor.SparseTensor"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value_type를 사용해서 element spec에 의해 표현되는 값의 타입 살펴보기\n",
    "dataset4.element_spec.value_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset` 변환은 모든 구조의 데이터셋을 지원합니다. <br>\n",
    "각 원소에 적용 되는 함수인 `Dataset.map()` 또는 `Dataset.filter()` 변환을 사용할 떄, 원소 구조는 함수의 인자로 결정됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (10,), types: tf.int32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1 = tf.data.Dataset.from_tensor_slices(\n",
    "    tf.random.uniform([4, 10], minval=1, maxval=10, dtype=tf.int32)\n",
    ")\n",
    "dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1 4 5 3 8 8 3 2 8]\n",
      "[3 7 7 6 6 6 2 3 6 4]\n",
      "[3 4 7 2 6 7 8 1 1 1]\n",
      "[6 1 4 3 6 5 8 1 6 8]\n"
     ]
    }
   ],
   "source": [
    "for z in dataset1:\n",
    "    print(z.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((), (100,)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
    "    (tf.random.uniform([4]),\n",
    "     tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)\n",
    "    \n",
    "    )\n",
    ")\n",
    "\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ZipDataset shapes: ((10,), ((), (100,))), types: (tf.int32, (tf.float32, tf.int32))>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
    "\n",
    "dataset3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: (10,), (), (100,)\n",
      "shapes: (10,), (), (100,)\n",
      "shapes: (10,), (), (100,)\n",
      "shapes: (10,), (), (100,)\n"
     ]
    }
   ],
   "source": [
    "for a, (b, c) in dataset3:\n",
    "    print(f'shapes: {a.shape}, {b.shape}, {c.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력 데이터 읽어오기\n",
    "#### 넘파이 행렬 읽어오기\n",
    "더 많은 예제는 [Loading NumPy arrays](https://www.tensorflow.org/tutorials/load_data/numpy)를 확인하세요.<br><br>\n",
    "만약 모든 입력 데이터가 메모리에 올라갈 수 있다면, `Dataset`을 만드는 가장 쉬운 방법은 `tf.Tensor` 객체로 변환하거나, `Dataset.from_tensor_slices()` 함수를 사용하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((28, 28), ()), types: (tf.float64, tf.uint8)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, labels = train\n",
    "images = images/255.0\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: 위의 코드 스니펫은 변수(feature)와 라벨(label) 행렬을 `tf.constant()` 연산으로써 텐서플로우 그래프에 포함합니다. 이러한 작업은 작은 데이터셋에는 유용할 수 있으나, 여러번에 걸쳐 행렬의 내용을 복사하므로 메모리를 낭비할 수 있습니다. 그리고 2GB로 제한된 `tf.GraphDef` 프로토콜 버퍼의 크기에 도달할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파이썬 제너레이터로 읽어오기\n",
    "다른 `tf.data.Dataset`으로써 쉽게 수집될 수 있는 일반적인 데이터 소스는 파이썬 제너레이터가 있습니다.\n",
    "\n",
    "> 주의: 이것은 이식성과 확장성을 제한할 수 있는 가장 편리한 접근 방법이지만, 이러한 작업은 반드시 제너레이터를 생성한 같은 파이썬 프로세스에서 실행되어야하며, 여전히 파이썬 GIL이 적용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(stop):\n",
    "    i = 0\n",
    "    while i<stop :\n",
    "        yield i\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for n in count(5):\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset.from_generator` 생성자는 파이썬 제너레이터를 완전히 함수적인 `tf.data.Dataset`으로 변환합니다.<br><br>\n",
    "그 생성자는 입력으로써 iterator가 아닌 callable을 사용합니다. 이를 통해 제너레이터가 끝에 다다랐을 때 재시작될 수 있도록 합니다.<br>\n",
    "선택적으로 callable의 인자로서 넘겨줄 수 있도록 `args` 인수를 사용할 수 있습니다.<br><br>\n",
    "`output_types` 인수는 내부적으로 `tf.data`가 `tf.Graph`를 만들기 때문에 요구됩니다. 그리고 그래프 엣지는 `tf.dtype`을 요구합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_counter = tf.data.Dataset.from_generator(count,\n",
    "                                            args=[25],\n",
    "                                            output_types=tf.int32,\n",
    "                                            output_shapes = (), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[10 11 12 13 14 15 16 17 18 19]\n",
      "[20 21 22 23 24  0  1  2  3  4]\n",
      "[ 5  6  7  8  9 10 11 12 13 14]\n",
      "[15 16 17 18 19 20 21 22 23 24]\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[10 11 12 13 14 15 16 17 18 19]\n",
      "[20 21 22 23 24  0  1  2  3  4]\n",
      "[ 5  6  7  8  9 10 11 12 13 14]\n",
      "[15 16 17 18 19 20 21 22 23 24]\n"
     ]
    }
   ],
   "source": [
    "for count_batch in ds_counter.repeat().batch(10).take(10):\n",
    "    print(count_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`outpyt_shapes` 인수는 요구되지 않습니다. 하지만 많은 텐서플로우 연산들이 알려지지 않은(unknown) 랭크를 지원하지 않기 때문에 입력하는 것을 강력히 권해드립니다.<br>\n",
    "만약 특정한 축의 길이가 알려지지 않거나 또는 변할 수 있다면, `output_shape`에 `None` 을 세팅합니다.<br><br>\n",
    "또한, `output_shapes`와 `output_types`는 다른 데이터 메소드와서도 동일한 중첩 규칙을 따른다는 것도 중요합니다.<br><br>\n",
    "여기에 두 측면을 살펴볼 제너레이터 예제가 있습니다. 이 제너레이터는 행렬의 튜플을 반환하며, 두번째 행렬은 길이가 알려지지 않은 벡터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_series():\n",
    "    i = 0\n",
    "    while True:\n",
    "        size = np.random.randint(0, 10)\n",
    "        yield i, np.random.normal(size=(size,))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : [-0.3067  0.201 ]\n",
      "1 : [ 0.7493  0.0074 -2.5439 -1.1259  0.6635]\n",
      "2 : [0.4542]\n",
      "3 : [-0.1142 -0.1459  0.3864 -0.2247 -0.5987 -1.5147  0.3366  1.156 ]\n",
      "4 : [-0.6354 -0.1216  2.5141  1.1198  0.8809  0.2903 -0.1733]\n",
      "5 : [ 0.7937  0.5675  0.3085 -1.1925]\n",
      "6 : [0.9524]\n"
     ]
    }
   ],
   "source": [
    "for i, series in gen_series():\n",
    "    print(i, ':', str(series))\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 출력 값은 `int32`이고, 두 번째 값은 `float32`<br><br>\n",
    "첫 번째 아이템은 `()` 차원의 스칼라이며, 두 번째는 알려지지 않은 길이인 `(None, )`차원의 벡터 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((), <unknown>), types: (tf.int32, tf.float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_series = tf.data.Dataset.from_generator(\n",
    "    gen_series,\n",
    "    output_types=(tf.int32, tf.float32),\n",
    "    output_shapes=((), (None)))\n",
    "\n",
    "ds_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 일반적인 `tf.data.Dataset`처럼 사용될 수 있습니다. 변화할 수 있는 차원을 가진 데이터셋을 배치로 만들 때, `Dataset.padded_batch`를 사용할 필요가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9  0 15 17  7  8 13  2 18 28]\n",
      "\n",
      "[[ 6.7815e-01 -5.1399e-01 -8.0192e-01  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00]\n",
      " [-7.5827e-01  6.5268e-01  1.0872e+00 -4.8418e-02 -9.3717e-01 -6.2134e-02\n",
      "  -1.0863e+00]\n",
      " [ 9.7172e-01  2.0246e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00]\n",
      " [-9.9467e-01 -1.2203e+00 -8.0778e-01  2.1094e-01 -2.4404e+00 -2.4722e-02\n",
      "   1.6860e-01]\n",
      " [-1.2260e+00  6.8184e-01  1.6287e-03  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00]\n",
      " [ 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00]\n",
      " [ 4.0038e-01  8.4403e-01 -2.8068e-01 -1.1095e+00 -3.0465e-01  5.7514e-01\n",
      "   0.0000e+00]\n",
      " [ 1.9293e-01 -6.8509e-01 -9.0497e-01  1.1282e-01  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00]\n",
      " [-3.6144e-01  1.0965e+00  4.4853e-01  3.9100e-01  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00]\n",
      " [ 7.1206e-01 -4.4640e-01  4.4542e-01  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "   0.0000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# padded_batch를 사용해서 배치 사이즈를 통해 일정한 길이로 맞춰줄 수 있습니다.\n",
    "ds_series_batch = ds_series.shuffle(20).padded_batch(10, padded_shapes=([], [None]))\n",
    "\n",
    "ids, sequence_batch = next(iter(ds_series_batch))\n",
    "print(ids.numpy(), end='\\n\\n')\n",
    "print(sequence_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 현실적인 예제로, `tf.data.Dataset`로써 `preprocessing.image.ImageDataGenerator`로 래핑을 시도해보겠습니다.<br><br>\n",
    "먼저 데이터를 다운받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers = tf.keras.utils.get_file(\n",
    "    'flower_photos',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    untar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`image.ImageDataGenerator`를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(img_gen.flow_from_directory(flowers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (32, 256, 256, 3)\n",
      "float32 (32, 5)\n"
     ]
    }
   ],
   "source": [
    "print(images.dtype, images.shape)\n",
    "print(labels.dtype, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((32, 256, 256, 3), (32, 5)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = tf.data.Dataset.from_generator(\n",
    "    img_gen.flow_from_directory, \n",
    "    args=[flowers],\n",
    "    output_types=(tf.float32, tf.float32), \n",
    "    output_shapes=([32,256,256,3], [32,5])\n",
    ")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecord 데이터 사용하기\n",
    "\n",
    "End-to-end 예제는 [Loading TFRecords](https://www.tensorflow.org/tutorials/load_data/tf_records)를 살펴보세요.<br><br>\n",
    "`tf.data` API는 다양한 파일 포맷을 지원합니다. 그래서 데이터 셋이 메모리에 전부 적재되지 않더라도 크기가 큰 데이터셋을 처리할 수 있습니다.<br>\n",
    "예를 들어, TFRecord 파일 포맷은 많은 텐서플로우 어플리케이션이 학습 데이터에 사용하는 간단한 레코드-지향 이진 포맷입니다.<br>\n",
    "`tf.data.TFRecordDataset` 클래스는 입력 파이프라인의 일부로써 하나 또는 그 이상의 TFRecord 파일의 컨텐츠를 스트림할 수 있도록 해줍니다.<br><br>\n",
    "French Street Name Signs(FSNS) 데이터셋의 테스트 파일을 사용한 예제를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 개의 파일의 예제의 전부를 불러와 데이터셋을 생성합니다.\n",
    "fsns_test_file = tf.keras.utils.get_file(\"fsns.tfrec\", \n",
    "                                         \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.data.TFRecordDataset`의 `filename` 인자는 문자열이거나 문자열의 리스트 또는, `tf.Tensor` 문자열일 수도 있습니다.<br>\n",
    "그러므로 학습과 검증의 목적으로 두개로 이루어진 파일을 가지고 있다면, 입력 인자로써 파일 이름을 사용해 데이터셋을 생성하기 위한 팩토리 메소드를 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TFRecordDatasetV2 shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "많은 텐서플로우 프로젝트는 TFRecord 파일에 있는 직렬화된 `tf.train.Example` 레코드를 사용합니다.<br>\n",
    "해당 레코드들은 확인되기 전에, 복호화가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes_list {\n",
       "  value: \"Rue Perreyon\"\n",
       "}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_example = next(iter(dataset))\n",
    "parsed = tf.train.Example.FromString(raw_example.numpy())\n",
    "\n",
    "parsed.features.feature['image/text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 데이터 사용하기\n",
    "\n",
    "End-to-End 예제는 [Loading Text](https://www.tensorflow.org/tutorials/load_data/text)를 살펴보세요.<br><br>\n",
    "많은 데이터셋이 하나 또는 그 이상의 텍스트 파일로 분산되어 있습니다.<br>\n",
    "`tf.data.TextLineDataset`은 하나 또는 이상의 텍스트 파일로부터 행을 추출하는 가장 쉬운 방법을 제공합니다을<br>\n",
    "하나 또는 더 많은 파일 이름이 주어지면, `TextLineDataset`은 해당 파일의 행 당 하나의 문자열-값 원소를 생성하게됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_url = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
    "file_names = ['cowper.txt', 'derby.txt', 'butler.txt']\n",
    "\n",
    "file_paths = [\n",
    "    tf.keras.utils.get_file(file_name, directory_url + file_name) for file_name in file_names\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 파일의 앞의 몇 행을 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus' son;\"\n",
      "b'His wrath pernicious, who ten thousand woes'\n",
      "b\"Caused to Achaia's host, sent many a soul\"\n",
      "b'Illustrious into Ades premature,'\n",
      "b'And Heroes gave (so stood the will of Jove)'\n"
     ]
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 파일들 사이에서 행이 번갈아 일어나도록 `Dataset,interleave` 함수를 사용합니다.<br>\n",
    "이는 파일이 손쉽게 섞일 수 있도록 만들어줍니다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "b\"\\xef\\xbb\\xbfAchilles sing, O Goddess! Peleus' son;\"\n",
      "b\"\\xef\\xbb\\xbfOf Peleus' son, Achilles, sing, O Muse,\"\n",
      "b'\\xef\\xbb\\xbfSing, O goddess, the anger of Achilles son of Peleus, that brought'\n",
      "\n",
      "b'His wrath pernicious, who ten thousand woes'\n",
      "b'The vengeance, deep and deadly; whence to Greece'\n",
      "b'countless ills upon the Achaeans. Many a brave soul did it send'\n",
      "\n",
      "b\"Caused to Achaia's host, sent many a soul\"\n",
      "b'Unnumbered ills arose; which many a soul'\n",
      "b'hurrying down to Hades, and many a hero did it yield a prey to dogs and'\n"
     ]
    }
   ],
   "source": [
    "files_ds = tf.data.Dataset.from_tensor_slices(file_paths)\n",
    "lines_ds = files_ds.interleave(tf.data.TextLineDataset, cycle_length=3)\n",
    "\n",
    "for i, line in enumerate(lines_ds.take(9)):\n",
    "    if i % 3 == 0:\n",
    "        print()\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본 값으로 `TextLineDataset`은 각 파일의 모든 행을 생성하므로 파일이 주석을 포함하고 있거나 헤더 라인부터 시작된다면, 이는 바람직하지 않을 수 있습니다.<br>\n",
    "그런 행들은 `Dataset.skip()` 또는 `Dataset.filter()` 변환을 사용하여 제거할 수 있습니다.<br>\n",
    "첫 번째 행을 건너뛰고 생존자만을 필터링하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_file = tf.keras.utils.get_file(\"train.csv\", \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")\n",
    "titanic_lines = tf.data.TextLineDataset(titanic_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone'\n",
      "b'0,male,22.0,1,0,7.25,Third,unknown,Southampton,n'\n",
      "b'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n'\n",
      "b'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y'\n",
      "b'1,female,35.0,1,0,53.1,First,C,Southampton,n'\n",
      "b'0,male,28.0,0,0,8.4583,Third,unknown,Queenstown,y'\n",
      "b'0,male,2.0,3,1,21.075,Third,unknown,Southampton,n'\n",
      "b'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n'\n",
      "b'1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n'\n",
      "b'1,female,4.0,1,1,16.7,Third,G,Southampton,n'\n"
     ]
    }
   ],
   "source": [
    "for line in titanic_lines.take(10):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def survived(line):\n",
    "    return tf.not_equal(tf.strings.substr(line, 0, 1), '0')\n",
    "\n",
    "survivors = titanic_lines.skip(1).filter(survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'1,female,38.0,1,0,71.2833,First,C,Cherbourg,n'\n",
      "b'1,female,26.0,0,0,7.925,Third,unknown,Southampton,y'\n",
      "b'1,female,35.0,1,0,53.1,First,C,Southampton,n'\n",
      "b'1,female,27.0,0,2,11.1333,Third,unknown,Southampton,n'\n",
      "b'1,female,14.0,1,0,30.0708,Second,unknown,Cherbourg,n'\n",
      "b'1,female,4.0,1,1,16.7,Third,G,Southampton,n'\n",
      "b'1,male,28.0,0,0,13.0,Second,unknown,Southampton,y'\n",
      "b'1,female,28.0,0,0,7.225,Third,unknown,Cherbourg,y'\n",
      "b'1,male,28.0,0,0,35.5,First,A,Southampton,y'\n",
      "b'1,female,38.0,1,5,31.3875,Third,unknown,Southampton,n'\n"
     ]
    }
   ],
   "source": [
    "for line in survivors.take(10):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV 데이터 사용하기\n",
    "더 많은 예제는 [Loading CSV Files](https://www.tensorflow.org/tutorials/load_data/csv)와 [Loading Pandas DataFrames](https://www.tensorflow.org/tutorials/load_data/pandas)를 확인하세요.<br><br>\n",
    "\n",
    "CSV는 일반 텍스트로 이루어진 정형 데이터를 저장하기 위한 인기있는 파일 포맷입니다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_file = tf.keras.utils.get_file(\"train.csv\", \n",
    "                                       \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived     sex   age  n_siblings_spouses  parch     fare  class     deck  \\\n",
       "0         0    male  22.0                   1      0   7.2500  Third  unknown   \n",
       "1         1  female  38.0                   1      0  71.2833  First        C   \n",
       "2         1  female  26.0                   0      0   7.9250  Third  unknown   \n",
       "3         1  female  35.0                   1      0  53.1000  First        C   \n",
       "4         0    male  28.0                   0      0   8.4583  Third  unknown   \n",
       "\n",
       "   embark_town alone  \n",
       "0  Southampton     n  \n",
       "1    Cherbourg     n  \n",
       "2  Southampton     y  \n",
       "3  Southampton     n  \n",
       "4   Queenstown     y  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(titanic_file, index_col=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 데이터 파일을 모두 메모리에 적재할 수 있다면, 동일한 `Dataset.from_tensor_slices` 메소드에 딕셔너리 형태로 전달하여 작동시킵니다.<br>\n",
    "이는 데이터를 쉽게 불러올 수 있도록 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   'survived'          : 0\n",
      "   'sex'               : b'male'\n",
      "   'age'               : 22.0\n",
      "   'n_siblings_spouses': 1\n",
      "   'parch'             : 0\n",
      "   'fare'              : 7.25\n",
      "   'class'             : b'Third'\n",
      "   'deck'              : b'unknown'\n",
      "   'embark_town'       : b'Southampton'\n",
      "   'alone'             : b'n'\n"
     ]
    }
   ],
   "source": [
    "titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))\n",
    "\n",
    "for feature_batch in titanic_slices.take(1):\n",
    "    for key, value in feature_batch.items():\n",
    "        print('   {!r:20s}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더욱 확장성있는 접근 방법은 필요할 때 디스크에서 불러오는 방법이 있습니다.<br><br>\n",
    "`tf.data` 모듈은 `RFC 4180`을 따르는 하나 또는 그 이상의 CSV 파일로부터 레코드를 추출하는 메소드를 제공합니다.<br><br>\n",
    "`experimental.make_csv_dataset` 함수는 CSV 파일 집합을 읽기 위한 고수준 인터페이스입니다. <br>\n",
    "해당 함수는 열 타입의 추론과 배치 만들기, 섞기 등 많은 기능들을 제공하여 사용하기에 간편합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kyle/kyle/lib/python3.6/site-packages/tensorflow_core/python/data/experimental/ops/readers.py:521: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
      "WARNING:tensorflow:From /Users/kyle/kyle/lib/python3.6/site-packages/tensorflow_core/python/data/experimental/ops/readers.py:215: shuffle_and_repeat (from tensorflow.python.data.experimental.ops.shuffle_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.shuffle(buffer_size, seed)` followed by `tf.data.Dataset.repeat(count)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    }
   ],
   "source": [
    "titanic_batches = tf.data.experimental.make_csv_dataset(\n",
    "    titanic_file, batch_size=4,\n",
    "    label_name='survived'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'survived' : [0 1 1 1]\n",
      "features\n",
      "    'sex'               : [b'male' b'female' b'female' b'female']\n",
      "    'age'               : [27. 42. 50. 30.]\n",
      "    'n_siblings_spouses': [0 0 0 0]\n",
      "    'parch'             : [0 0 0 0]\n",
      "    'fare'              : [26.  13.  10.5 31. ]\n",
      "    'class'             : [b'Second' b'Second' b'Second' b'First']\n",
      "    'deck'              : [b'unknown' b'unknown' b'unknown' b'unknown']\n",
      "    'embark_town'       : [b'Southampton' b'Southampton' b'Southampton' b'Cherbourg']\n",
      "    'alone'             : [b'y' b'y' b'y' b'y']\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in titanic_batches.take(1):\n",
    "    print(f\"'survived' : {label_batch}\")\n",
    "    print('features')\n",
    "    for key, value in feature_batch.items():\n",
    "        print('    {!r:20s}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특정한 컬럼들의 부분 집합이 필요하다면, `select_columns` 인자를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_batches = tf.data.experimental.make_csv_dataset(\n",
    "    titanic_file, batch_size=4,\n",
    "    label_name='survived', select_columns=['class', 'fare', 'survived']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'survived': [0 1 0 1]\n",
      "    'fare'              : [27.9    55.     21.     14.4542]\n",
      "    'class'             : [b'Third' b'First' b'Second' b'Third']\n"
     ]
    }
   ],
   "source": [
    "for feature_batch, label_batch in titanic_batches.take(1):\n",
    "    print(f\"'survived': {label_batch}\")\n",
    "    for key, value in feature_batch.items():\n",
    "        print('    {!r:20s}: {}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더욱 세분화된 제어를 제공하는 저수준 `experimental.CsvDataset` 클래스가 있습니다.<br>\n",
    "해당 클래스는 열 타입 추론을 제공하지는 않습니다. 대신에 각 열에 대한 타입을 반드시 명확히 해야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, b'male', 22.0, 1, 0, 7.25, b'Third', b'unknown', b'Southampton', b'n']\n",
      "[1, b'female', 38.0, 1, 0, 71.2833, b'First', b'C', b'Cherbourg', b'n']\n",
      "[1, b'female', 26.0, 0, 0, 7.925, b'Third', b'unknown', b'Southampton', b'y']\n",
      "[1, b'female', 35.0, 1, 0, 53.1, b'First', b'C', b'Southampton', b'n']\n",
      "[0, b'male', 28.0, 0, 0, 8.4583, b'Third', b'unknown', b'Queenstown', b'y']\n",
      "[0, b'male', 2.0, 3, 1, 21.075, b'Third', b'unknown', b'Southampton', b'n']\n",
      "[1, b'female', 27.0, 0, 2, 11.1333, b'Third', b'unknown', b'Southampton', b'n']\n",
      "[1, b'female', 14.0, 1, 0, 30.0708, b'Second', b'unknown', b'Cherbourg', b'n']\n",
      "[1, b'female', 4.0, 1, 1, 16.7, b'Third', b'G', b'Southampton', b'n']\n",
      "[0, b'male', 20.0, 0, 0, 8.05, b'Third', b'unknown', b'Southampton', b'y']\n"
     ]
    }
   ],
   "source": [
    "# 이런 방식으로 각 열의 타입을 명확하게 해야합니다.\n",
    "titanic_types = [tf.int32, tf.string, tf.float32, tf.int32, tf.int32, tf.float32, tf.string,\n",
    "                 tf.string, tf.string, tf.string]\n",
    "dataset = tf.data.experimental.CsvDataset(titanic_file, titanic_types, header=True)\n",
    "\n",
    "for line in dataset.take(10):\n",
    "    print([item.numpy() for item in line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "몇몇의 열이 비어있다면, 이 저수준 인터페이스는 열 타입 대신 기본 값을 제공할 수 있도록 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting missing.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile missing.csv\n",
    "1,2,3,4\n",
    ",2,3,4\n",
    "1,,3,4\n",
    "1,2,,4\n",
    "1,2,3,\n",
    ",,,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (4,), types: tf.int32>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두개의 CSV 파일 레코드를 읽어 데이터셋을 생성합니다.\n",
    "# 각각 4개의 실수 열이 있으며, 결측값을 가지고 있을 수 있습니다.\n",
    "\n",
    "record_defaults = [999, 999, 999, 999]\n",
    "dataset = tf.data.experimental.CsvDataset('missing.csv', record_defaults)\n",
    "dataset = dataset.map(lambda *items: tf.stack(items))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n",
      "[999   2   3   4]\n",
      "[  1 999   3   4]\n",
      "[  1   2 999   4]\n",
      "[  1   2   3 999]\n",
      "[999 999 999 999]\n"
     ]
    }
   ],
   "source": [
    "for line in dataset:\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본 값으로, `CsvDataset`은 파일의 모든 행에 대한 모든 컬럼을 생성합니다. 이는 파일이 헤더나 입력으로 요구되지 않는 컬럼으로 시작될 경우에 바람직하지 않을 수도 있습니다.<br>\n",
    "이러한 행들들과 필드는 `header`와 `select_cols` 인수로 각각 제거할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: (2,), types: tf.int32>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두개의 CSV 파일 레코드를 읽어 데이터셋을 생성합니다.\n",
    "# 헤더와 2, 4번째 열의 데이터를 추출합니다.\n",
    "record_defaults = [999, 999]\n",
    "dataset = tf.data.experimental.CsvDataset('missing.csv',\n",
    "                                          record_defaults,\n",
    "                                          select_cols=[1, 3])\n",
    "dataset = dataset.map(lambda *items: tf.stack(items))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4]\n",
      "[2 4]\n",
      "[999   4]\n",
      "[2 4]\n",
      "[  2 999]\n",
      "[999 999]\n"
     ]
    }
   ],
   "source": [
    "for line in dataset:\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일 집합 사용하기\n",
    "파일의 집합으로서 많은 데이터셋이 분산되어 있습니다. 각 파일은 데이터 샘플 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers_root = tf.keras.utils.get_file(\n",
    "    'flower_photos',\n",
    "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
    "    untar=True)\n",
    "\n",
    "flowers_root = pathlib.Path(flowers_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 해당 이미지들은 CC-BY에 라이선스 되어있습니다. 상세한 내용은 LICENCE.txt 를 확인하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roses\n",
      "sunflowers\n",
      "daisy\n",
      "dandelion\n",
      "tulips\n",
      "LICENSE.txt\n"
     ]
    }
   ],
   "source": [
    "for item in flowers_root.glob('*'):\n",
    "    print(item.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 클래스 디렉토리의 파일은 데이터 샘플 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'/Users/kyle/.keras/datasets/flower_photos/dandelion/4573204407_babff0dce4_n.jpg'\n",
      "b'/Users/kyle/.keras/datasets/flower_photos/sunflowers/3001531316_efae24d37d_n.jpg'\n",
      "b'/Users/kyle/.keras/datasets/flower_photos/daisy/13953307149_f8de6a768c_m.jpg'\n",
      "b'/Users/kyle/.keras/datasets/flower_photos/roses/8644003462_2272de26eb.jpg'\n",
      "b'/Users/kyle/.keras/datasets/flower_photos/daisy/15760811380_4d686c892b_n.jpg'\n"
     ]
    }
   ],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))\n",
    "\n",
    "for f in list_ds.take(5):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.io.read_file` 함수를 사용하여 데이터를 읽을 수 있고, 경로로부터 라벨을 추출하여 `(image, label)`쌍을 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "    label = tf.strings.split(file_path, '/')[-2]\n",
    "    return tf.io.read_file(file_path), label\n",
    "\n",
    "labeled_ds = list_ds.map(process_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x03\\x02\\x02\\x03\\x02\\x02\\x03\\x03\\x03\\x03\\x04\\x03\\x03\\x04\\x05\\x08\\x05\\x05\\x04\\x04\\x05\\n\\x07\\x07\\x06\\x08\\x0c\\n\\x0c\\x0c\\x0b\\n\\x0b\\x0b\\r\\x0e\\x12\\x10\\r\\x0e\\x11\\x0e\\x0b\\x0b\\x10\\x16\\x10\\x11\\x13\\x14\\x15\\x15\\x15\\x0c\\x0f\\x17\\x18\\x16\\x14\\x18\\x12\\x14\\x15\\x14\\xff\\xdb\\x00C\\x01\\x03\\x04\\x04\\x05\\x04\\x05'\n",
      "\n",
      "b'daisy'\n"
     ]
    }
   ],
   "source": [
    "for image_raw, label_text in labeled_ds.take(1):\n",
    "    print(repr(image_raw.numpy()[:100]))\n",
    "    print()\n",
    "    print(label_text.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyle",
   "language": "python",
   "name": "kyle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
