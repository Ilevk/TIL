{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sX3pR_XtsvzG"
   },
   "source": [
    "# 4주차, 5일차 : 1-3주차 튜토리얼 작성하기\n",
    "- ### Contents \n",
    "    1. Basic classification: Classify images of clothing : https://www.tensorflow.org/tutorials/keras/basic_classification\n",
    "    2. Text classification with TensorFlow Hub: Movie reviews : https://www.tensorflow.org/tutorials/keras/basic_text_classification\n",
    "    3. Basic regression: Predict fuel efficiency : https://www.tensorflow.org/tutorials/keras/basic_regression\n",
    "    4. Build a linear model with Estimators : https://www.tensorflow.org/tutorials/estimator/linear\n",
    "    5. Overfit And Underfit : https://www.tensorflow.org/tutorials/keras/overfit_and_underfit\n",
    "    6. Load CSV Data : https://www.tensorflow.org/tutorials/load_data/csv\n",
    "    7. TFRecord and tf.Example : https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
    "    8. Save and load models : https://www.tensorflow.org/tutorials/keras/save_and_restore_models\n",
    "    9. Better performance with the tf.data API: https://www.tensorflow.org/guide/data_performance\n",
    "    10. Time Series Forecasting: https://www.tensorflow.org/tutorials/structured_data/time_series\n",
    "    11. Text Classification with an RNN: https://www.tensorflow.org/tutorials/text/text_classification_rnn\n",
    "    12. Distributed training with Keras: https://www.tensorflow.org/tutorials/distribute/keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NXSAyw_fsvzJ"
   },
   "source": [
    "## 1. Basic classification: Classify images of clothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YRyl1nxEsvzK"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D-SGcWVKsvzP"
   },
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zN6H0XesvzT"
   },
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker',\n",
    "              'Bag', 'Ankel boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qaev70X1svzV"
   },
   "outputs": [],
   "source": [
    "# Expore the data\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oBs7ZvzYsvzX"
   },
   "outputs": [],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SAOgdIACsvzZ"
   },
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9xLRZA4svza"
   },
   "outputs": [],
   "source": [
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GvAq600Gsvzb"
   },
   "outputs": [],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hcjfRN_rsvzc"
   },
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oANBgAvsvze"
   },
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images  = test_images  / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rN3YrQcmsvzf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "miLhdLP0svzg"
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UlqIZRLbsvzh"
   },
   "outputs": [],
   "source": [
    "# Compile th model\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZRuSCBB4svzi"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "68DBkF9usvzj"
   },
   "outputs": [],
   "source": [
    "# Evaluate accuracy\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x55UcRxLsvzk"
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bTGEX9eIsvzl"
   },
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Al85gQ2Ysvzm"
   },
   "outputs": [],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "krA9CvQNsvzo"
   },
   "outputs": [],
   "source": [
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fDz7CbaYsvzo"
   },
   "outputs": [],
   "source": [
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "    \n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "        \n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                        100*np.max(predictions_array),\n",
    "                                        class_names[true_label]),\n",
    "                                        color=color)\n",
    "    \n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    predictions_array, true_label = predictions_array, true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color='#777777')\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    \n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4-O-JdOsvzp"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(i, predictions[i], test_labels, test_images)\n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(i, predictions[i], test_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8uwi6Qysvzq"
   },
   "outputs": [],
   "source": [
    "i = 12\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(i, predictions[i], test_labels, test_images)\n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(i, predictions[i], test_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zG-L2aASsvzr"
   },
   "outputs": [],
   "source": [
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "    plot_image(i, predictions[i], test_labels, test_images)\n",
    "    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "    plot_value_array(i, predictions[i], test_labels)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m1oozMVjsvzr"
   },
   "outputs": [],
   "source": [
    "img = test_images[i]\n",
    "\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2B85VOUdsvzs"
   },
   "outputs": [],
   "source": [
    "img = (np.expand_dims(img,0))\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QpbJFoSsvzs"
   },
   "outputs": [],
   "source": [
    "predictions_single = model.predict(img)\n",
    "\n",
    "predictions_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kYeYcrVOsvzt"
   },
   "outputs": [],
   "source": [
    "plot_value_array(1, predictions_single[0], test_labels)\n",
    "_ = plt.xticks(range(10), class_names, rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h4DqT1pasvzu"
   },
   "outputs": [],
   "source": [
    "np.argmax(predictions_single[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRmJAogCsvzu"
   },
   "source": [
    "## 2. Test classification with TensorFlow Hub: Movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b4brh6xLsvzv"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print('Version: ', tf.__version__)\n",
    "print('Eager mode: ', tf.executing_eagerly())\n",
    "print('Hub version: ', hub.__version__)\n",
    "print('GPU is', 'avaliable' if tf.config.experimental.list_physical_devices('GPU') else 'NOT AVALIABLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gnfHpZNZsvzw"
   },
   "outputs": [],
   "source": [
    "# Download the IMDB dataset\n",
    "train_validation_split = tfds.Split.TRAIN.subsplit([6, 4])\n",
    "\n",
    "(train_data, validation_data), test_data = tfds.load(\n",
    "    name='imdb_reviews',\n",
    "    split=(train_validation_split, tfds.Split.TEST),\n",
    "    as_supervised=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jauFQ81ysvzx"
   },
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "train_examples_batch, train_labels_batch = next(iter(train_data.batch(10)))\n",
    "train_examples_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8gK8Z99svzz"
   },
   "outputs": [],
   "source": [
    "train_labels_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EHbHRl8rsvz0"
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "embedding = \"https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1\"\n",
    "hub_layer = hub.KerasLayer(embedding, input_shape=[],\n",
    "                          dtype=tf.string, trainable=True)\n",
    "hub_layer(train_examples_batch[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K9OmHsNSsvz1"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j7bASREwsvz2"
   },
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gion7QYMsvz2"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_data.shuffle(10000).batch(512),\n",
    "                   epochs=20,\n",
    "                   validation_data=validation_data.batch(512),\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DTAsKRC5svz3"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "results = model.evaluate(test_data.batch(512), verbose=2)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print('%s: %.3f' % (name, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YelLDWvesvz4"
   },
   "source": [
    "## 3. Basic regression: Predict fuel efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i6K9dP1Rsvz4"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qvr5VOAYsvz5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ng4W-Wr-svz5"
   },
   "outputs": [],
   "source": [
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UqWvvZlesvz6"
   },
   "outputs": [],
   "source": [
    "# The Auto MPG dataset\n",
    "# Get the data\n",
    "dataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\n",
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZlpW3PSTsvz7"
   },
   "outputs": [],
   "source": [
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "               'Acceleration', 'Model Year', 'Origin']\n",
    "raw_dataset = pd.read_csv(dataset_path, names=column_names,\n",
    "                         na_values='?', comment='\\t', sep=' ', skipinitialspace=True)\n",
    "dataset = raw_dataset.copy()\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CgbFNAtCsvz7"
   },
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eFWtm5YEsvz8"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsMMkIONsvz8"
   },
   "outputs": [],
   "source": [
    "dataset['Origin'] = dataset['Origin'].map(lambda x : {1:'USA', 2:'Europe', 3:'Japan'}.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fItVMe45svz9"
   },
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset, prefix='', prefix_sep='')\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "av92o_HDsvz-"
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset  = dataset.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYi0Uus9svz-"
   },
   "outputs": [],
   "source": [
    "# Inspect the data\n",
    "sns.pairplot(train_dataset[['MPG', 'Cylinders', 'Displacement', 'Weight']], diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpgRaovHsvz_"
   },
   "outputs": [],
   "source": [
    "train_stats = train_dataset.describe()\n",
    "train_stats.pop('MPG')\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hQkNuovEsvz_"
   },
   "outputs": [],
   "source": [
    "# Split features from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eabhLA1Bsvz_"
   },
   "outputs": [],
   "source": [
    "train_labels = train_dataset.pop('MPG')\n",
    "test_labels  = test_dataset.pop('MPG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kFxKed6osv0A"
   },
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "def norm(x):\n",
    "    return (x - train_stats['mean']) / train_stats['std']\n",
    "\n",
    "normed_train_data = norm(train_dataset)\n",
    "normed_test_data  = norm(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Giz2Nymzsv0A"
   },
   "outputs": [],
   "source": [
    "# The model \n",
    "# Build the model\n",
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "    \n",
    "    model.compile(loss='mse',\n",
    "                 optimizer=optimizer,\n",
    "                 metrics=['mae', 'mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7NuPqpxWsv0B"
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOYVAilysv0B"
   },
   "outputs": [],
   "source": [
    "# Inspect the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ks_Ts6ausv0B"
   },
   "outputs": [],
   "source": [
    "example_batch = normed_train_data[:10]\n",
    "example_result = model.predict(example_batch)\n",
    "example_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_cJ88VLsv0C"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "EPOCHS = 1000\n",
    "\n",
    "history = model.fit(\n",
    "    normed_train_data, train_labels,\n",
    "    epochs=EPOCHS, validation_split=0.2, verbose=0,\n",
    "    callbacks=[tfdocs.modeling.EpochDots()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EK6Skez2sv0C"
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gqnrPXC3sv0D"
   },
   "outputs": [],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Rj1G-PJsv0D"
   },
   "outputs": [],
   "source": [
    "plotter.plot({'Basic': history}, metric='mae')\n",
    "plt.ylim([0, 10])\n",
    "plt.ylabel('MAE [MPG]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyv-K7Qvsv0E"
   },
   "outputs": [],
   "source": [
    "plotter.plot({'Basic': history}, metric='mse')\n",
    "plt.ylim([0, 20])\n",
    "plt.ylabel('MSE [MPG^2]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KbvuhR_Tsv0E"
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "early_history = model.fit(normed_train_data, train_labels,\n",
    "                         epochs=EPOCHS, validation_split=0.2, verbose=0,\n",
    "                         callbacks=[early_stop, tfdocs.modeling.EpochDots()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8_bWet7Rsv0F"
   },
   "outputs": [],
   "source": [
    "plotter.plot({'Early Stopping': early_history}, metric='mae')\n",
    "plt.ylim([0, 10])\n",
    "plt.ylabel('MAE [MPG]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C95iyKmesv0G"
   },
   "outputs": [],
   "source": [
    "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)\n",
    "\n",
    "print('Testing set Mean Abs Error: {:5.2f} MPG'.format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kNKngyw9sv0G"
   },
   "outputs": [],
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xIcXMyfPsv0H"
   },
   "outputs": [],
   "source": [
    "test_predictions = model.predict(normed_test_data).flatten()\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(test_labels, test_predictions)\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]')\n",
    "lims = [0, 50]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ga59rAdZsv0H"
   },
   "outputs": [],
   "source": [
    "error = test_predictions - test_labels\n",
    "plt.hist(error, bins = 25)\n",
    "plt.xlabel('Prediction Error [MPG]')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p6iuXnLesv0I"
   },
   "source": [
    "## 4. Build a linear model with Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2r4ycqPEsv0I"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from six.moves import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OI0dhOpvsv0I"
   },
   "outputs": [],
   "source": [
    "# Load the titanic dataset\n",
    "import tensorflow.compat.v2.feature_column as fc\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgB6Msupsv0J"
   },
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')\n",
    "dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')\n",
    "y_train = dftrain.pop('survived')\n",
    "y_eval = dfeval.pop('survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tre0PBzisv0J"
   },
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jq-cTIp_sv0J"
   },
   "outputs": [],
   "source": [
    "dftrain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J0ACovSLsv0K"
   },
   "outputs": [],
   "source": [
    "dftrain.shape[0], dfeval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LYngumwHsv0K"
   },
   "outputs": [],
   "source": [
    "dftrain.age.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rvn9tK93sv0L"
   },
   "outputs": [],
   "source": [
    "dftrain.sex.value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CdlwkJTLsv0L"
   },
   "outputs": [],
   "source": [
    "dftrain['class'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gvF-2gbtsv0L"
   },
   "outputs": [],
   "source": [
    "pd.concat([dftrain, y_train], axis=1).groupby('sex').survived.mean().plot(kind='barh').set_xlabel('% survive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJKpKJsfsv0M"
   },
   "outputs": [],
   "source": [
    "# Feature Engineering for the Model\n",
    "CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\n",
    "                      'embark_town', 'alone']\n",
    "NUMERIC_COLUMNS = ['age', 'fare']\n",
    "\n",
    "feature_columns = []\n",
    "for feature_name in CATEGORICAL_COLUMNS:\n",
    "    vocabulary = dftrain[feature_name].unique()\n",
    "    feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, \n",
    "                                                                                    vocabulary))\n",
    "for feature_name in NUMERIC_COLUMNS:\n",
    "    feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SrUVzFcDsv0M"
   },
   "outputs": [],
   "source": [
    "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n",
    "    def input_function():\n",
    "        ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(1000)\n",
    "        ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "        return ds\n",
    "    return input_function\n",
    "\n",
    "train_input_fn = make_input_fn(dftrain, y_train)\n",
    "eval_input_fn  = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zu10pxCfsv0N"
   },
   "outputs": [],
   "source": [
    "ds = make_input_fn(dftrain, y_train, batch_size=10)()\n",
    "for feature_batch, label_batch in ds.take(1):\n",
    "    print('Some feature keys:', list(feature_batch.keys()))\n",
    "    print()\n",
    "    print('A batch of class:', feature_batch['class'].numpy())\n",
    "    print()\n",
    "    print('A batch of Labels:', label_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qq64-SSYsv0N"
   },
   "outputs": [],
   "source": [
    "age_column = feature_columns[7]\n",
    "tf.keras.layers.DenseFeatures([age_column])(feature_batch).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pun7PPUZsv0N"
   },
   "outputs": [],
   "source": [
    "gender_column = feature_columns[0]\n",
    "tf.keras.layers.DenseFeatures([tf.feature_column.indicator_column(gender_column)])(feature_batch).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZ-gjVX3sv0O"
   },
   "outputs": [],
   "source": [
    "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)\n",
    "linear_est.train(train_input_fn)\n",
    "result = linear_est.evaluate(eval_input_fn)\n",
    "\n",
    "clear_output()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KOe0wE7Msv0O"
   },
   "outputs": [],
   "source": [
    "age_x_gender = tf.feature_column.crossed_column(['age', 'sex'], hash_bucket_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYPUgYxZsv0P"
   },
   "outputs": [],
   "source": [
    "derived_feature_columns = [age_x_gender]\n",
    "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns+derived_feature_columns)\n",
    "linear_est.train(train_input_fn)\n",
    "result = linear_est.evaluate(eval_input_fn)\n",
    "\n",
    "clear_output()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lLSU6gq9sv0P"
   },
   "outputs": [],
   "source": [
    "pred_dicts = list(linear_est.predict(eval_input_fn))\n",
    "probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\n",
    "\n",
    "probs.plot(kind='hist', bins=20, title='predicted probabilities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OZb8ubjosv0Q"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_eval, probs)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel('false positive rate')\n",
    "plt.ylabel('true positive rate')\n",
    "plt.xlim(0,)\n",
    "plt.ylim(0,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UWSvbqg2txia"
   },
   "source": [
    "# 5. Overfit and underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6KKV-N2tt1nV"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "%tensorflow_version 2.x\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXqHkmt-vpPB"
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/tensorflow/docs\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.modeling\n",
    "import tensorflow_docs.plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jxlXBgIswekR"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pathlib\n",
    "import shutil\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fz3sedX2wnc5"
   },
   "outputs": [],
   "source": [
    "logdir = pathlib.Path(tempfile.mkdtemp())/'tensorboard_logs'\n",
    "shutil.rmtree(logdir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "74Ozra6VwxKt"
   },
   "outputs": [],
   "source": [
    "# The Higgs Dataset\n",
    "gz = tf.keras.utils.get_file('HIGGS.csv.gz',\n",
    "                             'https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZS2GzbFw2fy"
   },
   "outputs": [],
   "source": [
    "FEATURES = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U48sSlFvw6Wc"
   },
   "outputs": [],
   "source": [
    "ds = tf.data.experimental.CsvDataset(gz,[float(),]*(FEATURES+1), compression_type='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_TtAhK5oxA0Q"
   },
   "outputs": [],
   "source": [
    "def pack_row(*row):\n",
    "  label = row[0]\n",
    "  features = tf.stack(row[1:], 1)\n",
    "  return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-32bXiexKKb"
   },
   "outputs": [],
   "source": [
    "packed_ds = ds.batch(10000).map(pack_row).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQMyBbYXxNRZ"
   },
   "outputs": [],
   "source": [
    "for features, label in packed_ds.batch(1000).take(1):\n",
    "  print(features[0])\n",
    "  plt.hist(features.numpy().flatten(), bins=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e49mOwqkxTuG"
   },
   "outputs": [],
   "source": [
    "N_VALIDATION = int(1e3)\n",
    "N_TRAIN = int(1e4)\n",
    "BUFFER_SIZE = int(1e4)\n",
    "BATCH_SIZE = 500\n",
    "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UboM4dKrxd6r"
   },
   "outputs": [],
   "source": [
    "validate_ds = packed_ds.take(N_VALIDATION).cache()\n",
    "train_ds = packed_ds.skip(N_VALIDATION).take(N_TRAIN).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZkKcmRgyxmnC"
   },
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtgpFAcZxniG"
   },
   "outputs": [],
   "source": [
    "validate_ds = validate_ds.batch(BATCH_SIZE)\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).repeat().batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3decsHDdxt21"
   },
   "outputs": [],
   "source": [
    "# Demonstrate overfitting\n",
    "# Training precedure\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    0.001,\n",
    "    decay_steps=STEPS_PER_EPOCH*1000,\n",
    "    decay_rate=1,\n",
    "    staircase=False\n",
    ")\n",
    "\n",
    "def get_optimizer():\n",
    "  return tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2hkaIRo-yFOL"
   },
   "outputs": [],
   "source": [
    "step = np.linspace(0, 100000)\n",
    "lr = lr_schedule(step)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(step/STEPS_PER_EPOCH, lr)\n",
    "plt.ylim([0, max(plt.ylim())])\n",
    "plt.xlabel('Epoch')\n",
    "_ = plt.ylabel('Learning Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rx-HJ2q9yYXm"
   },
   "outputs": [],
   "source": [
    "def get_callbacks(name):\n",
    "  return [\n",
    "          tfdocs.modeling.EpochDots(),\n",
    "          tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=200),\n",
    "          tf.keras.callbacks.TensorBoard(logdir/name),\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9etWFwJFyo6E"
   },
   "outputs": [],
   "source": [
    "def compile_and_fit(model, name, optimizer=None, max_epochs=10000):\n",
    "  if optimizer is None:\n",
    "    optimizer = get_optimizer()\n",
    "  model.compile(optimizer=optimizer,\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy', 'binary_crossentropy'])\n",
    "  model.summary()\n",
    "\n",
    "  history = model.fit(\n",
    "      train_ds,\n",
    "      steps_per_epoch = STEPS_PER_EPOCH,\n",
    "      epochs=max_epochs,\n",
    "      validation_data=validate_ds,\n",
    "      callbacks=get_callbacks(name),\n",
    "      verbose=0\n",
    "  )\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UK3s8fN_z6uz"
   },
   "outputs": [],
   "source": [
    "# Tiny model\n",
    "tiny_model = tf.keras.Sequential([\n",
    "                                  layers.Dense(16, activation='elu', input_shape=(FEATURES,)),\n",
    "                                  layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gqg3d0Mu0GDa"
   },
   "outputs": [],
   "source": [
    "size_histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1pe9Nn9_0UdD"
   },
   "outputs": [],
   "source": [
    "size_histories['Tiny'] = compile_and_fit(tiny_model, 'sizes/Tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FAbalCJ_0gX2"
   },
   "outputs": [],
   "source": [
    "plotter = tfdocs.plots.HistoryPlotter(metric = 'binary_crossentropy', smoothing_std=10)\n",
    "plotter.plot(size_histories)\n",
    "plt.ylim([0.5, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kbjzwFNk0zvq"
   },
   "outputs": [],
   "source": [
    "# Small model\n",
    "small_model = tf.keras.Sequential([\n",
    "  # 'input_shape'를 입력해야 .summary()가 작동한다.\n",
    "  layers.Dense(16, activation='elu', input_shape=(FEATURES,)),\n",
    "  layers.Dense(16, activation='elu'),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DWE8ZLyx1IxW"
   },
   "outputs": [],
   "source": [
    "size_histories['Small'] = compile_and_fit(small_model, 'sizes/Small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t4l-1WuZ1M0S"
   },
   "outputs": [],
   "source": [
    "# Medium model\n",
    "medium_model = tf.keras.Sequential([\n",
    "  layers.Dense(64, activation='elu', input_shape=(FEATURES, )),\n",
    "  layers.Dense(64, activation='elu'),\n",
    "  layers.Dense(64, activation='elu'),\n",
    "  layers.Dense(1,  activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-XjAKOmq1ayQ"
   },
   "outputs": [],
   "source": [
    "size_histories['Medium'] = compile_and_fit(medium_model, 'sizes/Medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QRGQY0C_1kah"
   },
   "outputs": [],
   "source": [
    "# Large model\n",
    "large_model = tf.keras.Sequential([\n",
    "  layers.Dense(512, activation='elu', input_shape=(FEATURES,)),\n",
    "  layers.Dense(512, activation='elu'),\n",
    "  layers.Dense(512, activation='elu'),\n",
    "  layers.Dense(512, activation='elu'),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Cbk7Xgt1xRi"
   },
   "outputs": [],
   "source": [
    "size_histories['large'] = compile_and_fit(large_model, 'sizes/large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQnrnZmT113j"
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation losses\n",
    "plotter.plot(size_histories)\n",
    "a = plt.xscale('log')\n",
    "plt.xlim([5, max(plt.xlim())])\n",
    "plt.ylim([0.5, 0.7])\n",
    "plt.xlabel('Epochs [Log Scale]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z1RS3hzd2Bev"
   },
   "outputs": [],
   "source": [
    "# View in Tensorboard\n",
    " \n",
    "%tensorboard --logdir {logdir}/sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SOAZ9AbU3E15"
   },
   "outputs": [],
   "source": [
    "display.IFrame(\n",
    "    src=\"https://tensorboard.dev/experiment/vW7jmmF9TmKmy3rbheMQpw/#scalars&_smoothingWeight=0.97\",\n",
    "    width=\"100%\", height=\"800px\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KCm-z8pw3Iq_"
   },
   "outputs": [],
   "source": [
    "!tensorbard dev upload --logdir {logdir}/sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GvzubMVh3NHG"
   },
   "outputs": [],
   "source": [
    "#Strategies to prevent overfitting\n",
    "shutil.rmtree(logdir/'regularizers/Tiny', ignore_errors=True)\n",
    "shutil.copytree(logdir/'sizes/Tiny', logdir/'regularizers/Tiny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iSwUpeYP3b7c"
   },
   "outputs": [],
   "source": [
    "regularizer_histories={}\n",
    "regularizer_histories['Tiny'] = size_histories['Tiny']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9m-jfqtS3gn4"
   },
   "outputs": [],
   "source": [
    "# Add weight regularization\n",
    "l2_model = tf.keras.Sequential([\n",
    "  layers.Dense(512, activation='elu',\n",
    "               kernel_regularizer=regularizers.l2(0.001),\n",
    "               input_shape=(FEATURES, )),\n",
    "  layers.Dense(512, activation='elu',\n",
    "               kernel_regularizer=regularizers.l2(0.001)),\n",
    "  layers.Dense(512, activation='elu',\n",
    "               kernel_regularizer=regularizers.l2(0.001)),\n",
    "  layers.Dense(512, activation='elu',\n",
    "               kernel_regularizer=regularizers.l2(0.001)),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "regularizer_histories['l2'] = compile_and_fit(l2_model, 'regularizers/l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VBptxiPi4AQr"
   },
   "outputs": [],
   "source": [
    "plotter.plot(regularizer_histories)\n",
    "plt.ylim([0.5, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_68MJ6gW4Hp2"
   },
   "outputs": [],
   "source": [
    "result = l2_model(features)\n",
    "regularization_loss = tf.add_n(l2_model.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "27kTHTWh4MLw"
   },
   "outputs": [],
   "source": [
    "# Add dropout\n",
    "dropout_model = tf.keras.Sequential([\n",
    "  layers.Dense(512, activation='elu', input_shape=(FEATURES,)),\n",
    "  layers.Dropout(0.5),\n",
    "  layers.Dense(512, activation='elu'),\n",
    "  layers.Dropout(0.5),\n",
    "  layers.Dense(512, activation='elu'),\n",
    "  layers.Dropout(0.5),\n",
    "  layers.Dense(512, activation='elu'),\n",
    "  layers.Dropout(0.5),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "regularizer_histories['dropout'] = compile_and_fit(dropout_model, 'regularizers/dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hVdzE3fO4rzZ"
   },
   "outputs": [],
   "source": [
    "plotter.plot(regularizer_histories)\n",
    "plt.ylim([0.5, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSmBcw6n42x_"
   },
   "outputs": [],
   "source": [
    "# Combined L2 + dropout\n",
    "combined_model = tf.keras.Sequential([\n",
    "  layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
    "               activation='elu', input_shape=(FEATURES, )),\n",
    "  layers.Dropout(0.5),\n",
    "  layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
    "               activation='elu'),\n",
    "  layers.Dropout(0.5),\n",
    "  layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
    "               activation='elu'),\n",
    "  layers.Dropout(0.5),\n",
    "  layers.Dense(512, kernel_regularizer=regularizers.l2(0.0001),\n",
    "               activation='elu'),\n",
    "  layers.Dropout(0.5),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "regularizer_histories['combined'] = compile_and_fit(combined_model, 'regularizers/combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GTE62aFt5wc6"
   },
   "outputs": [],
   "source": [
    "plotter.plot(regularizer_histories)\n",
    "plt.ylim([0.5, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HgAskqm652LS"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir {logdir}/regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4uCWRCkQ56K7"
   },
   "outputs": [],
   "source": [
    "display.IFrame(\n",
    "    src=\"https://tensorboard.dev/experiment/fGInKDo8TXes1z7HQku9mw/#scalars&_smoothingWeight=0.97\",\n",
    "    width = \"100%\",\n",
    "    height=\"800px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vuI--S6T57Kg"
   },
   "source": [
    "# 6. Load CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I7CCyRkA6BPn"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6Ee9uaZ7P3E"
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    "\n",
    "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
    "test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfgkBE0n7YhL"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmoHHLhq7d6A"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "!head {train_file_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g-HLN5e47gOZ"
   },
   "outputs": [],
   "source": [
    "LABEL_COLUMN = 'survived'\n",
    "LABELS = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZVXUmr-v7kU8"
   },
   "outputs": [],
   "source": [
    "def get_dataset(file_path, **kwargs):\n",
    "  dataset = tf.data.experimental.make_csv_dataset(\n",
    "   file_path,\n",
    "   batch_size=5,\n",
    "   label_name=LABEL_COLUMN,\n",
    "   na_value=\"?\",\n",
    "   num_epochs=1,\n",
    "   ignore_errors=True,\n",
    "   **kwargs   \n",
    "  )\n",
    "\n",
    "  return dataset\n",
    "\n",
    "raw_train_data = get_dataset(train_file_path)\n",
    "raw_test_data = get_dataset(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRA9rtjE76qM"
   },
   "outputs": [],
   "source": [
    "def show_batch(dataset):\n",
    "  for batch, label in dataset.take(1):\n",
    "    for key, value in  batch.items():\n",
    "      print('{:20s}: {}'.format(key, value.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O4U-ufKE8C4r"
   },
   "outputs": [],
   "source": [
    "show_batch(raw_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TofsjJv88Eal"
   },
   "outputs": [],
   "source": [
    "CSV_COLUMNS = ['survived', 'sex', 'age', 'n_siblings_spouses', 'parch',\n",
    "               'fare', 'class', 'deck', 'embark_town', 'alone']\n",
    "temp_dataset = get_dataset(train_file_path, column_names=CSV_COLUMNS)\n",
    "\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MdzLWaF88Tvh"
   },
   "outputs": [],
   "source": [
    "SELECT_COLUMNS = ['survived', 'age', 'n_siblings_spouses', 'class', 'deck', 'alone']\n",
    "\n",
    "temp_dataset = get_dataset(train_file_path, select_columns=SELECT_COLUMNS)\n",
    "\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pSolfrzL8gZH"
   },
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "SELECT_COLUMNS = ['survived', 'age', 'n_siblings_spouses', 'parch', 'fare']\n",
    "DEFAULTS = [0, 0.0, 0.0, 0.0, 0.0]\n",
    "temp_dataset = get_dataset(train_file_path,\n",
    "                           select_columns=SELECT_COLUMNS,\n",
    "                           column_defaults=DEFAULTS)\n",
    "\n",
    "show_batch(temp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xDuhzpLx8ya6"
   },
   "outputs": [],
   "source": [
    "example_batch , labels_batch = next(iter(temp_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "410-NKc781dU"
   },
   "outputs": [],
   "source": [
    "def pack(features, label):\n",
    "  return tf.stack(list(features.values()), axis=-1), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96SiqnIB86NM"
   },
   "outputs": [],
   "source": [
    "packed_dataset = temp_dataset.map(pack)\n",
    "\n",
    "for features, labels in packed_dataset.take(1):\n",
    "  print(features.numpy())\n",
    "  print()\n",
    "  print(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z5jcneH587zS"
   },
   "outputs": [],
   "source": [
    "show_batch(raw_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KXBAYh-U9GCC"
   },
   "outputs": [],
   "source": [
    "example_batch, labels_batch = next(iter(temp_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRmVtmLV9JB3"
   },
   "outputs": [],
   "source": [
    "class PackNumericFeatures(object):\n",
    "  def __init__(self, names):\n",
    "    self.names = names\n",
    "\n",
    "  def __call__(self, features, labels):\n",
    "    numeric_features = [features.pop(name) for name in self.names]\n",
    "    numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
    "    numeric_features = tf.stack(numeric_features, axis=-1)\n",
    "    features['numeric'] = numeric_features\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wFUPzJhP9phm"
   },
   "outputs": [],
   "source": [
    "NUMERIC_FEATURES = ['age', 'n_siblings_spouses', 'parch', 'fare']\n",
    "\n",
    "packed_train_data = raw_train_data.map(\n",
    "    PackNumericFeatures(NUMERIC_FEATURES)\n",
    ")\n",
    "\n",
    "packed_test_data = raw_test_data.map(\n",
    "    PackNumericFeatures(NUMERIC_FEATURES)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-lFoALGM92h0"
   },
   "outputs": [],
   "source": [
    "show_batch(packed_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tcDwLTeT94KD"
   },
   "outputs": [],
   "source": [
    "example_batch, labels_batch = next(iter(packed_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xka0fPf99-zx"
   },
   "outputs": [],
   "source": [
    "# Data Normalization\n",
    "import pandas as pd\n",
    "desc = pd.read_csv(train_file_path)[NUMERIC_FEATURES].describe()\n",
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QA6Dw9cA-FIh"
   },
   "outputs": [],
   "source": [
    "MEAN = np.array(desc.T['mean'])\n",
    "STD  = np.array(desc.T['std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SbkIOd9j-Jvp"
   },
   "outputs": [],
   "source": [
    "def normalize_numeric_data(data, mean, std):\n",
    "  return (data-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u-MUDXty-Na7"
   },
   "outputs": [],
   "source": [
    "normalizer = functools.partial(normalize_numeric_data, mean=MEAN, std=STD)\n",
    "\n",
    "numeric_column = tf.feature_column.numeric_column('numeric', \n",
    "                                                  normalizer_fn=normalizer, \n",
    "                                                  shape=[len(NUMERIC_FEATURES)])\n",
    "numeric_columns = [numeric_column]\n",
    "numeric_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QkBj8rPc-iPj"
   },
   "outputs": [],
   "source": [
    "example_batch['numeric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4uhNLVB-k3L"
   },
   "outputs": [],
   "source": [
    "numeric_layer = tf.keras.layers.DenseFeatures(numeric_columns)\n",
    "numeric_layer(example_batch).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Axf7Dye7-pYA"
   },
   "outputs": [],
   "source": [
    "# Categorical data\n",
    "CATEGORIES = {\n",
    "    'sex' : ['male', 'female'],\n",
    "    'class' : ['First', 'Second', 'Third'],\n",
    "    'deck' : ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\n",
    "    'embark_town' : ['Cherbourg', 'Southhampton', 'Queenstown'],\n",
    "    'alone' : ['y', 'n']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IUGj_E_p-8O9"
   },
   "outputs": [],
   "source": [
    "categorical_columns = []\n",
    "for feature, vocab in CATEGORIES.items():\n",
    "  cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "      key=feature, vocabulary_list=vocab\n",
    "  )\n",
    "  categorical_columns.append(tf.feature_column.indicator_column(cat_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "viZhCXBF_LS3"
   },
   "outputs": [],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qEEtwdoD_MXU"
   },
   "outputs": [],
   "source": [
    "categorical_layer = tf.keras.layers.DenseFeatures(categorical_columns)\n",
    "print(categorical_layer(example_batch).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9K1rjlCW_Ttn"
   },
   "outputs": [],
   "source": [
    "# Combine preprocessing layer\n",
    "preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9hgu1kZ8_dM9"
   },
   "outputs": [],
   "source": [
    "print(preprocessing_layer(example_batch).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i7XmQd4a_gQb"
   },
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "  preprocessing_layer,\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pl_5tBiX_uvp"
   },
   "outputs": [],
   "source": [
    "# Train, evlauate, and predict\n",
    "train_data = packed_train_data.shuffle(500)\n",
    "test_data  = packed_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIE-1VcB_0cI"
   },
   "outputs": [],
   "source": [
    "model.fit(train_data, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L1Sy8x_J_2Ez"
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "\n",
    "print('\\n\\nTest Loss: {}, Test Accuracy: {}'.format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rrC1oZE__9yh"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data)\n",
    "\n",
    "for prediction, survived in zip(predictions[:10], list(test_data)[0][1][:10]):\n",
    "  print('Predicted survival: {:.2%}'.format(prediction[0]),\n",
    "        '| Acutal outcome: ',\n",
    "        ('SURVIVED' if bool(survived) else 'DIED'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTFGX4lFAR_g"
   },
   "source": [
    "# 7. TFRecord and tf.Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pe9CiGLKAwY8"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "%tensorflow_version 2.x\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8v_FvwuaA4hN"
   },
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy()\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8wPomGcBMNH"
   },
   "outputs": [],
   "source": [
    "print(_bytes_feature(b'test_string'))\n",
    "print(_bytes_feature(u'test_bytes'.encode('utf-8')))\n",
    "\n",
    "print(_float_feature(np.exp(1)))\n",
    "\n",
    "print(_int64_feature(True))\n",
    "print(_int64_feature(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSh8-MHcBiR6"
   },
   "outputs": [],
   "source": [
    "feature = _float_feature(np.exp(1))\n",
    "\n",
    "feature.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJ6jhS2UB7wz"
   },
   "outputs": [],
   "source": [
    "# Creating a tf.Example message\n",
    "n_observations = int(1e4)\n",
    "\n",
    "feature0 = np.random.choice([False, True], n_observations)\n",
    "\n",
    "feature1 = np.random.randint(0, 5, n_observations)\n",
    "\n",
    "strings = np.array([b'cat', b'dog', b'chicken', b'horse', b'goat'])\n",
    "feature2 = strings[feature1]\n",
    "\n",
    "feature3 = np.random.randn(n_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0XZ_RXFzCSD4"
   },
   "outputs": [],
   "source": [
    "def serialize_example(feature0, feature1, feature2, feature3):\n",
    "  feature = {\n",
    "      'feature0' : _int64_feature(feature0),\n",
    "      'feature1' : _int64_feature(feature1),\n",
    "      'feature2' : _bytes_feature(feature2),\n",
    "      'feature3' : _float_feature(feature3),\n",
    "  }\n",
    "\n",
    "  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "  return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lYmx42RTDb6N"
   },
   "outputs": [],
   "source": [
    "example_obsevation = []\n",
    "\n",
    "serialized_example = serialize_example(False, 4, b'goat', 0.9876)\n",
    "serialized_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0kPp-loDiBo"
   },
   "outputs": [],
   "source": [
    "example_proto = tf.train.Example.FromString(serialized_example)\n",
    "example_proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wCsDjZGGDr2c"
   },
   "outputs": [],
   "source": [
    "# TFRecord files using tf.data\n",
    "tf.data.Dataset.from_tensor_slices(feature1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aa-gahc2DzpZ"
   },
   "outputs": [],
   "source": [
    "features_dataset = tf.data.Dataset.from_tensor_slices((feature0, feature1,\n",
    "                                                       feature2, feature3))\n",
    "features_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rlMMBm2VD7jY"
   },
   "outputs": [],
   "source": [
    "for f0,f1,f2,f3 in features_dataset.take(1):\n",
    "  print(f0)\n",
    "  print(f1)\n",
    "  print(f2)\n",
    "  print(f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyEgXvO9EBQ_"
   },
   "outputs": [],
   "source": [
    "# 이 함수랑 serialize_example 함수랑은 뭔 차이인가.. 단순히 파이썬 함수를 텐서플로우 함수로 매핑해주는걸까..\n",
    "def tf_serialize_example(f0,f1,f2,f3):\n",
    "  tf_string = tf.py_function(\n",
    "      serialize_example,\n",
    "      (f0,f1,f2,f3),\n",
    "      tf.string\n",
    "  )\n",
    "  return tf.reshape(tf_string, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ns6LPCOLEKdh"
   },
   "outputs": [],
   "source": [
    "tf_serialize_example(f0,f1,f2,f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Zvtd9EdEMNk"
   },
   "outputs": [],
   "source": [
    "serialized_features_dataset = features_dataset.map(tf_serialize_example)\n",
    "serialized_features_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZU6ZcpBCEUpo"
   },
   "outputs": [],
   "source": [
    "def generator():\n",
    "  for features in features_dataset:\n",
    "    yield serialize_example(*features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jonf6KZGEtKV"
   },
   "outputs": [],
   "source": [
    "serialized_features_dataset = tf.data.Dataset.from_generator(\n",
    "    generator, output_types=tf.string, output_shapes=()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3QH2p_mgF283"
   },
   "outputs": [],
   "source": [
    "serialized_features_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hkAEVvx8F5AB"
   },
   "outputs": [],
   "source": [
    "filename = 'test.tfrecord'\n",
    "writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "writer.write(serialized_features_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S0uTMvS2F_9-"
   },
   "outputs": [],
   "source": [
    "# Reading a TFRecord file\n",
    "filenames = [filename]\n",
    "raw_dataset = tf.data.TFRecordDataset(filenames)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sxB2LbVWGKYG"
   },
   "outputs": [],
   "source": [
    "for raw_record in raw_dataset.take(10):\n",
    "  print(repr(raw_record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7BbDnjXGN_P"
   },
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    'feature0' : tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'feature1' : tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    'feature2' : tf.io.FixedLenFeature([], tf.string, default_value=''),\n",
    "    'feature3' : tf.io.FixedLenFeature([], tf.float32, default_value=0.0)\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  return tf.io.parse_single_example(example_proto, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Atj8v6BGKX9s"
   },
   "outputs": [],
   "source": [
    "parsed_dataset = raw_dataset.map(_parse_function)\n",
    "parsed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4EdLFMtjKb52"
   },
   "outputs": [],
   "source": [
    "for parsed_record in parsed_dataset.take(10):\n",
    "  print(repr(parsed_record))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yi0hU8N-KgI3"
   },
   "outputs": [],
   "source": [
    "# TFRecord files in Python\n",
    "# Writing a TFRecord file\n",
    "with tf.io.TFRecordWriter(filename) as writer:\n",
    "  for i in range(n_observations):\n",
    "    example = serialize_example(feature0[i], feature1[i], feature2[i], feature3[i])\n",
    "    writer.write(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VUFOEfxoKvZ6"
   },
   "outputs": [],
   "source": [
    "# Reading a TFRecored file\n",
    "filenames = [filename]\n",
    "raw_dataset = tf.data.TFRecordDataset(filename)\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98FU3INPK4Ih"
   },
   "outputs": [],
   "source": [
    "for raw_record in raw_dataset.take(1):\n",
    "  example = tf.train.Example()\n",
    "  example.ParseFromString(raw_record.numpy())\n",
    "  print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zlqkz5fNK-7-"
   },
   "outputs": [],
   "source": [
    "# Walkthrough: Reading and writing image data\n",
    "cat_in_snow  = tf.keras.utils.get_file('320px-Felis_catus-cat_on_snow.jpg', 'https://storage.googleapis.com/download.tensorflow.org/example_images/320px-Felis_catus-cat_on_snow.jpg')\n",
    "williamsburg_bridge = tf.keras.utils.get_file('194px-New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg','https://storage.googleapis.com/download.tensorflow.org/example_images/194px-New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEr0P8teLCxh"
   },
   "outputs": [],
   "source": [
    "display.display(display.Image(filename=cat_in_snow))\n",
    "display.display(display.HTML('Image cc-by: <a \"href=https://commons.wikimedia.org/wiki/File:Felis_catus-cat_on_snow.jpg\">Von.grzanka</a>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8BRD_DLFLDua"
   },
   "outputs": [],
   "source": [
    "display.display(display.Image(filename=williamsburg_bridge))\n",
    "display.display(display.HTML('<a \"href=https://commons.wikimedia.org/wiki/File:New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg\">From Wikimedia</a>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSUM8SUNLFmC"
   },
   "outputs": [],
   "source": [
    "# Write the TFREcord file\n",
    "image_labels = {\n",
    "    cat_in_snow : 0,\n",
    "    williamsburg_bridge : 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dLnuUm30LN3W"
   },
   "outputs": [],
   "source": [
    "image_string = open(cat_in_snow, 'rb').read()\n",
    "\n",
    "label = image_labels[cat_in_snow]\n",
    "\n",
    "def image_example(image_string, label):\n",
    "  image_shape = tf.image.decode_jpeg(image_string).shape\n",
    "\n",
    "  feature = {\n",
    "      'height': _int64_feature(image_shape[0]),\n",
    "      'width': _int64_feature(image_shape[1]),\n",
    "      'depth': _int64_feature(image_shape[2]),\n",
    "      'label': _int64_feature(label),\n",
    "      'image_raw': _bytes_feature(image_string)\n",
    "  }\n",
    "\n",
    "  return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "for line in str(image_example(image_string, label)).split('\\n')[:5]:\n",
    "  print(line)\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DZq3NLJ9Luix"
   },
   "outputs": [],
   "source": [
    "record_file = 'images.tfrecords'\n",
    "with tf.io.TFRecordWriter(record_file) as writer:\n",
    "  for filename, label in image_labels.items():\n",
    "    image_string = open(filename, 'rb').read()\n",
    "    tf_example = image_example(image_string, label)\n",
    "    writer.write(tf_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jWK18WdCZpcm"
   },
   "outputs": [],
   "source": [
    "# Read the TFRecord file\n",
    "raw_image_dataset = tf.data.TFRecordDataset('images.tfrecords')\n",
    "\n",
    "image_feature_description = {\n",
    "    'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def _parse_image_function(example_proto):\n",
    "  return tf.io.parse_single_example(example_proto, image_feature_description)\n",
    "\n",
    "parsed_image_dataset = raw_image_dataset.map(_parse_image_function)\n",
    "parsed_image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lsZ8fAe2aoX5"
   },
   "outputs": [],
   "source": [
    "for image_features in parsed_image_dataset:\n",
    "  image_raw = image_features['image_raw'].numpy()\n",
    "  display.display(display.Image(data=image_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save and load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_labels = train_labels[:1000]\n",
    "test_labels  = test_labels[:1000]\n",
    "\n",
    "train_images = train_images[:1000].reshape(-1, 28*28) / 255.0\n",
    "test_images  = test_images[:1000].reshape(-1, 28*28) / 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        keras.layers.Dense(512, activation='relu', input_shape=(784,)),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save checkpoints during training\n",
    "\n",
    "checkpoint_path = 'training_1/cp.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, \n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "model.fit(train_images,\n",
    "         train_labels,\n",
    "         epochs=10,\n",
    "         validation_data=(test_images, test_labels),\n",
    "         callbacks=[cp_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "\n",
    "loss, acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('Untrained model, accuracy: {:5.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)\n",
    "\n",
    "loss, acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint callback options\n",
    "checkpoint_path = 'tranining_2/cp-{epoch:04d}.ckpt'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    period=5\n",
    ")\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "model.fit(train_images,\n",
    "         train_labels,\n",
    "         epochs=50,\n",
    "         callbacks=[cp_callback],\n",
    "         validation_data = (test_images, test_labels),\n",
    "         verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "\n",
    "model.load_weights(latest)\n",
    "\n",
    "loss, acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maully save weights\n",
    "model.save_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.load_weights('./checkpoints/my_checkpoint')\n",
    "\n",
    "loss, acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "model = create_model()\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('my_model.h5')\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = new_model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SavedModel format\n",
    "\n",
    "model = create_model()\n",
    "model.fit(train_images, train_labels, epochs=5)\n",
    "\n",
    "!mkdir -p saved_model\n",
    "model.save('saved_model/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('saved_model/my_model')\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = new_model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('Restored model, accuracy: {:5.2f}%'.format(100*acc))\n",
    "\n",
    "print(new_model.predict(test_images).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Better performance with the tf.data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset\n",
    "class ArtificialDataset(tf.data.Dataset):\n",
    "    def _generator(num_samples):\n",
    "        time.sleep(0.03)\n",
    "        \n",
    "        for sample_idx in range(num_samples):\n",
    "            time.sleep(0.015)\n",
    "            \n",
    "            yield (sample_idx, )\n",
    "            \n",
    "        \n",
    "    def __new__(cls, num_samples=3):\n",
    "        return tf.data.Dataset.from_generator(\n",
    "            cls._generator,\n",
    "            output_types=tf.dtypes.int64,\n",
    "            output_shapes=(1,),\n",
    "            args=(num_samples,)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The traning loop\n",
    "def benchmark(dataset, num_epochs=2):\n",
    "    start_time = time.perf_counter()\n",
    "    for epoch_num in range(num_epochs):\n",
    "        for sample in dataset:\n",
    "            time.sleep(0.01)\n",
    "    tf.print('Execution time:', time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize perfomance\n",
    "benchmark(ArtificialDataset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefetching\n",
    "benchmark(\n",
    "    ArtificialDataset()\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelizeing data extraction\n",
    "# Sequential interleave\n",
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(ArtificialDataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel interleave\n",
    "benchmark(\n",
    "    tf.data.Dataset.range(2)\n",
    "    .interleave(\n",
    "            ArtificialDataset,\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelizing data transformation\n",
    "def mapped_function(s):\n",
    "    tf.py_function(lambda: time.sleep(0.03), [], ())\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential mapping\n",
    "benchmark(\n",
    "    ArtificialDataset()\n",
    "    .map(mapped_function)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel mapping\n",
    "benchmark(\n",
    "    ArtificialDataset()\n",
    "    .map(\n",
    "        mapped_function,\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching\n",
    "benchmark(\n",
    "    ArtificialDataset()\n",
    "    .map(\n",
    "        mapped_function\n",
    "    ).cache(\n",
    "    ), 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing mapping\n",
    "fast_dataset = tf.data.Dataset.range(10000)\n",
    "\n",
    "def fast_benchmark(dataset, num_epochs=2):\n",
    "    start_time = time.perf_counter()\n",
    "    for _ in tf.data.Dataset.range(num_epochs):\n",
    "        for _ in dataset:\n",
    "            pass\n",
    "    \n",
    "    tf.print('Execution time:', time.perf_counter() - start_time)\n",
    "    \n",
    "def increment(x):\n",
    "    return x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar mapping\n",
    "fast_benchmark(\n",
    "    fast_dataset\n",
    "    .map(increment)\n",
    "    .batch(256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized mapping\n",
    "fast_benchmark(\n",
    "    fast_dataset\n",
    "    .batch(256)\n",
    "    .map(increment)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Time series forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "    # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "        print(e)\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the weather dataset\n",
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True)\n",
    "csv_path, _ = os.path.splitext(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_data(dataset, start_index, end_index, history_size, target_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "        \n",
    "    \n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i)\n",
    "        \n",
    "        data.append(np.reshape(dataset[indices], (history_size, 1)))\n",
    "        labels.append(dataset[i+target_size])\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SPLIT = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Forecast a univariate time series\n",
    "uni_data = df['T (degC)']\n",
    "uni_data.index = df['Date Time']\n",
    "uni_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_data.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_data = uni_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_train_mean = uni_data[:TRAIN_SPLIT].mean()\n",
    "uni_train_std  = uni_data[:TRAIN_SPLIT].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_data = (uni_data - uni_train_mean)/uni_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_past_history = 20\n",
    "univariate_future_target = 0\n",
    "\n",
    "x_train_uni, y_train_uni = univariate_data(uni_data, 0, TRAIN_SPLIT,\n",
    "                                          univariate_past_history,\n",
    "                                          univariate_future_target)\n",
    "x_val_uni, y_val_uni = univariate_data(uni_data, TRAIN_SPLIT, None,\n",
    "                                      univariate_past_history,\n",
    "                                      univariate_future_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Single sindow of past history')\n",
    "print(x_train_uni[0])\n",
    "print('\\n Target temperature to predict')\n",
    "print(y_train_uni[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_steps(length):\n",
    "    time_steps = []\n",
    "    for i in range(-length, 0, 1):\n",
    "        time_steps.append(i)\n",
    "    return time_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(plot_data, delta, title):\n",
    "    labels = ['History', 'True Future', 'Model Prediction']\n",
    "    marker = ['.-', 'rx', 'go']\n",
    "    time_steps = create_time_steps(plot_data[0].shape[0])\n",
    "    \n",
    "    if delta:\n",
    "        future = delta\n",
    "    else:\n",
    "        future = 0\n",
    "    \n",
    "    plt.title(title)\n",
    "    for i, x in enumerate(plot_data):\n",
    "        if i:\n",
    "            plt.plot(future, plot_data[i], marker[i], markersize=10,\n",
    "                    label=labels[i])\n",
    "        else:\n",
    "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.xlim([time_steps[0], (future+5)*2])\n",
    "    plt.xlabel('Time-Step')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot([x_train_uni[0], y_train_uni[0]], 0, 'Sample Example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "def baseline(history):\n",
    "    return np.mean(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot([x_train_uni[0], y_train_uni[0], baseline(x_train_uni[0])], 0,\n",
    "         'Baseline Prediction Example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent neural network\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "train_univariate = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))\n",
    "train_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_univariate = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni))\n",
    "val_univariate = val_univariate.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(8, input_shape=x_train_uni.shape[-2:]),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "simple_lstm_model.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in val_univariate.take(1):\n",
    "    print(simple_lstm_model.predict(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_INTERVAL = 200\n",
    "EPOCHS = 10\n",
    "\n",
    "simple_lstm_model.fit(train_univariate, epochs=EPOCHS,\n",
    "                     steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                     validation_data=val_univariate, validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the simple LSTM model\n",
    "for x, y in val_univariate.take(3):\n",
    "    plot = show_plot([x[0].numpy(), y[0].numpy(),\n",
    "                     simple_lstm_model.predict(x)[0]], 0, 'Simple LSTM model')\n",
    "    plot.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: forecast a multivariate time series\n",
    "features_considered = ['p (mbar)', 'T (degC)', 'rho (g/m**3)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[features_considered]\n",
    "features.index = df['Date Time']\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.plot(subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = features.values\n",
    "data_mean = dataset[:TRAIN_SPLIT].mean(axis=0)\n",
    "data_std  = dataset[:TRAIN_SPLIT].std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (dataset - data_mean) / data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single step model\n",
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                     target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "        \n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i - history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "        \n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "            \n",
    "    \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_history = 720\n",
    "future_target = 72\n",
    "STEP = 6\n",
    "\n",
    "x_train_single, y_train_single = multivariate_data(dataset, dataset[:, 1], 0,\n",
    "                                                  TRAIN_SPLIT, past_history,\n",
    "                                                  future_target, STEP,\n",
    "                                                  single_step=True)\n",
    "x_val_single, y_val_single = multivariate_data(dataset, dataset[:, 1], TRAIN_SPLIT,\n",
    "                                              None, past_history,\n",
    "                                              future_target, STEP,\n",
    "                                              single_step=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Single window of past history : {}'.format(x_train_single[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_single = tf.data.Dataset.from_tensor_slices((x_train_single, y_train_single))\n",
    "train_data_single = train_data_single.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_single = tf.data.Dataset.from_tensor_slices((x_val_single, y_val_single))\n",
    "val_data_single = val_data_single.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_step_model = tf.keras.models.Sequential()\n",
    "single_step_model.add(tf.keras.layers.LSTM(32,\n",
    "                                           input_shape=x_train_single.shape[-2:]))\n",
    "single_step_model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "single_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in val_data_single.take(1):\n",
    "    print(single_step_model.predict(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_step_history = single_step_model.fit(train_data_single, epochs=EPOCHS,\n",
    "                                           steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                           validation_data=val_data_single,\n",
    "                                           validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_history(history, title):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    epochs = range(len(loss))\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_history(single_step_history,\n",
    "                  'Single Step Training and Validation Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict a single step future\n",
    "for x, y in val_data_single.take(3):\n",
    "    plot = show_plot([x[0][:, 1].numpy(), y[0].numpy(),\n",
    "                     single_step_model.predict(x)[0]], 12,\n",
    "                    'Single Step Prediction')\n",
    "    plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Step Prediction\n",
    "future_target = 72\n",
    "x_train_multi, y_train_multi = multivariate_data(dataset, dataset[:, 1], 0,\n",
    "                                                TRAIN_SPLIT, past_history,\n",
    "                                                future_target, STEP)\n",
    "x_val_multi, y_val_multi = multivariate_data(dataset, dataset[:, 1], TRAIN_SPLIT,\n",
    "                                            None, past_history,\n",
    "                                            future_target, STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Single window of past history : {}'.format(x_train_multi[0].shape))\n",
    "print('\\n Target temperature to predict : {}'.format(y_train_multi[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi, y_train_multi))\n",
    "train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_multi = tf.data.Dataset.from_tensor_slices((x_val_multi, y_val_multi))\n",
    "val_data_multi = val_data_multi.batch(BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_step_plot(history, true_future, prediction):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    num_in = create_time_steps(len(history))\n",
    "    num_out = len(true_future)\n",
    "    \n",
    "    plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
    "    plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
    "            label='True Furue')\n",
    "    if prediction.any():\n",
    "        plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',\n",
    "                label='Prediction Future')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_data_multi.take(1):\n",
    "    multi_step_plot(x[0], y[0], np.array([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_step_model = tf.keras.models.Sequential()\n",
    "multi_step_model.add(tf.keras.layers.LSTM(32,\n",
    "                                         return_sequences=True,\n",
    "                                         input_shape=x_train_multi.shape[-2:]))\n",
    "multi_step_model.add(tf.keras.layers.LSTM(16, activation='relu'))\n",
    "multi_step_model.add(tf.keras.layers.Dense(72))\n",
    "\n",
    "multi_step_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in val_data_multi.take(1):\n",
    "    print (multi_step_model.predict(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_step_history = multi_step_model.fit(train_data_multi, epochs=EPOCHS,\n",
    "                                         steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                         validation_data=val_data_multi,\n",
    "                                         validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_train_history(multi_step_history, 'Multi-Step Training and Validation Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in val_data_multi.take(3):\n",
    "    multi_step_plot(x[0], y[0], multi_step_model.predict(x)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Text classification with an RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_' + string])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup input pipeline\n",
    "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n",
    "                         as_supervised=True)\n",
    "train_dataset, test_dataset = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vocabulary size: {}'.format(encoder.vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_string = 'Hello Tensorflow'\n",
    "\n",
    "encoded_string = encoder.encode(sample_string)\n",
    "print('Encoded string is {}'.format(encoded_string))\n",
    "\n",
    "original_string = encoder.decode(encoded_string)\n",
    "print('The original string: {}'.format(original_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert original_string == sample_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in encoded_string:\n",
    "    print('{} ----> {}'.format(index, encoder.decode([index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for training\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "train_output_shape = tf.compat.v1.data.get_output_shapes(train_dataset)  # 추가\n",
    "test_output_shape = tf.compat.v1.data.get_output_shapes(test_dataset)  # 추가\n",
    "\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE, train_output_shape)\n",
    "\n",
    "test_dataset = test_dataset.padded_batch(BATCH_SIZE, test_output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=10,\n",
    "                   validation_data=test_dataset,\n",
    "                   validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_size(vec, size):\n",
    "    zeros = [0] * (size - len(vec))\n",
    "    vec.extend(zeros)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_predict(sentence, pad):\n",
    "    encoded_sample_pred_text = encoder.encode(sample_pred_text)\n",
    "    \n",
    "    if pad:\n",
    "        encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\n",
    "    encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\n",
    "    predictions = model.predict(tf.expand_dims(encoded_sample_pred_text, 0))\n",
    "    \n",
    "    return (predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
    "                    'were out of this world. I would recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=False)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred_text = ('The movie was cool. The animation and the graphics '\n",
    "                    'were out of this world. I would recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=True)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack two or more LSTM Layers\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "             optimizers=tf.keras.optimizers.Adam(1e-4),\n",
    "             metircs=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=10,\n",
    "                   validation_data=test_dataset,\n",
    "                   validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred_text = ('The movie was not good. The animation and the graphics '\n",
    "                    'were terrible. I would not recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=False)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred_text = ('The movie was not good. The animation and the graphics '\n",
    "                    'were terrible. I would not recommend this movie.')\n",
    "predictions = sample_predict(sample_pred_text, pad=True)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history,'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Distributed training with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "mnist_train, mnist_test = datasets['train'], datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of deviced: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup input pipeline\n",
    "num_train_examples = info.splits['train'].num_examples\n",
    "num_test_examples = info.splits['test'].num_examples\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.0\n",
    "    \n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "eval_dataset  = mnist_test.map(scale).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "with strategy.scope():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                 optimizer=tf.keras.optimizers.Adam(),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the callbacks\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay(epoch):\n",
    "    if epoch < 3:\n",
    "        return 1e-3\n",
    "    elif epoch >= 3 and epoch < 7:\n",
    "        return 1e-4\n",
    "    else :\n",
    "        return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "                                                         model.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                       save_weights_only=True),\n",
    "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
    "    PrintLR()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Evaluate\n",
    "model.fit(train_dataset, epochs=12, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "eval_loss, eval_acc = model.evaluate(eval_dataset)\n",
    "\n",
    "print('Eval Loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to SavedModel\n",
    "path = 'saved_model/'\n",
    "model.save(path, save_format('tf'))\n",
    "\n",
    "# Load the model without strategy.scope\n",
    "unreplicated_model = tf.keras.models.load_model(path)\n",
    "\n",
    "unreplicated_model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "eval_loss, eval_acc = unreplicated_model.evaluate(eval_dataset)\n",
    "\n",
    "print('Eval Loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with strategy.scope\n",
    "with strategy.scope():\n",
    "    replicated_model = tf.keras.models.load_model(path)\n",
    "    replicated_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                            optimizer=tf.keras.optimizers.Adam(),\n",
    "                            metrics=['accuracy'])\n",
    "    \n",
    "    eval_loss, eval_acc = replicated_model.evaluate(eval_dataset)\n",
    "    print('Eval Loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[Studypie]Week_4_Day_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kcs",
   "language": "python",
   "name": "kcs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
