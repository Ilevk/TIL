{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4주차, 1일차 : 임베딩 이해하기 (총 80분)\n",
    "- ### Contents \n",
    "    1. Embeddings: https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation from Collaborative Filtering\n",
    "협업 필터링은 다른 여러 사용자의 관심분야를 바탕으로 특정 사용자의 관심분야를 예측하는 작업입니다. <br>\n",
    "예를 들어 영화 추천 작업을 살펴보겠습니다. 1,000,000명의 사용자와 500,000편의 영화 중 각 사용자가 본 영화의 목록이 있다고 가정합시다.<br>\n",
    "사용자에게 영화를 추천해주는 것이 목표입니다.<br>\n",
    "이 문제를 해결하려면 어떤 영화가 서로 비슷한지 파악하는 방법이 필요합니다.<br>\n",
    "이를 위해 영화가 서로 인접하도록 만들어진 저차원 공간에 영화를 임베딩할 수 있습니다.\n",
    "\n",
    "### 1) 1차원 수직선에 영화 정렬\n",
    "영화|등급|설명\n",
    ":---:|:---:|---\n",
    "세 가지 색: 블루|R|프랑스의 미망인이 교통사고로 남편과 딸을 잃은 후 슬픔에 빠집니다.\n",
    "다크 나이트 라이즈|PG-13|DC 코믹스 유니버스를 배경으로 하는 다크 나이트의 속편으로, 배트맨이 핵폭발로부터 고담시를 지키기 위해 고군분투 합니다.\n",
    "해리포터와 마법사의 돌|PG|고아가 된 소년은 자신이 마법사임을 깨닫고 호그와트 마법학교에 입학해 사악한 볼드모트 경과 첫 전투를 벌입니다.\n",
    "인크레더블|PG|교외에서 평범한 삶을 살게 된 슈퍼히어로 가족이 은퇴 생활에서 벗어나 악당 신드롬과 로봇 군단으로부터 슈퍼히어로들을 구합니다.\n",
    "슈렉|PG|친근한 괴물과 동료 당나귀가 용에 의해 성에 갇힌 피오나 공주를 구하기 위한 여정을 떠납니다.\n",
    "스타워즈|PG|루크 스카이워커와 한 솔로는 로봇 두 대와 함께 레아 공주와 은하계를 구하기 위해 힘을 합칩니다.\n",
    "벨빌의 세쌍둥이|PG-13|자전거 경주 선수가 투르 드 프랑스 경기 도중 납치당하자 그의 할머니와 뚱뚱한 개가 재즈 음악가 세 명의 도움을 받아 바다를 건너 손자를 구하기 위해 떠납니다.\n",
    "메멘토|R|기억 상실증 환자가 자신의 몸에 문신을 남기며 아내의 살인 사건을 해결하기 위해 고군분투합니다.\n",
    "\n",
    "#### 한 가지 솔루션 예(매우 부정확)\n",
    "\n",
    "<img src=https://developers.google.com/machine-learning/crash-course/images/Embedding1d.svg style='background-color:white'/>\n",
    "<b>Figure 1. 1차원 정렬의 예</b>\n",
    "이 임베딩 작업은 영화가 아동과 성인 중 어느 쪽을 대상으로 하는지 파악하는데 도움이 되지만 영화를 추천하려면 영화의 특성을 더 많이 캡쳐해야 합니다.\n",
    "\n",
    "### 2) 2차원 공간에서 영화 정렬\n",
    "<img src=https://developers.google.com/machine-learning/crash-course/images/Embedding2dWithLabels.svg style='background-color:white'/>\n",
    "<b>Figure 2. 2차원 정렬의 예</b>\n",
    "이 2차원 임베딩을 통해 아동과 성인 쪽 어느 쪽을 대상으로 하는지 여부와 블록버스터 영화와 예술 영화 중 어느 쪽에 가까운이 여부에 따라 서로 가깝고 비슷한 것으로 추론되는 영화 간의 거리를 정의합니다.\n",
    "<br>\n",
    "이 예에서는 각 차원에 이름을 지정했습니다. 임베딩을 학습할 때 개별 차원은 이름으로 학습되지 않습니다. 이러한 차원을 간혹 잠재 차원(latent dimension)이라고 하는데 잠재 차원이 나타내는 특징이 데이터상에 명확하게 드러나는 것이 아니라 유추되는 것이기 때문에 이렇게 불리곤 합니다.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Categorical Input Data\n",
    "범주형 데이터란 선택사항이 유한한 집합에 속한 하나 이상의 이산 항목을 표현하는 입력 특성을 가리킵니다. <br>\n",
    "범주형 데이터(Categorical data)는 대부분의 요소가 0인 희소 텐서를 통해 가장 효율적으로 표현됩니다. \n",
    "\n",
    "<img src=https://developers.google.com/machine-learning/crash-course/images/InputRepresentationWithValues.png style='background-color:white'/>\n",
    "<b>Figure 3. 영화 추천 문제에 관한 데이터 </b>\n",
    "\n",
    "그림 3 행렬의 각 행은 사용자의 영화 시청 기록을 포착한 예입니다. 각 사용자는 가능한 모든 영화 중 일부만 시청했을 것이기 때문에 이 행은 희소 텐서로 표현됩니다. 마지막 행은 영화 아이콘 위에 표시된 어휘 색인을 사용하여 희소 텐서 (1, 3, 999999)에 대응합니다.\n",
    "<br><br>\n",
    "마찬가지로 단어, 문장, 문서를 희소 벡터로 표현할 수 있는데, 이 때는 각 어휘 목록에 속한 단어가 앞에서 살펴본 영화 추천 예제에서의 영화와 비슷한 역할을 하게 됩니다. 의미론적으로 유사한 항목이 벡터 공간에서 비슷한 거리에 있도록 각 희소 벡터를 숫자 벡터로 표현하는 방법이 필요합니다.\n",
    "<br><br>\n",
    "가장 간단한 방법은 어휘의 모든 단어에 대해 노드가 있는 거대한 입력 레이어를 정의하거나, 적어도 데이터에 나타나는 각 단어에 대해 노드가 있는 입력 레이어를 정의하는 것입니다. 이러한 방법으로 원-핫 인코딩이 있습니다.<br><br>\n",
    "더 일반적으로는 벡터에 더 큰 텍스트 뭉치에 있는 단어의 수가 포함될 수 있습니다. 이는 BOW(bag of words)표현으로 알려져 있습니다. BOW 벡터에서는 500,000개의 노드 중 여러 개의 노드의 값이 0이 아닐 수 있습니다.<br><br>\n",
    "하지만 어떤 방식으로든 0이 아닌 값을 결정하더라도 단어당 노드 1개 방식으로는 0이 아닌 값이 상대적으로 거의 없는 매우 큰 벡터인 지극히 희소한 입력 벡터가 발생합니다. \n",
    "\n",
    "### 망 크기\n",
    "입력 벡터가 거대해지면 신경망에 엄청나게 많은 가중치가 만들어집니다. 어휘에 M개의 단어가 있고 입력 위에 있는 망의 첫 번째 레이어에 N개의 노드가 있으면 해당 레이어에 대해 MxN개의 가중치를 학습시켜야 합니다. 가중치의 수가 많아지면 다음과 같은 문제가 발생합니다.\n",
    "\n",
    "- 데이터의 양: 모델의 가중치가 많을수록 효과적인 학습을 위해 더 많은 데이터가 필요합니다.\n",
    "- 계산량: 가중치가 많을수록 모델을 학습하고 사용하는 데 더 많은 계산이 필요합니다.\n",
    "\n",
    "### 벡터 간의 의미 있는 관계 부족\n",
    "이미지 분류자에 RGB 채널의 픽셀 값을 공급하는 경우, '가까운'값에 대해 언급할 필요가 있습니다. 불그스름한 파란색은 의미론적으로든 벡터간 기하학적 거리로든 순수한 청색에 가깝습니다. 그러나 '말'에 대한 인덱스 1247에 1을 가진 벡터는 '텔레비전'에 대한 인덱스 238에서 1을 갖는 벡터보다 '영양'에 대한 인덱스 50,430에서 1을 갖는 벡터에 더 가깝지 않습니다. (말은 텔레비전 보다는 영양에 더 가까운 거리를 나타내야 하지만, 예제에서 정의한 내용은 그렇지 않다.)\n",
    "\n",
    "### 솔루션: 임베딩\n",
    "이러한 문제에 대한 해결책은 크기가 큰 희소 벡터를 의미론적 관계를 보존하는 저차원 공간으로 변환하는 임베딩을 사용하는 것\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translating to a Lower-Dimensional Space\n",
    "고차원 데이터를 저차원 공간에 매핑하여 희소한 입력 데이터의 핵심 문제를 해결할 수 있습니다. <br><br>\n",
    "실습에서 볼 수 있듯이, 작은 다차원 공간에서도 의미론적으로 유사한 항목은 한데 묶고 유사하지 않은 항목은 서로 떨어뜨리는 작업을 자유롭게 수행할 수 있습니다. 벡터 공간의 위치(거리와 방향)는 좋은 임베딩을 통해 의미론을 인코딩할 수 있습니다.\n",
    "\n",
    "<img src=https://developers.google.com/machine-learning/crash-course/images/linear-relationships.svg style='background-color:white'/>\n",
    "<b> Figure 4. 임베딩은 우수한 유추를 제공할 수 있습니다. </b>\n",
    "\n",
    "### 망 축소하기\n",
    "풍부한 의미론적 관계를 이코딩하기에 충분한 차원이 필요하지만 그와 동시에 시스템을 더 빠르게 학습할 수 있게 할만큼의 작은 임베딩 공간도 필요합니다. 유용한 임베딩은 대략 수백 차원에 달할 수 있습니다. 이 크기는 자연어 작업에 필요한 어휘의 크기보다 10의 몇 승 만큼이나 더 작습니다.\n",
    "\n",
    "### 검색표로서의 임베딩\n",
    "임베딩은 하나의 행렬이고, 행렬의 각 열은 어휘 항목 하나에 대응합니다. 단일 어휘 항목에 대한 밀집벡터를 얻으려면 해당 항목에 대응하는 열을 검색합니다. 희소 벡터에 어휘 항목의 수가 포함되어 있으면 각 임베딩에 해당 항목의 수를 곱한 다음 합계에 추가할 수 있습니다. \n",
    "(문장인 경우에는 각 항목들의 임베딩 벡터를 모두 더 한다)\n",
    "\n",
    "### 행렬 곱셈으로서의 임베딩 검색\n",
    "방금 설명한 검색, 곱셈, 덧셈 절차는 행렬 곱셈과 동일합니다. 1xN 크기의 희소 표현 S와 NxM 크기의 임베딩 표 E가 주어지면 행렬 곱셈 SxE를 통해 1xM 밀집 벡터를 얻을 수 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Obtaining Embeddings\n",
    "### 표준 차원 축소 기법\n",
    "저차원 공간에서 고차원 공간의 중요한 구조를 캡처할 수 있는 여러가지 수학적 기법이 존재합니다. 이론상 이 기법들은 어느 것이든 머신러닝 시스템용 임베딩을 만드는 데 사용할 수 있습니다.<br><br>\n",
    "예를 들어 주성분 분석(PCA)은 단어 임베딩을 만드는 데 사용되어 왔습니다. BOW 벡터와 같은 인스턴스의 집합이 주어지면 PCA는 단일 차원으로 축소할 수 있는 높은 상관도를 갖는 차원을 찾습니다.<br>\n",
    "\n",
    "### Word2vec\n",
    "Word2vec은 <b>분포 가설</b>에 기반하여 의미론적으로 유사한 단어를 기하학적으로 가까운 임베딩 벡터로 매핑합니다.<br><br>\n",
    "분포 가설은 주로 같은 단어가 인접하는 단어 간에는 의미론적으로 유사한 경향이 있다고 봅니다. <br><br>\n",
    "Word2vec은 실제로 함께 등장하는 단어 그룹과 무작위로 그룹화된 단어를 구분하도록 신경망을 학습시켜 이와 같은 문맥상의 정보를 활용합니다. 입력 레이어는 하나 이상의 문맥 단어와 함께 대상 단어의 희소 표현을 취합니다. 이 입력은 더 작은 히든 레이어 하나에 연결됩니다. \n",
    "\n",
    "### 더 큰 모델의 일부로서 임베딩 학습\n",
    "임베딩을 대상 작업을 위한 신경망의 일부로서 학습할 수도 있습니다. 이 접근법은 특정 시스템에 맞게 임베딩을 효과적으로 맞춤화할 수 있지만 임베딩을 별도로 학습하는 것보다 시간이 오래걸릴 수 있습니다.<br><br>\n",
    "일반적으로, 희소 데이터 또는 임베딩하려는 밀집 데이터가 있는 경우 크리가 d인 특수 유형의 은닉 단위인 임베딩 단위를 만들 수 있습니다. 이 임베딩 레이어는 다른 특성 및 히든 레이어와 결합할 수 있습니다. \n",
    "\n",
    "<img src=https://developers.google.com/machine-learning/crash-course/images/EmbeddingExample3-1.svg style='background-color:white'/>\n",
    "<b>Figure 5. 협업 필터링 데이터에서 영화 임베딩을 학습하는 샘플 DNN 아키텍처</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kyle",
   "language": "python",
   "name": "kyle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
