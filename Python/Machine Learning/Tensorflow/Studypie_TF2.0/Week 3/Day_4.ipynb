{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3주차, 4일차 : 다중 클래스 신경망 이해하기 (총 50분)\n",
    "- ### Contents \n",
    "    1. Multi Class Neural Networks: https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/video-lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi Class Neural Networks: One vs. All\n",
    "<b>One vs. all</b> 은 이진 분류를 활용할 수 있는 방법을 제공합니다. N개의 가능한 솔루션을 가진 분류 문제에서 one-vs.-all 솔루션은 N개의 분리된 이진 분류기로 구성되어 있습니다. 모델은 이진 분류기의 시퀀스를 통해 분리된 분류 질문에 대해 대답하도록 학습됩니다.<br>\n",
    "예를 들어 강아지 사진이 주어지면, 다섯 개의 다른 인식자가 학습될 수 있습니다. 4개는 이미지를 부정으로 보고(개가 아님) 하나는 긍정으로 봅니다.(개임)\n",
    "\n",
    "1. 이 사진은 사과인가요? 아니오\n",
    "2. 이 사진은 곰인가요? 아니오\n",
    "3. 이 사진은 사탕인가요? 아니오\n",
    "4. 이 사진은 개인가요? 예\n",
    "5. 이 사진은 달걀인가요? 아니오\n",
    "\n",
    "이러한 접근법은 전체 클래스의 수가 적을 때 상당히 합리적이라고 할 수 있습니다. 하지만 클래스의 개수가 많아질수록 비효율성이 높아집니다. \n",
    "<br>\n",
    "<br>\n",
    "각 출력 노드들이 다른 클래스를 표현하게 함으로써 딥 뉴럴네트워크에서 one-vs.-all 모델을 더 효율적으로 생성할 수 있습니다.<br>\n",
    "다음 사진이 모델의 구조를 나타냅니다.\n",
    "\n",
    "<img src=https://developers.google.com/machine-learning/crash-course/images/OneVsAll.svg />\n",
    "<b> Figure 1. One-vs.-All 신경망 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi Class Neural Networks: Softmax\n",
    "로지스틱 회귀에서 0 ~ 1.0 사이로 실수 값을 출력했던 것을 떠올려보겠습니다. 예를 들어 로지스틱 회귀의 출력이 0.8이라는건 이메일 분류기가 스팸이라는 것에 80%를 제시한 것이고, 스팸이 아니라는 것에 20%를 부여한 것입니다.<br>\n",
    "명확하게는 이메일이 스팸이다 또는 스팸이 아니다라는 것에 대한 확률의 합은 1.0 이라는 겁니다.\n",
    "<br><br>\n",
    "<b>Softmax</b>는 이러한 아이디어를 다중 클래스의 분류의 세계로 확장하는 개념입니다. Softmax는 다중 클래스 문제의 각 클래스들에 대한 확률에 실수 값은 부여합니다. 이러한 실수 확률은 반드시 최대 1.0이 됩니다. <br>이러한 추가적인 제약 조건은 다른 방법보다 모델이 더욱 빠르게 수렴하도록 도와줍니다.\n",
    "<br><br>\n",
    "예를 들어, Figure 1에서 보았던 이미지를 생각해봅시다. Softmax는 특정한 클래스를 포함한 이미지의 Likelihoods(그럴싸한 정도)를 만들어 냅니다.\n",
    "\n",
    "Class|Probability\n",
    ":---:|---\n",
    "Apple|0.001\n",
    "Bear|0.04\n",
    "Candy|0.008\n",
    "Dog|0.95\n",
    "Egg|0.001\n",
    "\n",
    "Softmax는 신경망에서 출력층 직전에 구현되어 있습니다. Softmax 층은 반드시 출력층과 동일한 수의 노드를 가지고 있어야합니다.\n",
    "\n",
    "<img src=https://developers.google.com/machine-learning/crash-course/images/SoftmaxLayer.svg style='background-color:white'/>\n",
    "<b>Figure 2. 신경망에서의 소프트 맥스 층</b>\n",
    "\n",
    "Softmax 방정식은 다음과 같다.\n",
    "\n",
    "$$p(y=j|\\boldsymbol x)={{e^{(w_j^T\\boldsymbol x +b_j)}} \\over {\\sum_{k \\in K}e^{(w_k^T\\boldsymbol x +b_j)}}}$$\n",
    "\n",
    "### Softmax Options\n",
    "Softmax의 변형을 고려해봐야 합니다.\n",
    "- Full Softmax: 우리가 지금까지 이야기한 Softmax 입니다. 각 가능한 클래스에 대해 확률을 계산하는 것\n",
    "- Candidate sampling: 모든 양수 레이블에 대한 확률을 계산하지만, 음수 레이블의 무작이 샘플에 대해서만 확률을 계산함을 의미한다. 클래스가 많은 경우 소프트맥스의 연산량을 줄이기 위한 방법.\n",
    "\n",
    "### One Label vs. Many Labels\n",
    "Softmax는 정확하게 한 클래스 클래스 멤버에 대해 가정합니다. 하지만 동시에 여러 클래스의 멤버가 될 수도 있습니다. 이러한 경우에\n",
    "- Softmax를 사용하면 안됩니다.\n",
    "- 다중 로지스틱 회귀를 사용해야 합니다.\n",
    "예를 들어 하나의 Example이 정확하게 한 하이템을 포함하고 있는 이미지(과일)라고 가정해봅시다. Softmax는 배, 오랜지, 사과 등등에 대한 확률을 결정할 수 있습니다. 만약에 이미지가 다른 것들을 포함하고 있다면 어떨까요. (다른 과일을 담고있는 그릇이라던가..) 이러한 경우 다중 로지스틱 회귀를 사용해야 할겁니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kyle",
   "language": "python",
   "name": "kyle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
