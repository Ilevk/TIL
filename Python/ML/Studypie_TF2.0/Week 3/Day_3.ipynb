{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3주차, 3일차 : 신경망 학습 이해하기 (총 40분)\n",
    "- ### Contents \n",
    "    1. Training Neural Nets: https://developers.google.com/machine-learning/crash-course/training-neural-networks/video-lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Failure Cases\n",
    "몇 가지 일반적인 이유로 인해 역전파에서 문제가 나타날 수 있습니다.\n",
    "\n",
    "##### Vanishing Gradients\n",
    "입력 쪽에 가까운 하위 레이어의 가중치 기울기가 매우 작아질 수 있습니다. 이러한 경사를 계산할 때는 많은 작은 항의 곱을 구하는 과정이 포함될 수 있습니다.<br>\n",
    "하위 레이어의 경사가 0에 가깝게 소실되면 이러한 레이어에서 학습 속도가 크게 저하되거나 중지됩니다.<br>\n",
    "ReLU 활성화 함수를 통해 기울기가 소실되는 것을 방지할 수 있습니다.\n",
    "\n",
    "##### Exploding Gradients\n",
    "네트워크의 가중치가 매우 크다면, 낮은 레이어의 기울기가 많은 큰 항의 곱을 포함하게 될 수 있습니다. 이러한 경우 기울기 폭발 현상이 일어나며, 기울기가 매우 커지거나 발산할 수 있습니다.<br>\n",
    "Batch 정규화를 통해 기울기 폭발 현상을 예방할 수 있습니다. 또는 학습률을 낮추어 예방합니다.\n",
    "\n",
    "##### Dead ReLU Units\n",
    "한번 ReLU 유닛을 통해 들어온 값이 0보다 낮아지면, ReLU 유닛은 멈추게 됩니다. 그러면 출력은 0 값을 갖는 활성화 값이 나올거고, 네트워크의 출력에 아무것도 기여할 수 없게 됩니다. 또한, 기울기가 더 이상 역전파를 통해 흐르지 않게 됩니다. 기울기의 소스가 차단되면, ReLU의 입력에 대한 가중치 합이 0 이상으로 변할 수 없게 될지도 모릅니다.\n",
    "\n",
    "#### Dropout Regularization\n",
    "또 다른 형태의 정규화로, 드롭 아웃이 있습니다. 이는 신경망에서 매우 효율적입니다. 드롭 아웃은 임의로 특정 한 기울기 스텝에 대해 활성화 값을 꺼버립니다. 드랍 아웃을 많이 할수록, 정규화가 더 강하게 들어갑니다.\n",
    "\n",
    "- 0.0 = 정규화 드롭 아웃을 사용하지 않음.\n",
    "- 1.0 = 모든 것을 꺼버린다, 모델이 학습할 수 없음.\n",
    "- 0.0 ~ 1.0 = 매우 유용하다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kyle",
   "language": "python",
   "name": "kyle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
