{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2주차, 3일차 : L2 정규화 및 람다 이해하기, 로지스틱 회귀 이해하기 (총 60분)\n",
    "\n",
    "- ### Contents \n",
    "    1. Ragularization: Simplicity : https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/video-lecture\n",
    "    2. Logistic Regression: https://developers.google.com/machine-learning/crash-course/logistic-regression/video-lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ragularization: L2 Regularization\n",
    "\n",
    "- 다음에 제시된 일반화 곡선은 학습 반복 횟수에 대해 학습 세트와 검증 세트의 손실을 보여준다.\n",
    "\n",
    "<img src=https://developers.google.com/machine-learning/crash-course/images/RegularizationTwoLossFunctions.svg />\n",
    "<b> Figure 1. 학습 셋과 검증 셋에서의 손실</b>\n",
    "\n",
    "- 그림 1은 학습 손실은 점차 감소하지만 검증 손실은 결국 증가하는 모델을 보여줍니다. <br>즉, 이 일반화 곡선은 모델이 학습 데이터 셋에 과적합하다는 것을 보여줍니다. <br> 복잡한 모델에 페널티를 부여하는 정규화를 진행하여 과적합을 방지하도록 합니다.\n",
    "\n",
    "- 다시 말해 다음은 단순히 손실을 최소화하는 것만을 목표로 삼습니다.(경험적 위험 최소화, ERM)\n",
    "$$최소화(손실(데이터|모델))$$\n",
    "- 이제 구조적 위험 최소화를 통해 다음과 같이 손실과 복잡도를 함께 최소화해 보겠습니다.\n",
    "$$최소화(손실(데이터|모델)+복잡도(모델))$$\n",
    "- 이제 우리의 학습 최적화 알고리즘은 모델이 얼마나 적합한지 측정하는 손실 항과 모델의 복잡도를 측정하는 정규화 항의 함수가 됩니다.\n",
    "\n",
    "- 본 머신러닝 단기집중과정에서는 일반적인 2가지 방법으로 모델 복잡도를 다루게 됩니다\n",
    "    - 모델의 모든 특성의 가중치에 대한 함수로서의 모델 복잡도(가중치로 복잡도를 보겠다)\n",
    "    - 0이 아닌 가중치를 사용하는 특성의 총 개수에 대한 함수로서의 모델 복잡도(입력 차원의 크기로 복잡도를 보겠다.)\n",
    "\n",
    "\n",
    "- 모델의 가중치가 대체적으로 큰 경우 모델의 복잡도가 높아지는 경향이 있다. L2 정규화는 이러한 모델의 가중치 값을 줄여주어 과적합을 방지한다.\n",
    "\n",
    "- 모든 특성 가중치를 제곱한 값의 합계로서 정규화 항을 정의하는 $L_2$정규화 공식을 사용하여 복잡도를 수치화할 수 있다.\n",
    "$$L_2정규화\\ 항=||w||^2_2=w_1^2+w_2^2+....+w_n^2$$\n",
    "- 이 공식에서 0에 가까운 가중치는 모델 복잡도에 거의 영향을 미치지 않는 반면, 극단적인 가중치는 큰 영향을 미칠 수 있습니다.\n",
    "- $L_2$정규화는 가중치들의 값을 0에 가깝도록 만들어줍니다.\n",
    "- 모든 특성 가중치의 절대값의 합계인 $L_1$정규화는 몇몇 가중치의 값을 0으로 만들어 특성을 선택하는 효과를 가집니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Regularization: Lambda\n",
    "- 모델 개발자는 람다라는 스칼라(정규화율이라고도 한다)값을 정규화 항의 값에 곱하여 정규화 항의 전반적인 영향을 조정합니다. \n",
    "$$최소화(손실(데이터|모델)+\\lambda복잡도(모델))$$\n",
    "- $L_2$정규화를 수행하면 모델에 다음과 같은 효과를 줄 수 있습니다.\n",
    "    - 가중치 값을 0으로 유도(정확히 0은 아니다)\n",
    "    - 정규(종 모양 또는 가우시안) 분포를 사용하여 가중치 평균을 0으로 유도\n",
    "- 람다 값을 높이면 정규화 효과가 강화됩니다. 높은 람다 값에 대한 가중치 히스토그램은 그림 2처럼 보일 수 있습니다.\n",
    "\n",
    "<img src=https://developers.google.com/machine-learning/crash-course/images/HighLambda.svg />\n",
    "<b>Figure 2. 가중치 히스토그램</b>\n",
    "\n",
    "- 람다 값을 낮추면 다음과 같이 더 평평한 히스토그램이 산출되는 경향이 있습니다.\n",
    "\n",
    "<img src=https://developers.google.com/machine-learning/crash-course/images/LowLambda.svg />\n",
    "<b>Figure 3. 더 낮은 람다 값으로 생성된 가중치 히스토그램</b>\n",
    "\n",
    "- 람다 값을 선택할 때 세워야 할 목표\n",
    "    - 단순성과 학습 데이터 정합성 사이에 적절한 균형을 맞춰야한다.\n",
    "    - 람다 값이 너무 높으면 모델이 단순해지지만, 데이터가 과소적합 될 수 있습니다. 충분히 데이터를 학습할 수 있는 정도의 정규화를 진행해야 합니다.\n",
    "    - 람다 값이 너무 낮으면 모델은 더 복잡해지고, 데이터가 과적합 될 수 있습니다. 일반화된 모델을 얻을 수 있도록 람다값을 적절하게 높여야 합니다.\n",
    "    \n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression: Calculating a Probability\n",
    "- 많은 문제에 확률 추정치가 출력으로 필요합니다. 로지스틱 회귀는 매우 효율적인 확률 계산 알고리즘 입니다. <br>실제로 반환된 확률을 다음 두 방법 중 하나로 사용할 수 있습니다.\n",
    "    - 있는 그대로\n",
    "    - 이진 카테고리로 변환\n",
    "    \n",
    "- 확률을 `있는 그대로`사용하는 방법을 살펴보겠습니다. 한밤중에 개가 짖는 확률을 예측하기 위한 로지스틱 회귀 모델입니다.<br>이 확률은 다음과 같이 표시합니다.\n",
    "\n",
    "$$p(bark|night)$$\n",
    "\n",
    "- 로지스틱 회귀 모델이 예측한 $p(bark|night)$가 0.005이면 개 주인은 1년동안 약 18번 놀라서 깨게됩니다.\n",
    "\n",
    "$$startled=p(bark|night)*nights$$\n",
    "$$18~=0.05*365$$\n",
    "\n",
    "- 많은 경우 로지스틱 회귀 출력을 이진 분류 문제의 해결 방법으로 매핑합니다. 이진 분류 문제의 목표는 가능한 두 라벨 중 하나를 올바로 예측하는 것입니다. \n",
    "\n",
    "- 로지스틱 회귀 모델의 출력이 항상 0과 1사이에 포함되는지는 시그모이드 함수를 통해 확인할 수 있습니다.\n",
    "\n",
    "$$y={1\\over{1+e^{-z}}}$$\n",
    "\n",
    "<img src=https://developers.google.com/machine-learning/crash-course/images/SigmoidFunction.png />\n",
    "<b> Figure 1. 시그모이드 함수</b>\n",
    "\n",
    "- `z`가 로지스틱 회귀를 사용하여 학습된 모델의 선형 레이어의 출력을 나타내는 경우 sigmoid(Z)는 0과 1사이의 값(확률)을 생성합니다. \n",
    "\n",
    "$$y'={1\\over{1+e^{-(z)}}}$$\n",
    "\n",
    "- 여기서 \n",
    "    - y'는 특정 예에 관한 로지스틱 회귀 모델의 출력입니다.\n",
    "    - $z=b+w_1x_1+w_2x_2+...w_Nx_N$\n",
    "        - w 값은 모델의 학습된 가중치이고, b는 편향입니다.\n",
    "        - x 값은 특정 예에 대한 특성 값입니다.\n",
    "\n",
    "- z는 `z`를 '1'라벨(예:개가 짖음)의 확률을 '0'라벨(예:개가 짖지않음)의 확률로 나눈 값의 로그로 정의할 수 있는 시그모이드 상태의 역수이므로 로그 오즈(Log-odds)라고도 합니다.\n",
    "\n",
    "- 다음은 ML 라벨이 포함된 시그모이드 함수입니다. \n",
    "\n",
    "<img src=https://developers.google.com/machine-learning/crash-course/images/LogisticRegressionOutput.svg  style='background-color:white'/>\n",
    "<b>Figure 2. 로지스틱 회귀 출력</b>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression: Loss and Regularizaiton\n",
    "### 로지스틱 회귀의 손실 함수\n",
    "- 선형 회귀의 손실 함수는 제곱 손실입니다. 로지스틱 회귀의 손실 함수는 **로그 손실**로 다음과 같이 정의됩니다.\n",
    "\n",
    "$$로그손실=\\sum_{(x,y)\\in D}{-ylog(y')-(1-y)log(1-y')}$$\n",
    "\n",
    "- 여기서 \n",
    "    - $(x,y)\\in D$는 라벨이 있는 예(x,y 쌍)가 많이 포함된 데이터셋입니다.\n",
    "    - y는 라벨이 있는 예의 라벨입니다. 로지스틱 회귀이므로 y값은 모두 0 또는 1이어야 합니다.\n",
    "    - y'는 x의 특성 세트에 대한 예측 값(0~1 사이 값) 입니다.\n",
    "\n",
    "- 로그 손실 방정식은 정보 이론에서 말하는 섀넌의 엔트로피 측정과 밀접한 관련이 있으며, 또한 우도 함수의 음의 로그로 y의 베르누이 분포를 가정합니다. 실제로 손실 함수를 최소화하면 최대 우도 추정치가 생성됩니다. 따라서 Maximum Likelihood Estimation을 통해 손실을 최소화합니다.\n",
    "\n",
    "### 로지스틱 회귀의 정규화\n",
    "- 정규화는 로지스틱 회귀 모델링에서 매우 중요합니다. 정규화 하지 않으면 로지스틱 회귀의 점근 특성이 고차원에서 계속 손실을 0으로 만들려고 시도합니다.\n",
    "- 대부분의 로지스틱 회귀 모델에서 모델 복잡성을 줄이기 위해 다음 두 전략 중 하나를 사용합니다.\n",
    "    - $L_2$정규화\n",
    "    - 조기 중단, 즉 학습 단계 수 또는 학습률을 제한합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kyle",
   "language": "python",
   "name": "kyle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
